{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create AI-Tutor vector database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(\n",
    "        encoding.encode(\n",
    "            string, disallowed_special=(encoding.special_tokens_set - {\"<|endoftext|>\"})\n",
    "        )\n",
    "    )\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "def clean_jsonl_file(input_filepath, output_filepath):\n",
    "    cleaned_data = []\n",
    "\n",
    "    with open(input_filepath, \"r\") as file:\n",
    "        for line in file:\n",
    "            json_obj = json.loads(line)\n",
    "            content = json_obj.get(\"content\", \"\")\n",
    "            token_count = num_tokens_from_string(content, \"cl100k_base\")\n",
    "\n",
    "            # Check conditions for keeping the line\n",
    "            if token_count > 7 and not (\n",
    "                token_count == 92 and json_obj.get(\"name\") == \"Transformers\"\n",
    "            ):\n",
    "                # Create a new OrderedDict with 'tokens' as the first key\n",
    "                new_obj = OrderedDict([(\"tokens\", token_count)])\n",
    "                # Add the rest of the key-value pairs from the original object\n",
    "                new_obj.update(json_obj)\n",
    "                cleaned_data.append(new_obj)\n",
    "\n",
    "    with open(output_filepath, \"w\") as file:\n",
    "        for item in cleaned_data:\n",
    "            json.dump(item, file)\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "    print(f\"Original number of lines: {sum(1 for _ in open(input_filepath))}\")\n",
    "    print(f\"Cleaned number of lines: {len(cleaned_data)}\")\n",
    "\n",
    "\n",
    "# Usage\n",
    "input_filepath = \"../hf_transformers_v4_42_0.jsonl\"\n",
    "output_filepath = \"../hf_transformers_v4_42_0_cleaned.jsonl\"\n",
    "clean_jsonl_file(input_filepath, output_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merges lines by 'URL' and creates a new file with the merged data.\n",
    "\n",
    "Fixes the 'name'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(\n",
    "        encoding.encode(\n",
    "            string, disallowed_special=(encoding.special_tokens_set - {\"<|endoftext|>\"})\n",
    "        )\n",
    "    )\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "def should_not_merge(url):\n",
    "    \"\"\"Check if the URL contains any of the exclusion patterns.\"\"\"\n",
    "    exclusion_patterns = [\"model_doc\", \"internal\", \"main_classes\"]\n",
    "    return any(pattern in url for pattern in exclusion_patterns)\n",
    "\n",
    "\n",
    "def merge_jsonl(input_file, output_file):\n",
    "    # Dictionary to store merged data\n",
    "    merged_data = defaultdict(list)\n",
    "\n",
    "    # Read and process the input file\n",
    "    with open(input_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            url = data[\"url\"]\n",
    "            merged_data[url].append(data)\n",
    "\n",
    "    # Write the merged data to the output file\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for url, entries in merged_data.items():\n",
    "            if len(entries) == 1 or should_not_merge(url):\n",
    "                # If there's only one entry or it shouldn't be merged, write all entries as is\n",
    "                for entry in entries:\n",
    "                    entry[\"retrieve_doc\"] = False\n",
    "                    json.dump(entry, f)\n",
    "                    f.write(\"\\n\")\n",
    "            else:\n",
    "                # Merge the entries\n",
    "                merged_entry = entries[0].copy()\n",
    "                merged_entry[\"content\"] = \"\\n\\n\".join(\n",
    "                    entry[\"content\"] for entry in entries\n",
    "                )\n",
    "                merged_entry[\"tokens\"] = num_tokens_from_string(\n",
    "                    merged_entry[\"content\"], \"cl100k_base\"\n",
    "                )\n",
    "                merged_entry[\"retrieve_doc\"] = True\n",
    "                json.dump(merged_entry, f)\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "\n",
    "# Usage\n",
    "input_file = \"../hf_transformers_v4_42_0_cleaned.jsonl\"\n",
    "output_file = \"../hf_transformers_v4_42_0_merged.jsonl\"\n",
    "merge_jsonl(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count tokens of lines in merged file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import tiktoken\n",
    "\n",
    "\n",
    "# def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "#     \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "#     encoding = tiktoken.get_encoding(encoding_name)\n",
    "#     num_tokens = len(\n",
    "#         encoding.encode(\n",
    "#             string, disallowed_special=(encoding.special_tokens_set - {\"<|endoftext|>\"})\n",
    "#         )\n",
    "#     )\n",
    "#     return num_tokens\n",
    "\n",
    "\n",
    "# def count_tokens(input_file):\n",
    "\n",
    "#     # Read and process the input file\n",
    "#     with open(input_file, \"r\") as f:\n",
    "#         for i, line in enumerate(f):\n",
    "#             data = json.loads(line)\n",
    "#             content = data[\"content\"]\n",
    "#             nb_tokens = num_tokens_from_string(content, \"cl100k_base\")\n",
    "#             # print(i + 1, data[\"url\"], nb_tokens)\n",
    "#             if nb_tokens > 2000:\n",
    "#                 print(i + 1, data[\"url\"], data[\"name\"], nb_tokens)\n",
    "#             # if nb_tokens < 8:\n",
    "#             # print(nb_tokens)\n",
    "#             # print(data[\"url\"])\n",
    "#             # print(data[\"content\"])\n",
    "\n",
    "\n",
    "# # Usage\n",
    "# input_file = \"../hf_transformers_v4_42_0_merged.jsonl\"\n",
    "# # input_file = \"../hf_transformers_v4_42_0.jsonl\"\n",
    "# count_tokens(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a set of llama-index Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.schema import MetadataMode\n",
    "import json\n",
    "\n",
    "\n",
    "def create_docs(input_file):\n",
    "    with open(input_file, \"r\") as f:\n",
    "        documents = []\n",
    "        for i, line in enumerate(f):\n",
    "            data = json.loads(line)\n",
    "            documents.append(\n",
    "                Document(\n",
    "                    text=data[\"content\"],\n",
    "                    metadata={\n",
    "                        \"url\": data[\"url\"],\n",
    "                        \"title\": data[\"name\"],\n",
    "                        \"tokens\": data[\"tokens\"],\n",
    "                        \"retrieve_doc\": data[\"retrieve_doc\"],\n",
    "                    },\n",
    "                    excluded_llm_metadata_keys=[\n",
    "                        \"url\",\n",
    "                        \"title\",\n",
    "                        \"tokens\",\n",
    "                        \"retrieve_doc\",\n",
    "                    ],\n",
    "                    excluded_embed_metadata_keys=[\n",
    "                        \"url\",\n",
    "                        \"title\",\n",
    "                        \"tokens\",\n",
    "                        \"retrieve_doc\",\n",
    "                    ],\n",
    "                )\n",
    "            )\n",
    "        return documents\n",
    "\n",
    "\n",
    "documents = create_docs(\"../hf_transformers_v4_42_0_merged.jsonl\")\n",
    "print(documents[0])\n",
    "print(documents[0].metadata)\n",
    "\n",
    "document_dict = {doc.doc_id: doc for doc in documents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import chromadb\n",
    "\n",
    "# # create client and a new collection\n",
    "# # chromadb.EphemeralClient saves data in-memory.\n",
    "# chroma_client = chromadb.PersistentClient(path=\"./ai-tutor-dataset\")\n",
    "# chroma_collection = chroma_client.create_collection(\"ai-tutor-dataset\")\n",
    "\n",
    "# from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "# from llama_index.core import StorageContext\n",
    "\n",
    "# # Define a storage context object using the created vector database.\n",
    "# vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "# storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# Build index / generate embeddings using OpenAI embedding model\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    # embed_model=OpenAIEmbedding(model=\"text-embedding-3-small\"),\n",
    "    embed_model=OpenAIEmbedding(model=\"text-embedding-3-large\", mode=\"similarity\"),\n",
    "    transformations=[SentenceSplitter(chunk_size=800, chunk_overlap=400)],\n",
    "    show_progress=True,\n",
    "    use_async=True,\n",
    "    # storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# llm = OpenAI(temperature=1, model=\"gpt-3.5-turbo\", max_tokens=None)\n",
    "# query_engine = index.as_query_engine(\n",
    "#     llm=llm,\n",
    "#     similarity_top_k=5,\n",
    "#     embed_model=OpenAIEmbedding(model=\"text-embedding-3-small\"),\n",
    "#     use_async=True,\n",
    "# )\n",
    "retriever = index.as_retriever(\n",
    "    similarity_top_k=10,\n",
    "    use_async=True,\n",
    "    embed_model=OpenAIEmbedding(model=\"text-embedding-3-large\", mode=\"similarity\"),\n",
    "    # embed_model=OpenAIEmbedding(model=\"text-embedding-3-large\", mode=\"text_search\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.data_structs import Node\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "# res = query_engine.query(\"What is the LLaMa model?\")\n",
    "# res.response\n",
    "\n",
    "# query = \"fine-tune a pretrained model\"\n",
    "# query = \"fine-tune an llm\"\n",
    "query = \"how to fine-tune an llm?\"\n",
    "\n",
    "nodes_context = []\n",
    "nodes = retriever.retrieve(query)\n",
    "\n",
    "\n",
    "# # Filter nodes with the same ref_doc_id\n",
    "# def filter_nodes_by_unique_doc_id(nodes):\n",
    "#     unique_nodes = {}\n",
    "#     for node in nodes:\n",
    "#         doc_id = node.node.ref_doc_id\n",
    "#         if doc_id is not None and doc_id not in unique_nodes:\n",
    "#             unique_nodes[doc_id] = node\n",
    "#     return list(unique_nodes.values())\n",
    "\n",
    "\n",
    "# nodes = filter_nodes_by_unique_doc_id(nodes)\n",
    "\n",
    "for node in nodes:\n",
    "    print(\"Node ID\\t\", node.node_id)\n",
    "    print(\"Title\\t\", node.metadata[\"title\"])\n",
    "    print(\"Text\\t\", node.text)\n",
    "    print(\"Score\\t\", node.score)\n",
    "    print(\"Metadata\\t\", node.metadata)\n",
    "    print(\"-_\" * 20)\n",
    "    if node.metadata[\"retrieve_doc\"] == True:\n",
    "        print(\"This node will be replaced by the document\")\n",
    "        doc = document_dict[node.node.ref_doc_id]\n",
    "        # print(doc.text)\n",
    "        new_node = (\n",
    "            NodeWithScore(\n",
    "                node=Node(text=doc.text, metadata=node.metadata), score=node.score\n",
    "            ),\n",
    "        )\n",
    "        nodes_context.append(new_node)\n",
    "    else:\n",
    "        nodes_context.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core.schema import TextNode\n",
    "\n",
    "# for src in res.source_nodes:\n",
    "#     print(src.node.ref_doc_id)\n",
    "#     # print(src.node.get_metadata_str())\n",
    "#     print(\"Node ID\\t\", src.node_id)\n",
    "#     print(\"Title\\t\", src.metadata[\"title\"])\n",
    "#     print(\"Text\\t\", src.text)\n",
    "#     print(\"Score\\t\", src.score)\n",
    "#     print(\"Metadata\\t\", src.metadata)\n",
    "#     print(\"-_\" * 20)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.data_structs import Node\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "from tutor_prompts import (\n",
    "    TEXT_QA_TEMPLATE,\n",
    ")\n",
    "\n",
    "\n",
    "# llm = Gemini(model=\"models/gemini-1.5-flash\", temperature=1, max_tokens=None)\n",
    "# llm = Gemini(model=\"models/gemini-1.5-pro\", temperature=1, max_tokens=None)\n",
    "# llm = OpenAI(temperature=1, model=\"gpt-3.5-turbo\", max_tokens=None)\n",
    "llm = OpenAI(temperature=1, model=\"gpt-4o\", max_tokens=None)\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    llm=llm, response_mode=\"simple_summarize\", text_qa_template=TEXT_QA_TEMPLATE\n",
    ")\n",
    "\n",
    "response = response_synthesizer.synthesize(\n",
    "    query,\n",
    "    nodes=nodes,\n",
    "    # nodes=[\n",
    "    #     NodeWithScore(\n",
    "    #         node=Node(text=\"LLama2 model has a total of 2B parameters.\"), score=1.0\n",
    "    #     ),\n",
    "    # ],\n",
    "    # text_chunks=[\"text1\", \"text2\", \"text3\"],\n",
    ")\n",
    "print(response.response)\n",
    "# for src in response.source_nodes:\n",
    "#     print(src.node.ref_doc_id)\n",
    "#     print(\"Node ID\\t\", src.node_id)\n",
    "#     print(\"Title\\t\", src.metadata[\"title\"])\n",
    "#     print(\"Text\\t\", src.text)\n",
    "#     print(\"Score\\t\", src.score)\n",
    "#     print(\"Metadata\\t\", src.metadata)\n",
    "#     print(\"-_\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import chromadb\n",
    "\n",
    "# # create client and a new collection\n",
    "# # chromadb.EphemeralClient saves data in-memory.\n",
    "# chroma_client = chromadb.PersistentClient(path=\"./ai-tutor-db\")\n",
    "# chroma_collection = chroma_client.create_collection(\"ai-tutor-db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "# from llama_index.core import StorageContext\n",
    "\n",
    "# vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# # Define a storage context object using the created vector store.\n",
    "# storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from llama_index.core.schema import TextNode\n",
    "\n",
    "\n",
    "# def load_jsonl_create_nodes(filepath):\n",
    "#     nodes = []  # List to hold the created node objects\n",
    "#     with open(filepath, \"r\") as file:\n",
    "#         for line in file:\n",
    "#             # Load each line as a JSON object\n",
    "#             json_obj = json.loads(line)\n",
    "#             # Extract required information\n",
    "#             title = json_obj.get(\"title\")\n",
    "#             url = json_obj.get(\"url\")\n",
    "#             content = json_obj.get(\"content\")\n",
    "#             source = json_obj.get(\"source\")\n",
    "#             # Create a TextNode object and append to the list\n",
    "#             node = TextNode(\n",
    "#                 text=content,\n",
    "#                 metadata={\"title\": title, \"url\": url, \"source\": source},\n",
    "#                 excluded_embed_metadata_keys=[\"title\", \"url\", \"source\"],\n",
    "#                 excluded_llm_metadata_keys=[\"title\", \"url\", \"source\"],\n",
    "#             )\n",
    "#             nodes.append(node)\n",
    "#     return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath = \"../combined_data.jsonl\"\n",
    "# nodes = load_jsonl_create_nodes(filepath)\n",
    "\n",
    "# print(f\"Loaded {len(nodes)} nodes/chunks from the JSONL file\\n \")\n",
    "\n",
    "# node = nodes[0]\n",
    "# print(f\"ID: {node.id_} \\nText: {node.text}, \\nMetadata: {node.metadata}\")\n",
    "\n",
    "# print(\"\\n\")\n",
    "\n",
    "# node = nodes[-10000]\n",
    "# print(f\"ID: {node.id_} \\nText: {node.text}, \\nMetadata: {node.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the pipeline to apply the transformation on each chunk,\n",
    "# # and store the transformed text in the chroma vector store.\n",
    "# pipeline = IngestionPipeline(\n",
    "#     transformations=[\n",
    "#         text_splitter,\n",
    "#         QuestionsAnsweredExtractor(questions=3, llm=llm),\n",
    "#         SummaryExtractor(summaries=[\"prev\", \"self\"], llm=llm),\n",
    "#         KeywordExtractor(keywords=10, llm=llm),\n",
    "#         OpenAIEmbedding(),\n",
    "#     ],\n",
    "#     vector_store=vector_store\n",
    "# )\n",
    "\n",
    "# nodes = pipeline.run(documents=documents, show_progress=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "# from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# # embeds = OpenAIEmbedding(model=\"text-embedding-3-small\", mode=\"similarity\")\n",
    "# # embeds = OpenAIEmbedding(model=\"text-embedding-3-large\", mode=\"similarity\")\n",
    "# embeds = OpenAIEmbedding(model=\"text-embedding-3-large\", mode=\"text_search\")\n",
    "# # embeds = OpenAIEmbedding(model=\"text-embedding-ada-002\", mode=\"similarity\")\n",
    "\n",
    "# # Build index / generate embeddings using OpenAI.\n",
    "# index = VectorStoreIndex(\n",
    "#     nodes=nodes,\n",
    "#     show_progress=True,\n",
    "#     use_async=True,\n",
    "#     storage_context=storage_context,\n",
    "#     embed_model=embeds,\n",
    "#     insert_batch_size=3000,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\", max_tokens=None)\n",
    "# query_engine = index.as_query_engine(llm=llm, similarity_top_k=5, embed_model=embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = query_engine.query(\"What is the LLaMa model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for src in res.source_nodes:\n",
    "#     print(\"Node ID\\t\", src.node_id)\n",
    "#     print(\"Title\\t\", src.metadata[\"title\"])\n",
    "#     print(\"Text\\t\", src.text)\n",
    "#     print(\"Score\\t\", src.score)\n",
    "#     print(\"Metadata\\t\", src.metadata)\n",
    "#     print(\"-_\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load DB from disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "\n",
    "# logger = logging.getLogger(__name__)\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "# import chromadb\n",
    "# from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "# # Create your index\n",
    "# db2 = chromadb.PersistentClient(path=\"./ai-tutor-db\")\n",
    "# chroma_collection = db2.get_or_create_collection(\"ai-tutor-db\")\n",
    "# vector_store = ChromaVectorStore(chroma_collection=chroma_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create your index\n",
    "# from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "# from llama_index.llms.openai import OpenAI\n",
    "# from llama_index.core.vector_stores import (\n",
    "#     ExactMatchFilter,\n",
    "#     MetadataFilters,\n",
    "#     MetadataFilter,\n",
    "#     FilterOperator,\n",
    "#     FilterCondition,\n",
    "# )\n",
    "\n",
    "# filters = MetadataFilters(\n",
    "#     filters=[\n",
    "#         MetadataFilter(key=\"source\", value=\"lanchain_course\"),\n",
    "#         MetadataFilter(key=\"source\", value=\"langchain_docs\"),\n",
    "#     ],\n",
    "#     condition=FilterCondition.OR,\n",
    "# )\n",
    "\n",
    "# llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\", max_tokens=None)\n",
    "# embeds = OpenAIEmbedding(model=\"text-embedding-3-large\", mode=\"text_search\")\n",
    "# # query_engine = index.as_query_engine(\n",
    "# #     llm=llm, similarity_top_k=5, embed_model=embeds, verbose=True, streaming=True, filters=filters\n",
    "# # )\n",
    "# query_engine = index.as_query_engine(\n",
    "#     llm=llm,\n",
    "#     similarity_top_k=5,\n",
    "#     embed_model=embeds,\n",
    "#     verbose=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = query_engine.query(\"What is the LLama model?\")\n",
    "\n",
    "# # history = \"\"\n",
    "# # for token in res.response_gen:\n",
    "# #     history += token\n",
    "# #     print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for src in res.source_nodes:\n",
    "#     print(\"Node ID\\t\", src.node_id)\n",
    "#     print(\"Source\\t\", src.metadata[\"source\"])\n",
    "#     print(\"Title\\t\", src.metadata[\"title\"])\n",
    "#     print(\"Text\\t\", src.text)\n",
    "#     print(\"Score\\t\", src.score)\n",
    "#     print(\"-_\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "# # define prompt viewing function\n",
    "# def display_prompt_dict(prompts_dict):\n",
    "#     for k, p in prompts_dict.items():\n",
    "#         text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
    "#         display(Markdown(text_md))\n",
    "#         print(p.get_template())\n",
    "#         display(Markdown(\"<br><br>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts_dict = query_engine.get_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
