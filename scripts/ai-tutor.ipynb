{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create AI-Tutor vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the \"OPENAI_API_KEY\" in the Python environment. Will be used by OpenAI client later.\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.9, model=\"gpt-3.5-turbo\", max_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# create client and a new collection\n",
    "# chromadb.EphemeralClient saves data in-memory.\n",
    "chroma_client = chromadb.PersistentClient(path=\"./ai-tutor-db\")\n",
    "chroma_collection = chroma_client.create_collection(\"ai-tutor-db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "# Define a storage context object using the created vector database.\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "def load_csv_files_from_directory(directory):\n",
    "    nodes = []\n",
    "    node_count = 0\n",
    "\n",
    "    # Iterate over all files in the given directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, mode='r', encoding='utf-8') as file:\n",
    "                csv_reader = csv.reader(file)\n",
    "                headers = next(csv_reader, None)  # Read the header row\n",
    "                \n",
    "                # Dynamically determine the column indices\n",
    "                title_idx = headers.index('title') if 'title' in headers else None\n",
    "                url_idx = headers.index('url') if 'url' in headers else None\n",
    "                content_idx = headers.index('content') if 'content' in headers else None\n",
    "                source_idx = headers.index('source') if 'source' in headers else None\n",
    "                \n",
    "                for row in csv_reader:\n",
    "                    if title_idx is not None and url_idx is not None and content_idx is not None and source_idx is not None:\n",
    "                        node_id = f\"node_{node_count}\"\n",
    "                        node = TextNode(\n",
    "                            text=row[content_idx],\n",
    "                            metadata={\n",
    "                                \"title\": row[title_idx],\n",
    "                                \"url\": row[url_idx],\n",
    "                                \"source\": row[source_idx]\n",
    "                            },\n",
    "                            id_=node_id\n",
    "                        )\n",
    "                        nodes.append(node)\n",
    "                        node_count += 1\n",
    "\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: node_0 \n",
      "Text: # Introduction\n",
      "This lesson will explore the powerful concept of LangChain memory, which is designed to help chatbots maintain context and improve their conversational capabilities in more details. The traditional approach to chatbot development involves processing user prompts independently and without considering the history of interactions. This can lead to disjointed and unsatisfactory user experiences. LangChain provides memory components to manage and manipulate previous chat messages and incorporate them into chains. This is crucial for chatbots, which require remembering the prior interactions. ![ Image by Midjourney](Mastering%20Memory%20Types%20in%20LangChain%20A%20Comprehensiv%209a0515e0407345888439a8c036e47e43/membot.png) Image by Midjourney By default, LLMs are stateless, which means they process each incoming query in isolation, without considering previous interactions. To overcome this limitation, LangChain offers a standard interface for memory, a variety of memory implementations, and examples of chains and agents that employ memory. It also provides Agents that have access to a suite of Tools. Depending on the user’s input, an Agent can decide which Tools to use., \n",
      "Metadata: {'title': 'Mastering Memory Types in LangChain: A Comprehensive Guide with Practical Examples', 'url': 'https://learn.activeloop.ai/courses/take/langchain/multimedia/46318209-mastering-memory-types-in-langchain-a-comprehensive-guide-with-practical-examples', 'source': 'langchain_course'}\n",
      "ID: node_20677 \n",
      "Text: rue (to lift the ambiguity with a batch of sequences).   add_special_tokens (bool, optional, defaults to True) —\n",
      "Whether or not to add special tokens when encoding the sequences. This will use the underlying\n",
      "PretrainedTokenizerBase.build_inputs_with_special_tokens function, which defines which tokens are\n",
      "automatically added to the input ids. This is usefull if you want to add bos or eos tokens\n",
      "automatically.   padding (bool, str or PaddingStrategy, optional, defaults to False) —\n",
      "Activates and controls padding. Accepts the following values:\n",
      "True or 'longest': Pad to the longest sequence in the batch (or no padding if only a single\n",
      "sequence if provided).\n",
      "'max_length': Pad to a maximum length specified with the argument max_length or to the maximum\n",
      "acceptable input length for the model if that argument is not provided.\n",
      "False or 'do_not_pad' (default): No padding (i.e., can output a batch with sequences of different\n",
      "lengths).\n",
      "   truncation (bool, str or Truncation, \n",
      "Metadata: {'title': 'PreTrainedTokenizerFast', 'url': 'https://huggingface.co/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast', 'source': 'hf_transformers'}\n"
     ]
    }
   ],
   "source": [
    "directory_path = '../data/ai-tutor-csv-files'\n",
    "nodes = load_csv_files_from_directory(directory_path)\n",
    "\n",
    "node = nodes[0]\n",
    "print(f\"ID: {node.id_} \\nText: {node.text}, \\nMetadata: {node.metadata}\")\n",
    "\n",
    "node = nodes[-5000]\n",
    "print(f\"ID: {node.id_} \\nText: {node.text}, \\nMetadata: {node.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/omar/Documents/ai_repos/ai-tutor-rag-system/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:01<00:00,  5.27it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:01<00:00,  7.23it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:00<00:00, 10.93it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:01<00:00,  6.51it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:00<00:00, 10.74it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:01<00:00,  9.41it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:01<00:00,  8.36it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:01<00:00,  6.57it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:01<00:00,  7.08it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:01<00:00,  9.90it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:01<00:00,  8.22it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:01<00:00,  6.77it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:01<00:00,  6.02it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:01<00:00,  8.81it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:01<00:00,  7.00it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:01<00:00,  9.67it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:01<00:00,  7.71it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:01<00:00,  9.51it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:00<00:00, 10.10it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:01<00:00,  7.14it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:01<00:00,  7.08it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:01<00:00,  7.79it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:01<00:00,  9.30it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:02<00:00,  4.43it/s]\n",
      "Generating embeddings: 100%|██████████| 10/10 [00:01<00:00,  5.92it/s]\n",
      "Generating embeddings: 100%|██████████| 7/7 [00:00<00:00,  8.70it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Build index / generate embeddings using OpenAI.\n",
    "index = VectorStoreIndex(nodes=nodes, show_progress=True, use_async=True, storage_context=storage_context, embed_model=OpenAIEmbedding(), insert_batch_size=1000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = query_engine.query(\"what can you tell me about the llama2 llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I cannot provide an answer to the query as there is no relevant information or context provided about \"llama2 llm\" in the given text.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID\t node_1708\n",
      "Title\t The Generative AI Revolution: Exploring the Current Landscape\n",
      "Text\t 1. OpenAI's GPT Models Notable Models Task specific models Find model information here: https://platform.openai.com/docs/models/gpt-3 Image & Audio Models OpenAI, the company behind the GPT models, is an AI research and deployment company. The San Francisco-based lab was founded in 2015 as a nonprofit with the goal of building \"artificial general intelligence\" (AGI), which is essentially software as smart as humans. OpenAI conducts innovative research in various fields of AI, such as deep learning, natural language processing, computer vision, and robotics, and develops AI technologies and products intended to solve real-world problems. OpenAI transitioned into a for-profit company in 2019. The company plans to cap the profit of the investors at a fixed multiple of their investment (noted by Sam Altman as currently ranging between 7x and 100x depending on the investment round date and risk). As per the WSJ OpenAI was initially funded by $130m of charity funding (Elon Musk tweeted he contributed $100m) and has since raised at least $13bn led by Microsoft (where OpenAI makes use of Azure cloud credits). With the Microsoft partnership, OpenAI's ChatGPT, along with Microsoft's own search AI, created an improved version of Bing and transformed Microsoft's Office productivity apps. In 2019, OpenAI released GPT-2, a model that could generate realistic human-like text in entire paragraphs with internal consistency, unlike any of the previous models. The next generation, GPT-3, launched in 2020, was trained with 175 billion parameters. GPT-3 is a multi-purpose language tool that users can access without requiring them to learn a programming language or other computer tools. In November 2022, OpenAI released ChatGPT, which is a superior version of the company's earlier text generation models with the capability to generate humanlike prose. After the success of ChatGPT (GPT 3.5), Open AI released GPT-4 in March 2023, which has multimodal capabilities. The model processes both image and text inputs for text generation. The model has a maximum token count of 32,768 capable of generating around 25,000 words as compared to GPT-3.5 which has 4,096 tokens context size. GPT-4 produces 40% more factual responses and its response rate for disallowed content is down by 82% as compared to previous models. (reported by OpenAI) \n",
      "Score\t 0.7294525989858827\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Node ID\t node_19679\n",
      "Title\t TFBartForConditionalGeneration\n",
      "Text\t ach tensor of shape (2, batch_size, num_heads, sequence_length, embed_size_per_head)).\n",
      "Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be\n",
      "used (see past_key_values input) to speed up sequential decoding.\n",
      "decoder_hidden_states (tuple(tf.Tensor), optional, returned when output_hidden_states=True is passed or when config.output_hidden_states=True) — Tuple of tf.Tensor (one for the output of the embeddings + one for the output of each layer) of shape\n",
      "(batch_size, sequence_length, hidden_size).\n",
      "Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.\n",
      "decoder_attentions (tuple(tf.Tensor), optional, returned when output_attentions=True is passed or when config.output_attentions=True) — Tuple of tf.Tensor (one for each layer) of shape (batch_size, num_heads, sequence_length, sequence_length).\n",
      "Attentions weights of the decoder, after the attention softmax, used to com\n",
      "Score\t 0.7243357660195968\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
     ]
    }
   ],
   "source": [
    "for src in res.source_nodes:\n",
    "  print(\"Node ID\\t\", src.node_id)\n",
    "  print(\"Title\\t\", src.metadata['title'])\n",
    "  print(\"Text\\t\", src.text)\n",
    "  print(\"Score\\t\", src.score)\n",
    "  print(\"-_\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load DB from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "# Create your index\n",
    "db2 = chromadb.PersistentClient(path=\"ai-tutor-db\")\n",
    "chroma_collection = db2.get_or_create_collection(\"ai-tutor-db\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your index\n",
    "from llama_index.core import VectorStoreIndex\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = query_engine.query(\"How many parameters LLaMA2 model has?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The LLaMA2 model has 13 billion parameters.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID\t node_3662\n",
      "Source\t towards_ai\n",
      "Title\t Sorting & Analytics Pane in Tableau: A Road to Tableau Desktop Specialist Certification\n",
      "Text\t Sample Certification Questions from this Topic Sorting from field label gives ______ sort by default.a. Nestedb. Non-Nestedc. Manuald. Data Source order Solution: Non-nested \n",
      "Score\t 0.7556534272859884\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "Node ID\t node_16411\n",
      "Source\t hf_transformers\n",
      "Title\t Overview\n",
      "Text\t The LLaMA model was proposed in LLaMA: Open and Efficient Foundation Language Models by Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample. It is a collection of foundation language models ranging from 7B to 65B parameters.\n",
      "The abstract from the paper is the following:\n",
      "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B \n",
      "Score\t 0.72749631299345\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
     ]
    }
   ],
   "source": [
    "for src in res.source_nodes:\n",
    "  print(\"Node ID\\t\", src.node_id)\n",
    "  print(\"Source\\t\", src.metadata['source'])\n",
    "  print(\"Title\\t\", src.metadata['title'])\n",
    "  print(\"Text\\t\", src.text)\n",
    "  print(\"Score\\t\", src.score)\n",
    "  print(\"-_\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
