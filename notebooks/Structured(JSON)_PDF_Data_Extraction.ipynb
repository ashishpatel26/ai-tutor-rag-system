{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/towardsai/ai-tutor-rag-system/blob/main/notebooks/Structured(JSON)_PDF_Data_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rCndQqynkNiQ",
        "outputId": "cf02b7a6-010e-4c20-d871-250c5dd46f0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m679.1/679.1 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.1/375.1 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q google-generativeai==0.5.4 openai tiktoken==0.7.0 llama-index-llms-gemini==0.1.10 arxiv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdAwk6gTsDA3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "Set the \"OPENAI_API_KEY\" in the Python environment. Will be used by OpenAI client later.\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"[OPENAI_API_KEY]\"\n",
        "\n",
        "# from google.colab import userdata\n",
        "# os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api_key')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZpYV-q0--QB"
      },
      "source": [
        "## 1. OpenAI Strucutred output (JSON)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iH1CDNr30dtU"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8702bCYi9xpy"
      },
      "outputs": [],
      "source": [
        "prompt = \"Give me the names of the 10 best-selling books, their authors, the year they were published, and a concise summary in JSON format\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlCRdRTQ9FNh"
      },
      "outputs": [],
      "source": [
        "# The response format- JSON schema\n",
        "response_format_json = {\n",
        "  \"type\": \"json_schema\",\n",
        "  \"json_schema\": {\n",
        "    \"name\": \"Top10BestSellingBooks\",\n",
        "    \"strict\": True,\n",
        "    \"schema\": {\n",
        "      \"type\": \"object\",\n",
        "      \"properties\": {\n",
        "        \"Top10BestSellingBooks\": {\n",
        "          \"type\": \"array\",\n",
        "          \"items\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "              \"title\": { \"type\": \"string\" },\n",
        "              \"author\": { \"type\": \"string\" },\n",
        "              \"yearPublished\": { \"type\": \"integer\" },\n",
        "              \"summary\": { \"type\": \"string\" }\n",
        "            },\n",
        "\n",
        "            \"required\": [\"title\", \"author\", \"yearPublished\", \"summary\"],\n",
        "            \"additionalProperties\": False\n",
        "          }\n",
        "        }\n",
        "      },\n",
        "      \"required\": [\"Top10BestSellingBooks\"],\n",
        "      \"additionalProperties\": False\n",
        "    }\n",
        "  }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kY_AfQvR91Ei",
        "outputId": "576cc28f-3613-412e-9228-c3c520485531"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"Top10BestSellingBooks\":[{\"title\":\"Don Quixote\",\"author\":\"Miguel de Cervantes\",\"yearPublished\":1605,\"summary\":\"This Spanish novel follows the adventures of a nobleman obsessed with chivalric ideals, who sets out to revive knighthood under the name Don Quixote.\"},{\"title\":\"A Tale of Two Cities\",\"author\":\"Charles Dickens\",\"yearPublished\":1859,\"summary\":\"Set in London and Paris before and during the French Revolution, this novel tells the story of Doctor Manette's release from imprisonment in the Bastille and his reunion with his daughter Lucie.\"},{\"title\":\"The Lord of the Rings\",\"author\":\"J.R.R. Tolkien\",\"yearPublished\":1954,\"summary\":\"A high fantasy epic that follows hobbits, elves, and humans as they battle the dark lord Sauron, who wishes to conquer Middle-earth by using the One Ring.\"},{\"title\":\"The Little Prince\",\"author\":\"Antoine de Saint-Exupéry\",\"yearPublished\":1943,\"summary\":\"A young boy who lives on a small asteroid recounts his life story and his travels to Earth, filled with philosophical musings and social criticism.\"},{\"title\":\"Harry Potter and the Philosopher's Stone\",\"author\":\"J.K. Rowling\",\"yearPublished\":1997,\"summary\":\"An orphaned boy learns on his eleventh birthday that he is a wizard and is invited to attend Hogwarts School of Witchcraft and Wizardry.\"},{\"title\":\"And Then There Were None\",\"author\":\"Agatha Christie\",\"yearPublished\":1939,\"summary\":\"Ten strangers are invited to an isolated island, where they are accused of murder. As they start dying one by one, the survivors must discover the killer.\"},{\"title\":\"The Hobbit\",\"author\":\"J.R.R. Tolkien\",\"yearPublished\":1937,\"summary\":\"Bilbo Baggins, a hobbit, joins a group of dwarves on a quest to reclaim their mountain home and the treasure within, guarded by the dragon Smaug.\"},{\"title\":\"Dream of the Red Chamber\",\"author\":\"Cao Xueqin\",\"yearPublished\":1791,\"summary\":\"This classic Chinese novel chronicles the rise and fall of the noble Jia family, offering insights into 18th-century Qing Dynasty life and culture.\"},{\"title\":\"The Lion, the Witch and the Wardrobe\",\"author\":\"C.S. Lewis\",\"yearPublished\":1950,\"summary\":\"Four siblings enter the magical land of Narnia through a wardrobe and help Aslan, the noble lion, to defeat the White Witch who has placed Narnia in eternal winter.\"},{\"title\":\"The Da Vinci Code\",\"author\":\"Dan Brown\",\"yearPublished\":2003,\"summary\":\"Symbologist Robert Langdon and cryptologist Sophie Neveu investigate a murder in the Louvre Museum and uncover a secret society's hidden secrets.\"}]}\n"
          ]
        }
      ],
      "source": [
        "# Making the API call\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-4o-2024-08-06\",\n",
        "  response_format=response_format_json,\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "  ]\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lY4mHR4wQEzq",
        "outputId": "8824b290-7d18-4d8d-e639-92981d345d71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n"
          ]
        }
      ],
      "source": [
        "print(type(response.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_A_XRmV2VkU",
        "outputId": "9d9be26e-137d-4e14-b35a-35074c8da6ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "{'title': 'Don Quixote', 'author': 'Miguel de Cervantes', 'yearPublished': 1605, 'summary': 'This Spanish novel follows the adventures of a nobleman obsessed with chivalric ideals, who sets out to revive knighthood under the name Don Quixote.'}\n",
            "-------------------------------------\n",
            "Don Quixote\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "result_book = json.loads(response.choices[0].message.content)\n",
        "print(type(result_book))\n",
        "print(result_book['Top10BestSellingBooks'][0])\n",
        "print(\"-------------------------------------\")\n",
        "print(result_book['Top10BestSellingBooks'][0]['title'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nISPPdFA1WvX"
      },
      "source": [
        "## 2. Strucutred output from PDF + OpenAI + pdf2images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HGxeZqCQ28bi",
        "outputId": "310d72d2-670c-4183-ac32-5ffff25ffeef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.5 [186 kB]\n",
            "Fetched 186 kB in 0s (704 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 123599 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.5_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U pdf2image\n",
        "!apt-get install poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "file_path = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"rag_research_paper.zip\",repo_type=\"dataset\",local_dir=\"/content\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "fd8136dfdf754412b594fd5f8abc41cf",
            "438eaa8ea12c404087f67bbf0613de3e",
            "9f0c94fe64394a93ba674cd09ca3c78d",
            "0aa6800b59aa4e4290cc16980d398ec5",
            "f9aa26bd4f52432e8f8b1c749f3a958f",
            "bfe657c8b7d643708ce6423db2e377f2",
            "d8c9a4b27a1b43219f115737f77d7391",
            "efa575419ba74580baaf6c1b4f26d76a",
            "e625bd212ec54d11b2c271340c5fe1ab",
            "b03ac91ff010426a8221df081d8b5f7f",
            "db8af4cc3a1745b980fcb5f214b29d2b"
          ]
        },
        "id": "n3QpOuoOiMvB",
        "outputId": "11c575df-93aa-4f5c-8620-50405df728b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:99: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "rag_research_paper.zip:   0%|          | 0.00/7.51M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd8136dfdf754412b594fd5f8abc41cf"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgQt32r8V50j",
        "outputId": "c9496d58-abd2-4bb1-8c5f-2a2e0c30a2cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/rag_research_paper.zip\n",
            "   creating: /content/rag_research_paper/\n",
            "  inflating: /content/rag_research_paper/2405.07437v2.pdf  \n",
            "  inflating: /content/rag_research_paper/2407.01219v1.pdf  \n",
            "  inflating: /content/rag_research_paper/2407.07858v1.pdf  \n",
            "  inflating: /content/rag_research_paper/2407.08223v1.pdf  \n",
            "  inflating: /content/rag_research_paper/2407.16833v1.pdf  \n",
            "  inflating: /content/rag_research_paper/2407.21712v1.pdf  \n",
            "  inflating: /content/rag_research_paper/2408.08067v2.pdf  \n",
            "  inflating: /content/rag_research_paper/2408.08921v1.pdf  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/rag_research_paper.zip -d /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lFWZkMuaZHS",
        "outputId": "791dcf51-087b-43f6-e40d-8e470979efeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/pages/2407.01219v1/page-001.png', '/content/pages/2407.01219v1/page-002.png', '/content/pages/2407.01219v1/page-003.png', '/content/pages/2407.01219v1/page-004.png', '/content/pages/2407.01219v1/page-005.png', '/content/pages/2407.01219v1/page-006.png', '/content/pages/2407.01219v1/page-007.png', '/content/pages/2407.01219v1/page-008.png', '/content/pages/2407.01219v1/page-009.png', '/content/pages/2407.01219v1/page-010.png', '/content/pages/2407.01219v1/page-011.png', '/content/pages/2407.01219v1/page-012.png', '/content/pages/2407.01219v1/page-013.png', '/content/pages/2407.01219v1/page-014.png', '/content/pages/2407.01219v1/page-015.png', '/content/pages/2407.01219v1/page-016.png', '/content/pages/2407.01219v1/page-017.png', '/content/pages/2407.01219v1/page-018.png', '/content/pages/2407.01219v1/page-019.png', '/content/pages/2407.01219v1/page-020.png', '/content/pages/2407.01219v1/page-021.png', '/content/pages/2407.01219v1/page-022.png', '/content/pages/2407.08223v1/page-001.png', '/content/pages/2407.08223v1/page-002.png', '/content/pages/2407.08223v1/page-003.png', '/content/pages/2407.08223v1/page-004.png', '/content/pages/2407.08223v1/page-005.png', '/content/pages/2407.08223v1/page-006.png', '/content/pages/2407.08223v1/page-007.png', '/content/pages/2407.08223v1/page-008.png', '/content/pages/2407.08223v1/page-009.png', '/content/pages/2407.08223v1/page-010.png', '/content/pages/2407.08223v1/page-011.png', '/content/pages/2407.08223v1/page-012.png', '/content/pages/2407.08223v1/page-013.png', '/content/pages/2407.08223v1/page-014.png', '/content/pages/2407.08223v1/page-015.png', '/content/pages/2407.08223v1/page-016.png', '/content/pages/2407.08223v1/page-017.png', '/content/pages/2408.08921v1/page-001.png', '/content/pages/2408.08921v1/page-002.png', '/content/pages/2408.08921v1/page-003.png', '/content/pages/2408.08921v1/page-004.png', '/content/pages/2408.08921v1/page-005.png', '/content/pages/2408.08921v1/page-006.png', '/content/pages/2408.08921v1/page-007.png', '/content/pages/2408.08921v1/page-008.png', '/content/pages/2408.08921v1/page-009.png', '/content/pages/2408.08921v1/page-010.png', '/content/pages/2408.08921v1/page-011.png', '/content/pages/2408.08921v1/page-012.png', '/content/pages/2408.08921v1/page-013.png', '/content/pages/2408.08921v1/page-014.png', '/content/pages/2408.08921v1/page-015.png', '/content/pages/2408.08921v1/page-016.png', '/content/pages/2408.08921v1/page-017.png', '/content/pages/2408.08921v1/page-018.png', '/content/pages/2408.08921v1/page-019.png', '/content/pages/2408.08921v1/page-020.png', '/content/pages/2408.08921v1/page-021.png', '/content/pages/2408.08921v1/page-022.png', '/content/pages/2408.08921v1/page-023.png', '/content/pages/2408.08921v1/page-024.png', '/content/pages/2408.08921v1/page-025.png', '/content/pages/2408.08921v1/page-026.png', '/content/pages/2408.08921v1/page-027.png', '/content/pages/2408.08921v1/page-028.png', '/content/pages/2408.08921v1/page-029.png', '/content/pages/2408.08921v1/page-030.png', '/content/pages/2408.08921v1/page-031.png', '/content/pages/2408.08921v1/page-032.png', '/content/pages/2408.08921v1/page-033.png', '/content/pages/2408.08921v1/page-034.png', '/content/pages/2408.08921v1/page-035.png', '/content/pages/2408.08921v1/page-036.png', '/content/pages/2408.08921v1/page-037.png', '/content/pages/2408.08921v1/page-038.png', '/content/pages/2408.08921v1/page-039.png', '/content/pages/2408.08921v1/page-040.png', '/content/pages/2408.08067v2/page-001.png', '/content/pages/2408.08067v2/page-002.png', '/content/pages/2408.08067v2/page-003.png', '/content/pages/2408.08067v2/page-004.png', '/content/pages/2408.08067v2/page-005.png', '/content/pages/2408.08067v2/page-006.png', '/content/pages/2408.08067v2/page-007.png', '/content/pages/2408.08067v2/page-008.png', '/content/pages/2408.08067v2/page-009.png', '/content/pages/2408.08067v2/page-010.png', '/content/pages/2408.08067v2/page-011.png', '/content/pages/2408.08067v2/page-012.png', '/content/pages/2408.08067v2/page-013.png', '/content/pages/2408.08067v2/page-014.png', '/content/pages/2408.08067v2/page-015.png', '/content/pages/2408.08067v2/page-016.png', '/content/pages/2408.08067v2/page-017.png', '/content/pages/2408.08067v2/page-018.png', '/content/pages/2408.08067v2/page-019.png', '/content/pages/2408.08067v2/page-020.png', '/content/pages/2408.08067v2/page-021.png', '/content/pages/2408.08067v2/page-022.png', '/content/pages/2408.08067v2/page-023.png', '/content/pages/2408.08067v2/page-024.png', '/content/pages/2408.08067v2/page-025.png', '/content/pages/2408.08067v2/page-026.png', '/content/pages/2408.08067v2/page-027.png', '/content/pages/2405.07437v2/page-001.png', '/content/pages/2405.07437v2/page-002.png', '/content/pages/2405.07437v2/page-003.png', '/content/pages/2405.07437v2/page-004.png', '/content/pages/2405.07437v2/page-005.png', '/content/pages/2405.07437v2/page-006.png', '/content/pages/2405.07437v2/page-007.png', '/content/pages/2405.07437v2/page-008.png', '/content/pages/2405.07437v2/page-009.png', '/content/pages/2405.07437v2/page-010.png', '/content/pages/2405.07437v2/page-011.png', '/content/pages/2405.07437v2/page-012.png', '/content/pages/2405.07437v2/page-013.png', '/content/pages/2405.07437v2/page-014.png', '/content/pages/2405.07437v2/page-015.png', '/content/pages/2405.07437v2/page-016.png', '/content/pages/2405.07437v2/page-017.png', '/content/pages/2405.07437v2/page-018.png', '/content/pages/2405.07437v2/page-019.png', '/content/pages/2405.07437v2/page-020.png', '/content/pages/2405.07437v2/page-021.png', '/content/pages/2407.21712v1/page-001.png', '/content/pages/2407.21712v1/page-002.png', '/content/pages/2407.21712v1/page-003.png', '/content/pages/2407.21712v1/page-004.png', '/content/pages/2407.21712v1/page-005.png', '/content/pages/2407.21712v1/page-006.png', '/content/pages/2407.21712v1/page-007.png', '/content/pages/2407.21712v1/page-008.png', '/content/pages/2407.21712v1/page-009.png', '/content/pages/2407.21712v1/page-010.png', '/content/pages/2407.21712v1/page-011.png', '/content/pages/2407.21712v1/page-012.png', '/content/pages/2407.07858v1/page-001.png', '/content/pages/2407.07858v1/page-002.png', '/content/pages/2407.07858v1/page-003.png', '/content/pages/2407.07858v1/page-004.png', '/content/pages/2407.07858v1/page-005.png', '/content/pages/2407.07858v1/page-006.png', '/content/pages/2407.07858v1/page-007.png', '/content/pages/2407.07858v1/page-008.png', '/content/pages/2407.16833v1/page-001.png', '/content/pages/2407.16833v1/page-002.png', '/content/pages/2407.16833v1/page-003.png', '/content/pages/2407.16833v1/page-004.png', '/content/pages/2407.16833v1/page-005.png', '/content/pages/2407.16833v1/page-006.png', '/content/pages/2407.16833v1/page-007.png', '/content/pages/2407.16833v1/page-008.png', '/content/pages/2407.16833v1/page-009.png', '/content/pages/2407.16833v1/page-010.png', '/content/pages/2407.16833v1/page-011.png', '/content/pages/2407.16833v1/page-012.png']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pdf2image import convert_from_path\n",
        "\n",
        "pdf_directory = \"/content/rag_research_paper\"\n",
        "output_dir = \"/content/pages\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "pages_png = []\n",
        "\n",
        "for pdf_file in os.listdir(pdf_directory):\n",
        "    if pdf_file.endswith(\".pdf\"):\n",
        "        pdf_path = os.path.join(pdf_directory, pdf_file)\n",
        "\n",
        "        convert = convert_from_path(pdf_path, use_pdftocairo=True)\n",
        "\n",
        "        pdf_output_dir = os.path.join(output_dir, os.path.splitext(pdf_file)[0])\n",
        "        os.makedirs(pdf_output_dir, exist_ok=True)\n",
        "\n",
        "        for page_num, image in enumerate(convert):\n",
        "            page_filename = f\"page-{str(page_num + 1).zfill(3)}.png\"\n",
        "            full_path = os.path.join(pdf_output_dir, page_filename)\n",
        "            image.save(full_path)\n",
        "\n",
        "            pages_png.append(full_path)\n",
        "\n",
        "print(pages_png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eo5qbcZ6-IHN"
      },
      "outputs": [],
      "source": [
        "from io import BytesIO\n",
        "import base64\n",
        "import json\n",
        "\n",
        "# Function to encode the image\n",
        "def encode_image(image_path):\n",
        "  with open(image_path, \"rb\") as image_file:\n",
        "    return base64.b64encode(image_file.read()).decode('utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPKc_1JkwQfd"
      },
      "outputs": [],
      "source": [
        "# The response format- JSON schema\n",
        "json_response_format = {\n",
        "  \"type\": \"json_schema\",\n",
        "  \"json_schema\": {\n",
        "    \"name\": \"research_paper_data\",\n",
        "    \"strict\": True,\n",
        "    \"schema\": {\n",
        "      \"type\": \"object\",\n",
        "      \"properties\": {\n",
        "        \"research_paper_data\": {\n",
        "          \"type\": \"array\",\n",
        "          \"items\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"name\": { \"type\": \"string\" },\n",
        "                \"source\": { \"type\": \"string\" },\n",
        "                \"content\": { \"type\": \"string\"},\n",
        "\n",
        "            },\n",
        "\n",
        "            \"required\": [\"name\", \"source\", \"content\"],\n",
        "            \"additionalProperties\": False\n",
        "          }\n",
        "        },\n",
        "      },\n",
        "      \"required\": [\"research_paper_data\"],\n",
        "      \"additionalProperties\": False\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "system_instruction_prompt = \"\"\" You are an expert in Structured data extraction.\n",
        "Task Description:\n",
        "You are provided with an image that is a page from a research paper in PDF format. Your task is to extract all the relevant information from this\n",
        "image. This includes all headlines, subheadlines, section headlines, and the content within those sections, as well as detailed information about\n",
        "any figures, graphs, architectures, tables, and charts mentioned or displayed on the page. Include as much as content in one Json content output. Don't fragement the information.\n",
        "Must consider note: Each generated response particluarly extracted content must have minimum of 100 words.\n",
        "Extraction Requirements:\n",
        "  1. (source) Research Paper ID: Identify and extract the research paper's Arxiv ID. This can often be found on the first page, typically on the left-hand side written from bottom to top.\n",
        "    The format should be similar to an arXiv ID (e.g., \"arXiv:2405.07437v2\", \"arXiv:2105.07327v1\"). Important: Include the prefix \"arXiv:\" in the extracted\n",
        "    ID (e.g., extract it as \"arXiv:2105.07327v1\", not \"2105.07327v1\").  **Please make sure one or more times the extracted Arxiv ID is correct or not.**\n",
        "\n",
        "  2. (name) Headlines and Subheadlines: Extract all primary and secondary headlines, which may include sections like: Abstract, Introduction, Evaluation, Conclusion,\n",
        "  References, etc. This also applies to subheadlines or subsection titles within those major sections. If the image lacks explicit headlines, create them based on the content.\n",
        "  3. Content Extraction: Extract all relevant content under each headline, subheadline. Ensure the complete text under each section is captured,\n",
        "  including paragraphs, lists, and bullet points. The content must also cover descriptions and information about any figures, graphs, architectures, tables,\n",
        "  and charts present in the image.\n",
        "    4.1. For each figure or graph, extract the following:\n",
        "        - Caption or title (if provided).\n",
        "        - Description of the figure, graph, or table.\n",
        "        - Insights, trends, or comparisons described within the content.\n",
        "        - Architecture designs or layouts shown, along with detailed analysis if available.\n",
        "\n",
        "Handling Missing Headlines:\n",
        "If the image does not include a visible headline, subheadline, or title, generate one based on the content. And Include all the content in that geadline.\n",
        " For example, if a section describes an evaluation method, create a headline like \"Evaluation\" and Include the mentioned content.\n",
        "\n",
        "Figures, Graphs, and Tables: For any figures, graphs, architectures, tables, or charts present: Extract the title or caption. Take the main points, trends, or comparisons shown. Provide descriptions of\n",
        "any architecture designs or layouts. Capture insights or observations related to the figure/chart that are mentioned in the text.\n",
        "\n",
        "Important Notes:\n",
        "- Fragmented Content: In cases where the text or content is fragmented (split across multiple parts), ensure the complete information is extracted and merged logically.\n",
        "- Accuracy: Ensure that all extracted content is precise and complete, especially for any complex figures, tables, or architectures.\n",
        "- No Summarization: Do not condense paragraphs or reduce the length of sentences. Extract each sentence and figure-related insight in full detail\n",
        "without cutting or omitting any part of the content.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcyxZ3hee4O2"
      },
      "outputs": [],
      "source": [
        "import arxiv\n",
        "import re\n",
        "\n",
        "def arxiv_extraction(arxiv_id):\n",
        "  client = arxiv.Client()\n",
        "  search = arxiv.Search(id_list=re.findall(r'(\\d{4}\\.\\d{5}|\\w+(?:-\\w+)?/\\d{7})', arxiv_id), max_results=1)\n",
        "  results = client.results(search)\n",
        "\n",
        "  for result in results:\n",
        "    return result.title, result.pdf_url\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrpPrN3vv6dk"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "desc = []\n",
        "for page in pages_png:\n",
        "  # Getting the base64\n",
        "  base64_image = encode_image(page)\n",
        "\n",
        "  try:\n",
        "    system_prompt = {\"role\": \"system\",\"content\":system_instruction_prompt}\n",
        "    user_prompt = {\"role\": \"user\",\"content\": [{\"type\": \"text\", \"text\": \"Please extract the content from this research paper page image.\"},\n",
        "                                              {\"type\": \"image_url\", \"image_url\": {\"url\":f\"data:image/jpeg;base64,{base64_image}\",\"detail\": \"high\"}}]}\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "      model=\"gpt-4o-2024-08-06\",\n",
        "      response_format=json_response_format,\n",
        "      messages= [system_prompt,user_prompt],)\n",
        "\n",
        "    if response.choices[0].message.content is None:\n",
        "      continue\n",
        "\n",
        "    result = json.loads(response.choices[0].message.content)\n",
        "\n",
        "    if 'page-001' in page:\n",
        "      research_paper_id = result['research_paper_data'][0]['source']\n",
        "      research_paper_title,research_paper_url = arxiv_extraction(research_paper_id)\n",
        "      for i in range(len(result['research_paper_data'])):\n",
        "        result['research_paper_data'][i]['source'] = research_paper_id\n",
        "        result['research_paper_data'][i]['name'] = research_paper_title +\":\"+ result['research_paper_data'][i]['name']\n",
        "        result['research_paper_data'][i]['url'] = research_paper_url\n",
        "\n",
        "    if 'page-001' not in page:\n",
        "      for i in range(len(result['research_paper_data'])):\n",
        "        result['research_paper_data'][i]['source'] = research_paper_id\n",
        "        result['research_paper_data'][i]['name'] = research_paper_title +\":\"+ result['research_paper_data'][i]['name']\n",
        "        result['research_paper_data'][i]['url'] = research_paper_url\n",
        "\n",
        "    desc.extend(result['research_paper_data'])\n",
        "\n",
        "  except Exception as e:\n",
        "    print(response.choices[0].finish_reason)\n",
        "    print(f\"Skipping {page}... error: {e}\")\n",
        "    continue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPlZyDWXfJhx",
        "outputId": "02dff9f1-e33d-48f8-deda-719d380d8b01"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "359"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(desc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNd7zjLZmHU_",
        "outputId": "778eb632-acba-4f4d-c513-fb6d263643c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content research paper title and Headline : Searching for Best Practices in Retrieval-Augmented Generation:Retriever and Generator Fine-tuning \n",
            "\n",
            "Content : Fine-tuning within the RAG framework is crucial for optimizing both retrievers and generators. Some research focuses on fine-tuning the generator to better utilize retriever context [30–32], ensuring faithful and robust generated content. Others fine-tune the retriever to learn to retrieve beneficial passages for the generator [33–35]. Holistic approaches treat RAG as an integrated system; fine-tuning both retriever and generator together to enhance overall performance [36–38], despite increased complexity and integration challenges. Several surveys have extensively discussed current RAG systems, covering aspects like text generation [7, 8], integration with LLMs [6, 39], multimodal [40], and AI-generated content [41]. While these surveys provide comprehensive overviews of existing RAG methodologies, selecting the appropriate approaches involves balancing performance and complexity. \n",
            "\n",
            "Source : Arxiv: 2407.01219 \n",
            "\n",
            "URL : http://arxiv.org/pdf/2407.01219v1\n"
          ]
        }
      ],
      "source": [
        "print(\"Content research paper title and Headline :\",desc[6]['name'],\"\\n\")\n",
        "print(\"Content :\",desc[6]['content'],\"\\n\")\n",
        "print(\"Source :\",desc[6]['source'],\"\\n\")\n",
        "print(\"URL :\",desc[6]['url'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source=[]\n",
        "for i in range(len(desc)):\n",
        "  source.append(desc[i]['source'])\n",
        "\n",
        "source=list(set(source))\n",
        "source"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcIr06B_8HG6",
        "outputId": "f54edbd4-9c8a-4b39-f605-d19cc7de7c9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Arxiv: 2407.08223',\n",
              " 'Arxiv: 2407.16833',\n",
              " 'Arxiv: 2408.08067',\n",
              " 'Arxiv: 2407.21712',\n",
              " 'Arxiv: 2407.01219',\n",
              " 'Arxiv: 2408.08921',\n",
              " 'Arxiv: 2407.07858',\n",
              " 'Arxiv: 2405.07437']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P64zuFRHub8H"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open(\"/content/ai_tutor_500.jsonl\", \"r\") as file:\n",
        "    result = [json.loads(line) for line in file]\n",
        "\n",
        "result[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3moEdG_EpX7",
        "outputId": "91b1a7aa-65da-47c1-c65d-4e1d3c3e73e5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'tokens': 3031,\n",
              " 'doc_id': '60deb74f-d8b5-47a6-93f2-425887a46e33',\n",
              " 'name': 'Named Entity Recognition in Ecommerce Industry  Custom model [Github Repo]  03/07/24',\n",
              " 'url': 'https://towardsai.net/p/machine-learning/named-entity-recognition-in-ecommerce-industry-custom-model-github-repo-03-07-24',\n",
              " 'source': 'tai_blog',\n",
              " 'content': \"Github Repo: https://github.com/vaibhawkhemka/ML-Umbrella/tree/main/NLP/Product-Categorization   From e-commerce to Customer support  all businesses require some kind of NER model to process huge amounts of texts from users.   To automate this whole  one requires NER models to extract relevant and important entities from text.   Final Result/OutputInput text = EL D68 (Green  32 GB) 3 GB RAM [3 GB RAM U+007C 32 GB ROM U+007C Expandable Upto 128 GB  15.46 cm (6.088 inch) Display  13MP Rear Camera U+007C 8MP Front Camera  4000 mAh Battery  Quad-Core Processor]   Output =   Green ->>>> COLOR 32 GB ->>>> STORAGE 3 GB RAM ->>>> RAM 3 GB RAM ->>>> RAM 32 GB ROM ->>>> STORAGE Expandable Upto 128 GB ->>>> EXPANDABLE_STORAGE 15.46 cm (6.088 inch) ->>>> SCREEN_SIZE 13MP Rear Camera ->>>> BACK_CAMERA 8MP Front Camera ->>>> FRONT_CAMERA 4000 mAh Battery ->>>> BATTERY_CAPACITY Quad-Core Processor ->>>> PROCESSOR_CORE   Data PreparationA tool for creating this dataset (https://github.com/tecoholic/ner-annotator)    Snapshot for the dataset for Mobile phone product description on Amazon:   A single record of the Data:   Converting into proper Spacy span format:The proper format that Spacy Ner model understands   import jsonlines  json file_path = Training Data/Mobile/Mobile_training.jsonl laptop_classes = [RAM STORAGE BATTERY CAPACITY PROCESSOR_TYPE SCREEN_SIZE REFRESH_RATE SCREEN_TYPE BACK_CAMERA FRONT_CAMERA] with jsonlines.open(file_path) as reader: output_json = {classes: laptop_classes  annotations: []} # Iterate over each line (JSON object) for obj in reader: processed_obj = [obj[text] {entities:obj[label]}] output_json[annotations].append(processed_obj) # Save the output JSON to a new file with open('Training Data/Mobile/Mobile_annotations.json'  'w') as f: json.dump(output_json  f  indent=None)Above is the code for converting into proper data format. Check out jupyter notebook: NER_model_Mobile.ipynb   Final pandas dataframe from processed data:   Splitting the dataset  10% test### Split the data from sklearn.model_selection import train_test_split train  test = train_test_split(df  test_size=0.1) train.head()Create spacy DocBin objects from annotated data to train Spacy NER model:import spacy from spacy.tokens import DocBin from tqdm import tqdm # Define a function to create spaCy DocBin objects from the annotated data def get_spacy_doc(data): # Create a blank spaCy pipeline nlp = spacy.blank('en') db = DocBin() # Initialize a counter for None spans none_spans = 0 spans = 0 for index  row in data.iterrows(): # Get the text and annotations text = row[Description] annotations = row[Annotations] # Check if the text is not empty if not text: continue # Process the text and annotations doc = nlp(text) if doc is None: print(fFailed to process text: {text}) continue ents = [] for start  end  label in annotations: if start < 0 or end < 0: print(fInvalid annotation: {start}  {end}  {label}) continue #print(text) span = doc.char_span(start  end  label=label) if span is None: print(fFailed to create span for annotation: {start}  {end}  {label}) none_spans += 1 continue else: spans+=1 ents.append(span) doc.ents = ents #Add the processed document to the DocBin db.add(doc) print(fNumber of None spans: {none_spans}) print(fNumber of spans: {spans}) return dbModellingArchitecture:The basic architecture for all spacy models:   Reference: https://explosion.ai/blog/deep-learning-formula-nlp   [Embed]HashEmbed  Sub-word features than character based richer representation and arbitrary sized vocabulary  Can use Word2vec/Glove etc   [Encode]  Context-independent to context-dependent using LSTM or CNN.   [Attend]  Attention mechanism by Key  Value pair  and context vectors   [Predict]  MLP   Tok2vec model [example]:   https://github.com/explosion/spaCy/blob/master/spacy/ml/models/tok2vec.py (Built using thinc framework)   NER Model  Transition-Based:   State(all three stack  buffer  and output) and Action   Structure Prediction.   The above shows how the transition-based approach works with stack  buffer  output  and Transition/action.   Reference: https://www.microsoft.com/en-us/research/video/transition-based-natural-language-processing/   The above shows How stacked LSTM works for encoding for all states and actions.   The final Prediction from MLP is the Multiclassification task with labels as SHIFT  OUT  and REDUCE   Spacy model layer and Config Mapping:   Example of a tok2vec config:   Model in thinc framework:   Respective config for the model:   Thinc deep learning framework is used as a backend to build spacy models instead of pytorch or TensorFlow.   Difference between normal pytorch and spacy models. => Spacy(easy  reliable and productionable)   The user can define and create this model using a configuration file for any task: NER  Tok2Vec  Tagger  Dependency Parser  Sentiment etc   One can also create thinc models and wrap around pytorch and TensorFlow. I will build it next blog.   NER Config file created here:   Reference: https://spacy.io/usage/training   config_ner.cfg :   [paths] train = null dev = null vectors = en_core_web_lg init_tok2vec = null [system] gpu_allocator = null seed = 0 [nlp] lang = en pipeline = [tok2vec ner] batch_size = 1000 disabled = [] before_creation = null after_creation = null after_pipeline_creation = null tokenizer = {@tokenizers:spacy.Tokenizer.v1} vectors = {@vectors:spacy.Vectors.v1} [components] [components.ner] factory = ner incorrect_spans_key = null moves = null scorer = {@scorers:spacy.ner_scorer.v1} update_with_oracle_cut_size = 100 [components.ner.model] @architectures = spacy.TransitionBasedParser.v2 state_type = ner extra_state_tokens = false hidden_width = 64 maxout_pieces = 2 use_upper = true nO = null [components.ner.model.tok2vec] @architectures = spacy.Tok2VecListener.v1 width = ${components.tok2vec.model.encode.width} upstream = * [components.tok2vec] factory = tok2vec [components.tok2vec.model] @architectures = spacy.Tok2Vec.v2 [components.tok2vec.model.embed] @architectures = spacy.MultiHashEmbed.v2 width = ${components.tok2vec.model.encode.width} attrs = [NORM PREFIX SUFFIX SHAPE] rows = [5000 1000 2500 2500] include_static_vectors = true [components.tok2vec.model.encode] @architectures = spacy.MaxoutWindowEncoder.v2 width = 256 depth = 8 window_size = 1 maxout_pieces = 3 [corpora] [corpora.dev] @readers = spacy.Corpus.v1 path = ${paths.dev} max_length = 0 gold_preproc = false limit = 0 augmenter = null [corpora.train] @readers = spacy.Corpus.v1 path = ${paths.train} max_length = 0 gold_preproc = false limit = 0 augmenter = null [training] dev_corpus = corpora.dev train_corpus = corpora.train seed = ${system.seed} gpu_allocator = ${system.gpu_allocator} dropout = 0.1 accumulate_gradient = 1 patience = 1600 max_epochs = 0 max_steps = 20000 eval_frequency = 200 frozen_components = [] annotating_components = [] before_to_disk = null before_update = null [training.batcher] @batchers = spacy.batch_by_words.v1 discard_oversize = false tolerance = 0.2 get_length = null [training.batcher.size] @schedules = compounding.v1 start = 100 stop = 1000 compound = 1.001 t = 0.0 [training.logger] @loggers = spacy.ConsoleLogger.v1 progress_bar = false [training.optimizer] @optimizers = Adam.v1 beta1 = 0.9 beta2 = 0.999 L2_is_weight_decay = true L2 = 0.01 grad_clip = 1.0 use_averages = false eps = 0.00000001 learn_rate = 0.001 [training.score_weights] ents_f = 1.0 ents_p = 0.0 ents_r = 0.0 ents_per_type = null [pretraining] [initialize] vectors = ${paths.vectors} init_tok2vec = ${paths.init_tok2vec} vocab_data = null lookups = null before_init = null after_init = null [initialize.components] [initialize.tokenizer]Output and Evaluation:Evaluation is done based on ENTS_P(Precision)  ENTS_R(Recall) and ENTS_F (F-Score).   After the 15th epoch Final ENTS_F is 57.64  which can be improved by providing more data for this case.   Intuition for Evaluation:We evaluate the NER model based on Span-Identification and Span-Prediction.   Span-Identification:   https://cees-roele.medium.com/custom-evaluation-of-spans-in-spacy-f1f2e7a99ad8   As discussed  NER is a multiclass Classification problem with SHIFT  OUT  and REDUCE as output. But we evaluate our models only based on REDUCE.   The above picture shows how Precision  Recall  and F-Score are calculated.   The code used for evaluating PRF (Precision-Recall-Fscore) by spacy:   def get_ner_prf(examples: Iterable[Example]  **kwargs) -> Dict[str  Any]: Compute micro-PRF and per-entity PRF scores for a sequence of examples. score_per_type = defaultdict(PRFScore) for eg in examples: if not eg.y.has_annotation(ENT_IOB): continue golds = {(e.label_  e.start  e.end) for e in eg.y.ents} align_x2y = eg.alignment.x2y for pred_ent in eg.x.ents: if pred_ent.label_ not in score_per_type: score_per_type[pred_ent.label_] = PRFScore() indices = align_x2y[pred_ent.start : pred_ent.end] if len(indices): g_span = eg.y[indices[0] : indices[-1] + 1] # Check we aren't missing annotation on this span. If so  # our prediction is neither right nor wrong  we just # ignore it. if all(token.ent_iob != 0 for token in g_span): key = (pred_ent.label_  indices[0]  indices[-1] + 1) if key in golds: score_per_type[pred_ent.label_].tp += 1 golds.remove(key) else: score_per_type[pred_ent.label_].fp += 1 for label  start  end in golds: score_per_type[label].fn += 1 totals = PRFScore() for prf in score_per_type.values(): totals += prf if len(totals) > 0: return { ents_p: totals.precision  ents_r: totals.recall  ents_f: totals.fscore  ents_per_type: {k: v.to_dict() for k  v in score_per_type.items()}  } else: return { ents_p: None  ents_r: None  ents_f: None  ents_per_type: None  }Reference: https://github.com/explosion/spaCy/blob/master/spacy/scorer.py#L760   Span Prediction :   There are 9 different entires like [RAM  STORAGE  BATTERY CAPACITY  PROCESSOR_TYPE  SCREEN_SIZE  REFRESH_RATE  SCREEN_TYPE  BACK_CAMERA  FRONT_CAMERA] to predict for REDUCE class.   It uses categorical crossentropy loss function to optimize NER models (More details in later blogs)   Testing and Final Results:Input text = EL D68 (Green  32 GB) 3 GB RAM [3 GB RAM U+007C 32 GB ROM U+007C Expandable Upto 128 GB  15.46 cm (6.088 inch) Display  13MP Rear Camera U+007C 8MP Front Camera  4000 mAh Battery  Quad-Core Processor]   Output =   Green ->>>> COLOR 32 GB ->>>> STORAGE 3 GB RAM ->>>> RAM 3 GB RAM ->>>> RAM 32 GB ROM ->>>> STORAGE Expandable Upto 128 GB ->>>> EXPANDABLE_STORAGE 15.46 cm (6.088 inch) ->>>> SCREEN_SIZE 13MP Rear Camera ->>>> BACK_CAMERA 8MP Front Camera ->>>> FRONT_CAMERA 4000 mAh Battery ->>>> BATTERY_CAPACITY Quad-Core Processor ->>>> PROCESSOR_CORE   Github Link: https://github.com/vaibhawkhemka/ML-Umbrella/tree/main/NLP/Product-Categorization   Thanks for reading the blog.   If you have any questions  hit me up on my LinkedIn: https://www.linkedin.com/in/vaibhaw-khemka-a92156176/   References for modeling:   https://explosion.ai/blog/deep-learning-formula-nlp => Embed  Encode  Attend and Predict => Position is imp in sequence in text.   https://support.prodi.gy/t/spacy-ner-models-architecture-details/4336   https://github.com/explosion/spaCy/blob/master/spacy/ml/models/tok2vec.py   https://spacy.io/usage/layers-architectures   https://spacy.io/api/architectures#CharacterEmbed   Understanding span:   https://spacy.io/api/span\"}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result[1]['tokens']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHWri74kx-22",
        "outputId": "6cf9d7aa-2dff-4250-acc2-79d67866da5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3031"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import uuid\n",
        "\n",
        "def token_encoding_and_doc_id(content):\n",
        "  encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "  tokens = encoding.encode(content)\n",
        "\n",
        "  num_tokens = len(tokens)\n",
        "  doc_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, content))\n",
        "\n",
        "  return num_tokens, doc_id\n",
        "\n",
        "for i in range(len(desc)):\n",
        "  desc[i]['tokens'], desc[i]['doc_id'] = token_encoding_and_doc_id(desc[i]['content'])\n",
        "\n",
        "for i in range(len(result)):\n",
        "  result[i]['tokens'], result[i]['doc_id'] = token_encoding_and_doc_id(result[i]['content'])"
      ],
      "metadata": {
        "id": "qMMsh0DsmTJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ai_tutor_knowledge = result + desc"
      ],
      "metadata": {
        "id": "ScadYbE-xnl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ai_tutor_knowledge[850]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cx_w2XckxpFo",
        "outputId": "479f4b22-f639-46e8-ebb6-68f943d7866a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach:References',\n",
              " 'source': 'Arxiv: 2407.16833',\n",
              " 'content': 'Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom Sorsokin, and Mikhail Burtsev. 2024. In search of needles in a 10m haystack: Recurrent memory finds what lms miss. arXiv preprint arXiv:2402.10790.',\n",
              " 'url': 'http://arxiv.org/pdf/2407.16833v1',\n",
              " 'tokens': 74,\n",
              " 'doc_id': '55194ca6-3fcc-50ba-914f-e07577bc9973'}"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ai_tutor_knowledge = [d for d in ai_tutor_knowledge if d['tokens'] > 100]"
      ],
      "metadata": {
        "id": "W4U1ct2h4a2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/ai_tutor_knowledge.jsonl', 'w') as f:\n",
        "    for item in ai_tutor_knowledge:\n",
        "        f.write(json.dumps(item) + '\\n')\n",
        "\n",
        "print(\"JSONL file saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvEaPAe9Lir2",
        "outputId": "be25dfed-d5c8-49e5-d947-834174534070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSONL file saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/ai_tutor_knowledge.jsonl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "B4RWqf0bdHnq",
        "outputId": "a152c05d-9738-492e-9c2d-18c4132e1eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_94c8d77b-6bf8-43ca-b85d-26190f911f4a\", \"ai_tutor_knowledge.jsonl\", 7166796)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFdK63RdjaHr"
      },
      "source": [
        "------------"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkJB2HfCZd8ziqt6woqA7F",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fd8136dfdf754412b594fd5f8abc41cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_438eaa8ea12c404087f67bbf0613de3e",
              "IPY_MODEL_9f0c94fe64394a93ba674cd09ca3c78d",
              "IPY_MODEL_0aa6800b59aa4e4290cc16980d398ec5"
            ],
            "layout": "IPY_MODEL_f9aa26bd4f52432e8f8b1c749f3a958f"
          }
        },
        "438eaa8ea12c404087f67bbf0613de3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfe657c8b7d643708ce6423db2e377f2",
            "placeholder": "​",
            "style": "IPY_MODEL_d8c9a4b27a1b43219f115737f77d7391",
            "value": "rag_research_paper.zip: 100%"
          }
        },
        "9f0c94fe64394a93ba674cd09ca3c78d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efa575419ba74580baaf6c1b4f26d76a",
            "max": 7514652,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e625bd212ec54d11b2c271340c5fe1ab",
            "value": 7514652
          }
        },
        "0aa6800b59aa4e4290cc16980d398ec5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b03ac91ff010426a8221df081d8b5f7f",
            "placeholder": "​",
            "style": "IPY_MODEL_db8af4cc3a1745b980fcb5f214b29d2b",
            "value": " 7.51M/7.51M [00:01&lt;00:00, 4.29MB/s]"
          }
        },
        "f9aa26bd4f52432e8f8b1c749f3a958f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfe657c8b7d643708ce6423db2e377f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8c9a4b27a1b43219f115737f77d7391": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "efa575419ba74580baaf6c1b4f26d76a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e625bd212ec54d11b2c271340c5fe1ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b03ac91ff010426a8221df081d8b5f7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db8af4cc3a1745b980fcb5f214b29d2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}