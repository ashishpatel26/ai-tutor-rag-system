{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/towardsai/ai-tutor-rag-system/blob/main/notebooks/17-Using_LLMs_to_rank_chunks_as_the_Judge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FbELaf7TrW7"
      },
      "source": [
        "# Install Packages and Setup Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yubz8AanRRSW",
        "outputId": "249b37da-5846-47e3-d921-056d636e7477",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.4/327.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m559.5/559.5 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m679.1/679.1 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.8/130.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for durationpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q huggingface_hub llama-index==0.10.49 llama-index-vector-stores-chroma==0.1.9 llama-index-llms-gemini==0.1.11 google-generativeai==0.5.4 openai==1.35.3 chromadb==0.5.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLXFuRW-TpUu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set the following API Keys in the Python environment. Will be used later.\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"[OPENAI_API_KEY]\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"<YOUR_API_KEY>\"\n",
        "\n",
        "# from google.colab import userdata\n",
        "# os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api_key')\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = userdata.get('Google_api_key')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6GCYYqqTuMc"
      },
      "source": [
        "# Load a Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pupJpdZaTu5m"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core import Settings\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "Settings.llm = OpenAI(temperature=1, model=\"gpt-4o-mini\")\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSGHsZMMZj4E"
      },
      "source": [
        "# Load Indexes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading vector store from Huggingface hub"
      ],
      "metadata": {
        "id": "z1lRMQtdOQ95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "vectorstore = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"vectorstore.zip\",repo_type=\"dataset\",local_dir=\"/content\")"
      ],
      "metadata": {
        "id": "o8Riolm_2iYd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1f0ca3bb1bb14be18aac2e2292faed69",
            "caec106d984d4aebaca8efe174430c9f",
            "a8b03b451cd348c88bf861352ac78525",
            "1053d43951a4441d9c3074f6ef97bf24",
            "2ec86a62bec34f1082eb1a3513e8e85f",
            "a366c17411de4de59dbb7bc03ca9b4d3",
            "cd5ad3ebd4264d69be2c6f903c5d0fe0",
            "330c90562b6a49b6a9eac2f6c33e575d",
            "e856c40a387b4ee39a39b3e0deaf50df",
            "747bfa2657d64e61a3bc7586a27f89ac",
            "fdfd90f122ba4147a7624c8d71fd1886"
          ]
        },
        "outputId": "0f544889-ab54-4f41-ee39-16d841f75c07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vectorstore.zip:   0%|          | 0.00/97.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f0ca3bb1bb14be18aac2e2292faed69"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8iaOOGyZeNp",
        "outputId": "75a96f07-a0f0-4fc5-df9b-f2de4d81ae94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/vectorstore.zip\n",
            "   creating: ai_tutor_knowledge/\n",
            "   creating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/\n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/length.bin  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/index_metadata.pickle  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/link_lists.bin  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/header.bin  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/data_level0.bin  \n",
            "  inflating: ai_tutor_knowledge/chroma.sqlite3  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/vectorstore.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tzS_EKPZeLS"
      },
      "outputs": [],
      "source": [
        "# Load the vector store from the local storage.\n",
        "import chromadb\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "\n",
        "db2 = chromadb.PersistentClient(path=\"/content/ai_tutor_knowledge\")\n",
        "chroma_collection = db2.get_or_create_collection(\"ai_tutor_knowledge\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0T6FL7J3ZrNK"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "# Create the index based on the vector store.\n",
        "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine(similarity_top_k=10)\n",
        "\n",
        "res = query_engine.query(\"Explain how Advance RAG works?\")"
      ],
      "metadata": {
        "id": "b8OilXIR8IO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res.response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "8RMKJJv98R2o",
        "outputId": "cf3547a0-444e-4857-c983-2b4af9341d43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Advance RAG refers to enhanced approaches within Retrieval Augmented Generation (RAG) systems, specifically focusing on methodologies like Self-Reflective RAG (Self-RAG), Corrective RAG (CRAG), and SPECULATIVE RAG.\\n\\n1. **Self-Reflective RAG (Self-RAG)**: This method involves instruction-tuning a large language model (LLM) to generate self-reflection tags. These tags guide the model in dynamically retrieving documents and assessing their relevance before generating responses.\\n\\n2. **Corrective RAG (CRAG)**: This approach employs an external evaluator to assess and refine the quality of retrieved documents prior to the generation process. It focuses on ensuring that the contextual information used during generation is of high quality.\\n\\n3. **SPECULATIVE RAG**: A proposed approach that further enhances RAG by employing a smaller RAG drafter model to generate multiple answer drafts based on subsets of retrieved documents. These drafts are then verified by a larger generalist LLM. This two-step process improves both the quality and speed of output generation by reducing input token counts and allowing for diverse perspectives in the answering process.\\n\\nOverall, Advanced RAG systems aim to enhance the generation quality and efficiency by combining various strategies for document retrieval, evaluation, and response generation.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2XBkzNwLle5"
      },
      "source": [
        "# RankGPT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_it2CxTtLmHT"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.postprocessor.rankGPT_rerank import RankGPTRerank\n",
        "\n",
        "rankGPT = RankGPTRerank(top_n=3,llm=Settings.llm, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YA3M9m9CL6AJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a54275dd-adf8-41d2-dc28-31f2ef70d2c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Reranking, new rank list for nodes: [1, 0, 3, 9, 6, 8, 4, 2, 7, 5]"
          ]
        }
      ],
      "source": [
        "# Define a query engine that is responsible for retrieving related pieces of text,\n",
        "# and using a LLM to formulate the final answer.\n",
        "# The `node_postprocessors` function will be applied to the retrieved nodes.\n",
        "query_engine = index.as_query_engine(similarity_top_k=10, node_postprocessors=[rankGPT])\n",
        "\n",
        "res = query_engine.query(\"Explain how Retrieval Augmented Generation (RAG) works?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "wgyjv9e6MCVm",
        "outputId": "4cac7348-f5c1-4fc1-c222-25b8276e6d6e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Retrieval-Augmented Generation (RAG) works by integrating two main components: the Retrieval mechanism and the Generation mechanism. \\n\\n1. **Retrieval Mechanism**: This component is responsible for extracting relevant information from external knowledge sources, such as a dense vector index (e.g., Wikipedia). It involves:\\n   - **Indexing**: Organizing documents for efficient retrieval using methods like inverted indexes or dense vector encoding.\\n   - **Searching**: Utilizing the created indexes to fetch relevant documents based on user queries, often enhanced with rerankers to refine the order of returned documents.\\n\\n2. **Generation Mechanism**: After retrieving the relevant documents, this component generates responses based on the retrieved content and user queries. It employs a sequence-to-sequence (seq2seq) model to produce coherent and contextually appropriate outputs. This involves:\\n   - Using prompting techniques like Chain of Thought and Tree of Thought to improve the generation quality.\\n   - Interpreting the input, which allows the model to provide accurate responses that align with the user’s intent without needing further fine-tuning.\\n\\nTogether, these components enable RAG systems to leverage both parametric memory (the seq2seq model) and non-parametric memory (the dense vector index), enhancing their performance, especially in knowledge-intensive tasks.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "res.response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUhOlwWcMEUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82f0ecb7-df87-4eb2-d11b-ad8cc56ab6c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node ID\t 1c324686-fad7-4a41-bfd3-d44f9612ca91\n",
            "Title\t Evaluation of Retrieval-Augmented Generation: A Survey:Research Paper Information\n",
            "Text\t Authors: Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu.Numerous studies of Retrieval-Augmented Generation (RAG) systems have emerged from various perspectives since the advent of Large Language Models (LLMs). The RAG system comprises two primary components: Retrieval and Generation. The retrieval component aims to extract relevant information from various external knowledge sources. It involves two main phases, indexing and searching. Indexing organizes documents to facilitate efficient retrieval, using either inverted indexes for sparse retrieval or dense vector encoding for dense retrieval. The searching component utilizes these indexes to fetch relevant documents based on the user's query, often incorporating the optional rerankers to refine the ranking of the retrieved documents. The generation component utilizes the retrieved content and question query to formulate coherent and contextually relevant responses with the prompting and inferencing phases. As the \"Emerging\" ability of LLMs and the breakthrough in aligning human commands, LLMs are the best performance choices model for the generation stage. Prompting methods like Chain of Thought, Tree of Thought, Rephrase and Respond guide better generation results. In the inferencing step, LLMs interpret the prompted input to generate accurate and in-depth responses that align with the query's intent and integrate the extracted information without further finetuning, such as fully finetuning or LoRA. Appendix A details the complete RAG structure. Figure 1 illustrates the structure of the RAG systems as mentioned.\n",
            "Score\t 0.5764780433820803\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t 2aa05360-f43a-4819-bce7-0acf7b897eab\n",
            "Title\t Searching for Best Practices in Retrieval-Augmented Generation:1 Introduction\n",
            "Text\t Generative large language models are prone to producing outdated information or fabricating facts, although they were aligned with human preferences by reinforcement learning [1] or lightweight alternatives [2–5]. Retrieval-augmented generation (RAG) techniques address these issues by combining the strengths of pretraining and retrieval-based models, thereby providing a robust framework for enhancing model performance [6]. Furthermore, RAG enables rapid deployment of applications for specific organizations and domains without necessitating updates to the model parameters, as long as query-related documents are provided. Many RAG approaches have been proposed to enhance large language models (LLMs) through query-dependent retrievals [6–8]. A typical RAG workflow usually contains multiple intervening processing steps: query classification (determining whether retrieval is necessary for a given input query), retrieval (efficiently obtaining relevant documents for the query), reranking (refining the order of retrieved documents based on their relevance to the query), repacking (organizing the retrieved documents into a structured one for better generation), summarization (extracting key information for response generation from the repacked document and eliminating redundancies) and models. Implementing RAG also requires decisions on the ways to properly split documents into chunks, the types of embeddings to use for semantically representing these chunks, the choice of\n",
            "Score\t 0.5921164811943592\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t e791eaf2-3cdf-477a-8602-1cbb30a3d48c\n",
            "Title\t RAG\n",
            "Text\t # RAG<div class=\"flex flex-wrap space-x-1\"><a href=\"https://huggingface.co/models?filter=rag\"><img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-rag-blueviolet\"></a></div>## OverviewRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) andsequence-to-sequence models. RAG models retrieve documents, pass them to a seq2seq model, then marginalize to generateoutputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowingboth retrieval and generation to adapt to downstream tasks.It is based on the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, VladimirKarpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela.The abstract from the paper is the following:*Large pre-trained language models have been shown to store factual knowledge in their parameters, and achievestate-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and preciselymanipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behindtask-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledgeremain open research problems. Pre-trained models with a differentiable access mechanism to explicit nonparametricmemory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore ageneral-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trainedparametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is apre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with apre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passagesacross the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate ourmodels on a wide range of knowledge-intensive\n",
            "Score\t 0.5609868435228658\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "# Show the retrieved nodes\n",
        "for src in res.source_nodes:\n",
        "    print(\"Node ID\\t\", src.node_id)\n",
        "    print(\"Title\\t\", src.metadata[\"title\"])\n",
        "    print(\"Text\\t\", src.text.strip())\n",
        "    print(\"Score\\t\", src.score)\n",
        "    print(\"-_\" * 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mcAcZqhQluE"
      },
      "source": [
        "# Custom Postprocessor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7v7vmJblQrN6"
      },
      "source": [
        "## The `Judger` Function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k8IKlN9QvU7"
      },
      "source": [
        "The following function will query GPT-4o to retrieve the top three nodes that has highest similarity to the asked question.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhtJ1OeF9L3G"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "def judger(nodes, query):\n",
        "\n",
        "    # The model's output template\n",
        "    class OrderedNodes(BaseModel):\n",
        "        \"\"\"A node with the id and assigned score.\"\"\"\n",
        "\n",
        "        node_id: list\n",
        "        score: list\n",
        "\n",
        "    # Prepare the nodes and wrap them in <NODE></NODE> identifier, as well as the query\n",
        "    the_nodes = \"\"\n",
        "    for idx, item in enumerate(nodes):\n",
        "        the_nodes += f\"<NODE{idx+1}>\\nNode ID: {item.node_id}\\nText: {item.text}\\n</NODE{idx+1}>\\n\"\n",
        "\n",
        "    query = \"<QUERY>\\n{}\\n</QUERY>\".format(query)\n",
        "\n",
        "    # Define the prompt template\n",
        "    prompt_tmpl = PromptTemplate(\n",
        "        \"\"\"\n",
        "    You receive a qurey along with a list of nodes' text and their ids. Your task is to assign score\n",
        "    to each node based on its contextually closeness to the given query. The final output is each\n",
        "    node id along with its proximity score.\n",
        "    Here is the list of nodes:\n",
        "    {nodes_list}\n",
        "\n",
        "    And the following is the query:\n",
        "    {user_query}\n",
        "\n",
        "    Score each of the nodes based on their text and their relevancy to the provided query.\n",
        "    The score must be a decimal number between 0 an 1 so we can rank them.\"\"\"\n",
        "    )\n",
        "\n",
        "    # Define the an instance of GPT-4 and send the request\n",
        "    llm = OpenAI(model=\"gpt-4o-mini\")\n",
        "    ordered_nodes = llm.structured_predict(\n",
        "        OrderedNodes, prompt_tmpl, nodes_list=the_nodes, user_query=query\n",
        "    )\n",
        "\n",
        "    return ordered_nodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5f1GrBKZprO"
      },
      "source": [
        "## Define Postprocessor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZujUJTvQ6Yu"
      },
      "source": [
        "The following class will use the `judger` function to rank the nodes, and filter them based on the ranks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QtyuC8fZun0"
      },
      "outputs": [],
      "source": [
        "from typing import List, Optional\n",
        "from llama_index.core import QueryBundle\n",
        "from llama_index.core.postprocessor.types import BaseNodePostprocessor\n",
        "from llama_index.core.schema import NodeWithScore\n",
        "\n",
        "\n",
        "class OpenaiAsJudgePostprocessor(BaseNodePostprocessor):\n",
        "    def _postprocess_nodes(\n",
        "        self, nodes: List[NodeWithScore], query_bundle: Optional[QueryBundle]\n",
        "    ) -> List[NodeWithScore]:\n",
        "\n",
        "        r = judger(nodes, query_bundle)\n",
        "\n",
        "        node_ids = r.node_id\n",
        "        scores = r.score\n",
        "\n",
        "        sorted_scores = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
        "        top_three_nodes = [sorted_scores[i][0] for i in range(3)]\n",
        "\n",
        "        selected_nodes_id = [node_ids[item] for item in top_three_nodes]\n",
        "\n",
        "        final_nodes = []\n",
        "        for item in nodes:\n",
        "            if item.node_id in selected_nodes_id:\n",
        "                final_nodes.append(item)\n",
        "\n",
        "        return final_nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jk-lqYlYLipi"
      },
      "outputs": [],
      "source": [
        "judge = OpenaiAsJudgePostprocessor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgtsvxR7SflP"
      },
      "source": [
        "## Query Engine with Postprocessor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Hh3RLCeLfXZ"
      },
      "outputs": [],
      "source": [
        "# Define a query engine that is responsible for retrieving related pieces of text,\n",
        "# and using a LLM to formulate the final answer.\n",
        "# The `node_postprocessors` function will be applied to the retrieved nodes.\n",
        "query_engine = index.as_query_engine(similarity_top_k=5, node_postprocessors=[judge])\n",
        "\n",
        "res = query_engine.query(\"Explain how Parameter Efficient Fine tuning (PEFT) works?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "zmZv0EIyF0wG",
        "outputId": "6fa19ee2-ab2e-45d1-f4c3-47e1b3ee2b83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Parameter-Efficient Fine-Tuning (PEFT) focuses on adapting large pretrained models for various applications by fine-tuning only a small subset of additional model parameters. This approach significantly reduces computational and storage requirements compared to traditional full fine-tuning methods. By doing so, it allows for high-performance adaptation of models while making the process more accessible for users with consumer hardware.\\n\\nPEFT integrates seamlessly with popular libraries like Transformers, Diffusers, and Accelerate, optimizing both the training and inference of large models. This integration enhances user experience and efficiency in handling large language models, making advanced machine learning techniques more manageable. Users can also find resources, including quick start guides and practical how-tos, to help implement PEFT methods across diverse tasks.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "res.response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBMaG6yaZzjA",
        "outputId": "8833ec2c-da03-4e91-bd74-df4a977a8ee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node ID\t ffd813ca-d987-47fa-988d-2cddaa749553\n",
            "Title\t PEFT\n",
            "Text\t # PEFT🤗 PEFT (Parameter-Efficient Fine-Tuning) is a library for efficiently adapting large pretrained models to various downstream applications without fine-tuning all of a model's parameters because it is prohibitively costly. PEFT methods only fine-tune a small number of (extra) model parameters - significantly decreasing computational and storage costs - while yielding performance comparable to a fully fine-tuned model. This makes it more accessible to train and store large language models (LLMs) on consumer hardware.PEFT is integrated with the Transformers, Diffusers, and Accelerate libraries to provide a faster and easier way to load, train, and use large models for inference.<div class=\"mt-10\">  <div class=\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2 md:gap-y-4 md:gap-x-5\">    <a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"quicktour\"      ><div class=\"w-full text-center bg-gradient-to-br from-blue-400 to-blue-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed\">Get started</div>      <p class=\"text-gray-700\">Start here if you're new to 🤗 PEFT to get an overview of the library's main features, and how to train a model with a PEFT method.</p>    </a>    <a class=\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\" href=\"./task_guides/image_classification_lora\"      ><div class=\"w-full text-center bg-gradient-to-br from-indigo-400 to-indigo-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed\">How-to guides</div>      <p class=\"text-gray-700\">Practical guides demonstrating how to apply various PEFT methods across different types of tasks like image classification, causal language modeling, automatic speech recognition, and more. Learn how to use 🤗 PEFT with the DeepSpeed and Fully Sharded Data Parallel scripts.</p>    </a>    <a\n",
            "Score\t 0.4452879050724265\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t 22fe2e92-ea31-4b34-9b8c-9d6f5fa66e87\n",
            "Title\t PEFT configurations and models\n",
            "Text\t p-tuning to  \"encoder_dropout\": 0.0,  \"encoder_hidden_size\": 128,  \"encoder_num_layers\": 2,  \"encoder_reparameterization_type\": \"MLP\",  \"inference_mode\": true,  \"num_attention_heads\": 16,  \"num_layers\": 24,  \"num_transformer_submodules\": 1,  \"num_virtual_tokens\": 20,  \"peft_type\": \"P_TUNING\", #PEFT method type  \"task_type\": \"SEQ_CLS\", #type of task to train model on  \"token_dim\": 1024}```You can create your own configuration for training by initializing a [`PromptEncoderConfig`].```pyfrom peft import PromptEncoderConfig, TaskTypep_tuning_config = PromptEncoderConfig(    encoder_reparameterization_type=\"MLP\",    encoder_hidden_size=128,    num_attention_heads=16,    num_layers=24,    num_transformer_submodules=1,    num_virtual_tokens=20,    token_dim=1024,    task_type=TaskType.SEQ_CLS)```</hfoption></hfoptions>## PEFT modelsWith a PEFT configuration in hand, you can now apply it to any pretrained model to create a [`PeftModel`]. Choose from any of the state-of-the-art models from the [Transformers](https://hf.co/docs/transformers) library, a custom model, and even new and unsupported transformer architectures.For this tutorial, load a base [facebook/opt-350m](https://huggingface.co/facebook/opt-350m) model to finetune.```pyfrom transformers import AutoModelForCausalLMmodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")```Use the [`get_peft_model`] function to create a [`PeftModel`] from the base facebook/opt-350m model and the `lora_config` you created earlier.```pyfrom peft import get_peft_modellora_model = get_peft_model(model, lora_config)lora_model.print_trainable_parameters()\"trainable params: 1,572,864 || all params: 332,769,280 || trainable%:\n",
            "Score\t 0.43581349301685324\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t dee706b6-2345-4e7f-b850-1d80e233d998\n",
            "Title\t BOFT\n",
            "Text\t # BOFT[Orthogonal Butterfly (BOFT)](https://hf.co/papers/2311.06243) is a generic method designed for finetuning foundation models. It improves the paramter efficiency of the finetuning paradigm -- Orthogonal Finetuning (OFT), by taking inspiration from Cooley-Tukey fast Fourier transform, showing favorable results across finetuning different foundation models, including large vision transformers, large language models and text-to-image diffusion models.The abstract from the paper is:*Large foundation models are becoming ubiquitous, but training them from scratch is prohibitively expensive. Thus, efficiently adapting these powerful models to downstream tasks is increasingly important. In this paper, we study a principled finetuning paradigm -- Orthogonal Finetuning (OFT) -- for downstream task adaptation. Despite demonstrating good generalizability, OFT still uses a fairly large number of trainable parameters due to the high dimensionality of orthogonal matrices. To address this, we start by examining OFT from an information transmission perspective, and then identify a few key desiderata that enable better parameter-efficiency. Inspired by how the Cooley-Tukey fast Fourier transform algorithm enables efficient information transmission, we propose an efficient orthogonal parameterization using butterfly structures. We apply this parameterization to OFT, creating a novel parameter-efficient finetuning method, called Orthogonal Butterfly (BOFT). By subsuming OFT as a special case, BOFT introduces a generalized orthogonal finetuning framework. Finally, we conduct an extensive empirical study of adapting large vision transformers, large language models, and text-to-image diffusion models to various downstream tasks in vision and language*.## BOFTConfig[[autodoc]] tuners.boft.config.BOFTConfig## BOFTModel[[autodoc]] tuners.boft.model.BOFTModel\n",
            "Score\t 0.43420374773083437\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "# Show the retrieved nodes\n",
        "for src in res.source_nodes:\n",
        "    print(\"Node ID\\t\", src.node_id)\n",
        "    print(\"Title\\t\", src.metadata[\"title\"])\n",
        "    print(\"Text\\t\", src.text)\n",
        "    print(\"Score\\t\", src.score)\n",
        "    print(\"-_\" * 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7sIPpFFTep3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1f0ca3bb1bb14be18aac2e2292faed69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_caec106d984d4aebaca8efe174430c9f",
              "IPY_MODEL_a8b03b451cd348c88bf861352ac78525",
              "IPY_MODEL_1053d43951a4441d9c3074f6ef97bf24"
            ],
            "layout": "IPY_MODEL_2ec86a62bec34f1082eb1a3513e8e85f"
          }
        },
        "caec106d984d4aebaca8efe174430c9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a366c17411de4de59dbb7bc03ca9b4d3",
            "placeholder": "​",
            "style": "IPY_MODEL_cd5ad3ebd4264d69be2c6f903c5d0fe0",
            "value": "vectorstore.zip: 100%"
          }
        },
        "a8b03b451cd348c88bf861352ac78525": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_330c90562b6a49b6a9eac2f6c33e575d",
            "max": 97198458,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e856c40a387b4ee39a39b3e0deaf50df",
            "value": 97198458
          }
        },
        "1053d43951a4441d9c3074f6ef97bf24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_747bfa2657d64e61a3bc7586a27f89ac",
            "placeholder": "​",
            "style": "IPY_MODEL_fdfd90f122ba4147a7624c8d71fd1886",
            "value": " 97.2M/97.2M [00:05&lt;00:00, 39.2MB/s]"
          }
        },
        "2ec86a62bec34f1082eb1a3513e8e85f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a366c17411de4de59dbb7bc03ca9b4d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd5ad3ebd4264d69be2c6f903c5d0fe0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "330c90562b6a49b6a9eac2f6c33e575d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e856c40a387b4ee39a39b3e0deaf50df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "747bfa2657d64e61a3bc7586a27f89ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdfd90f122ba4147a7624c8d71fd1886": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}