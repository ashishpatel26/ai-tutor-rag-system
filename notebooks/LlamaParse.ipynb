{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOiTiGLqcbLdLlrgUrLl3++",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6b5dc292a7a046158e026c8329a97fb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_38277982d8834ac5ae68f9ac072ab491",
              "IPY_MODEL_d66e5a4e20b44733b08699d148070610",
              "IPY_MODEL_05b1981a168e460984f2c922a1f81376"
            ],
            "layout": "IPY_MODEL_0408fefa75244c9b858266465e712b5e"
          }
        },
        "38277982d8834ac5ae68f9ac072ab491": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d85b33daf9643b1aa7c1b17391d7c68",
            "placeholder": "​",
            "style": "IPY_MODEL_53569a41f67f41409700df13ddd8026d",
            "value": "research_papers_llamaparse.zip: 100%"
          }
        },
        "d66e5a4e20b44733b08699d148070610": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1d75f70aa134267a557d9671db15323",
            "max": 13584308,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bfb4812c42874b018278a268e353d9eb",
            "value": 13584308
          }
        },
        "05b1981a168e460984f2c922a1f81376": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_734e5d7ea46240469fb86b75e70ef83b",
            "placeholder": "​",
            "style": "IPY_MODEL_14f41bd9e3004e6fa3a4c1dd37069946",
            "value": " 13.6M/13.6M [00:01&lt;00:00, 10.4MB/s]"
          }
        },
        "0408fefa75244c9b858266465e712b5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d85b33daf9643b1aa7c1b17391d7c68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53569a41f67f41409700df13ddd8026d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1d75f70aa134267a557d9671db15323": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfb4812c42874b018278a268e353d9eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "734e5d7ea46240469fb86b75e70ef83b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14f41bd9e3004e6fa3a4c1dd37069946": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/towardsai/ai-tutor-rag-system/blob/main/notebooks/LlamaParse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3iSJpxmEML0w",
        "outputId": "4ed11442-77dd-4c8a-f23e-8ad29af9b7f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.11.11-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting llama-parse\n",
            "  Downloading llama_parse-0.5.6-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting llama-index-agent-openai<0.4.0,>=0.3.4 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.3.4-py3-none-any.whl.metadata (728 bytes)\n",
            "Collecting llama-index-cli<0.4.0,>=0.3.1 (from llama-index)\n",
            "  Downloading llama_index_cli-0.3.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core<0.12.0,>=0.11.10 (from llama-index)\n",
            "  Downloading llama_index_core-0.11.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting llama-index-embeddings-openai<0.3.0,>=0.2.4 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.2.5-py3-none-any.whl.metadata (686 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.3.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
            "  Downloading llama_index_legacy-0.9.48.post3-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting llama-index-llms-openai<0.3.0,>=0.2.9 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.2.9-py3-none-any.whl.metadata (648 bytes)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.2.1-py3-none-any.whl.metadata (728 bytes)\n",
            "Collecting llama-index-program-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.2.0-py3-none-any.whl.metadata (766 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl.metadata (785 bytes)\n",
            "Collecting llama-index-readers-file<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.2.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.3.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting nltk>3.8.1 (from llama-index)\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.4.0,>=0.3.4->llama-index)\n",
            "  Downloading openai-1.47.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.10->llama-index) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index) (3.10.5)\n",
            "Collecting dataclasses-json (from llama-index-core<0.12.0,>=0.11.10->llama-index)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.12.0,>=0.11.10->llama-index)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.12.0,>=0.11.10->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index) (2024.6.1)\n",
            "Collecting httpx (from llama-index-core<0.12.0,>=0.11.10->llama-index)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index) (3.3)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index) (10.4.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index) (2.9.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.2.0 (from llama-index-core<0.12.0,>=0.11.10->llama-index)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.12.0,>=0.11.10->llama-index)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index) (4.12.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.12.0,>=0.11.10->llama-index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index) (1.16.0)\n",
            "Collecting llama-cloud>=0.0.11 (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index)\n",
            "  Downloading llama_cloud-0.0.17-py3-none-any.whl.metadata (751 bytes)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.1.4)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.12.3)\n",
            "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index)\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (2024.9.11)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (2.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.10->llama-index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.10->llama-index) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx->llama-index-core<0.12.0,>=0.11.10->llama-index)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.10->llama-index) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.10->llama-index) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.10->llama-index)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (1.7.0)\n",
            "Collecting jiter<1,>=0.4.0 (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.10->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.10->llama-index) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.10->llama-index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.10->llama-index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.10->llama-index) (3.1.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.10->llama-index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.12.0,>=0.11.10->llama-index)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.10->llama-index) (1.2.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.10->llama-index) (24.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.16.0)\n",
            "Downloading llama_index-0.11.11-py3-none-any.whl (6.8 kB)\n",
            "Downloading llama_parse-0.5.6-py3-none-any.whl (10 kB)\n",
            "Downloading llama_index_agent_openai-0.3.4-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.3.1-py3-none-any.whl (27 kB)\n",
            "Downloading llama_index_core-0.11.11-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.2.5-py3-none-any.whl (6.1 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.3.1-py3-none-any.whl (10 kB)\n",
            "Downloading llama_index_legacy-0.9.48.post3-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_llms_openai-0.2.9-py3-none-any.whl (12 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.2.1-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.2.0-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.2.2-py3-none-any.whl (38 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.3.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading llama_cloud-0.0.17-py3-none-any.whl (187 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.4/187.4 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.47.0-py3-none-any.whl (375 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.6/375.6 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: striprtf, dirtyjson, tenacity, pypdf, nltk, mypy-extensions, marshmallow, jiter, h11, deprecated, typing-inspect, tiktoken, httpcore, httpx, dataclasses-json, openai, llama-index-core, llama-cloud, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-legacy, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "Successfully installed dataclasses-json-0.6.7 deprecated-1.2.14 dirtyjson-1.0.8 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 llama-cloud-0.0.17 llama-index-0.11.11 llama-index-agent-openai-0.3.4 llama-index-cli-0.3.1 llama-index-core-0.11.11 llama-index-embeddings-openai-0.2.5 llama-index-indices-managed-llama-cloud-0.3.1 llama-index-legacy-0.9.48.post3 llama-index-llms-openai-0.2.9 llama-index-multi-modal-llms-openai-0.2.1 llama-index-program-openai-0.2.0 llama-index-question-gen-openai-0.2.0 llama-index-readers-file-0.2.2 llama-index-readers-llama-parse-0.3.0 llama-parse-0.5.6 marshmallow-3.22.0 mypy-extensions-1.0.0 nltk-3.9.1 openai-1.47.0 pypdf-4.3.1 striprtf-0.0.26 tenacity-8.5.0 tiktoken-0.7.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U llama-index llama-parse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# API access to llama-cloud\n",
        "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"llx-\"\n",
        "\n",
        "# from google.colab import userdata\n",
        "# os.environ[\"LLAMA_CLOUD_API_KEY\"] = userdata.get('LlamaParse_API_KEY')"
      ],
      "metadata": {
        "id": "n9OKejSAM1Kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading Research paper dataset from HuggingFace Hub\n",
        "from huggingface_hub import hf_hub_download\n",
        "file_path = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"research_papers_llamaparse.zip\",repo_type=\"dataset\",local_dir=\"/content\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6b5dc292a7a046158e026c8329a97fb9",
            "38277982d8834ac5ae68f9ac072ab491",
            "d66e5a4e20b44733b08699d148070610",
            "05b1981a168e460984f2c922a1f81376",
            "0408fefa75244c9b858266465e712b5e",
            "5d85b33daf9643b1aa7c1b17391d7c68",
            "53569a41f67f41409700df13ddd8026d",
            "a1d75f70aa134267a557d9671db15323",
            "bfb4812c42874b018278a268e353d9eb",
            "734e5d7ea46240469fb86b75e70ef83b",
            "14f41bd9e3004e6fa3a4c1dd37069946"
          ]
        },
        "id": "Xs5WjC01NCiy",
        "outputId": "c7ed997d-4035-45db-d4d1-7fde8a960e51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "research_papers_llamaparse.zip:   0%|          | 0.00/13.6M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b5dc292a7a046158e026c8329a97fb9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip research_papers_llamaparse.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRNqM8cIPQC_",
        "outputId": "7c78a9bc-d80c-4ebd-fa4e-86977ddf89df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  research_papers_llamaparse.zip\n",
            "   creating: research_papers_llamaparse/\n",
            "  inflating: research_papers_llamaparse/2106.09685v2.pdf  \n",
            "  inflating: research_papers_llamaparse/2404.19756v2.pdf  \n",
            "  inflating: research_papers_llamaparse/2405.07437v2.pdf  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "WC9urbdDOABf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parse directory to LlamaParse"
      ],
      "metadata": {
        "id": "itMgf7Dsptw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LlamaParse Implemetation\n",
        "from llama_parse import LlamaParse\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "#Parser\n",
        "parser = LlamaParse(\n",
        "    result_type=\"markdown\",\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "-JuFlFt2NcJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_extractor = {\".pdf\": parser}\n",
        "\n",
        "documents = SimpleDirectoryReader(\"/content/research_papers_llamaparse\", file_extractor=file_extractor).load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrPv4GtFODxP",
        "outputId": "79b92317-b7f9-4ae6-95f4-bd3506fbafa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started parsing the file under job_id 57f82fff-469b-4b27-a9cb-77e4e9a46327\n",
            "Started parsing the file under job_id 6c36b066-6f86-4414-9a43-8d7eec9be4d8\n",
            "Started parsing the file under job_id 017e96ba-3094-4012-988b-ffd2c8b75088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvWyrtWhPgkZ",
        "outputId": "c100faf0-f8e0-4361-8d2a-e64c1fe6b36f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(id_='b13c6802-d713-4ded-b772-34743922fce5', embedding=None, metadata={'file_path': '/content/research_papers_llamaparse/2106.09685v2.pdf', 'file_name': '2106.09685v2.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-09-22', 'last_modified_date': '2024-07-06'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='# LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\\n\\nEdward Hu∗, Yelong Shen∗, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen\\n\\nMicrosoft Corporation\\n\\n{edwardhu, yeshe, phwallis, zeyuana, yuanzhil, swang, luw, wzchen}@microsoft.com\\n\\nyuanzhil@andrew.cmu.edu\\n\\n(Version 2)\\n\\n# ABSTRACT\\n\\nAn important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example – deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.\\n\\n# 1 INTRODUCTION\\n\\nMany applications in natural language processing rely on adapting one large-scale, pre-trained language model to multiple downstream applications. Such adaptation is usually done via fine-tuning, which updates all the parameters of the pre-trained model. The major downside of fine-tuning is that the new model contains as many parameters as in the original model. As larger models are trained every few months, this changes from a mere “inconvenience” for GPT-2 (Radford et al., b) or RoBERTa large (Liu et al., 2019) to a critical deployment challenge for GPT-3 (Brown et al., 2020) with 175 billion trainable parameters.\\n\\nMany sought to mitigate this by adapting only some parameters or learning external modules for new tasks. This way, we only need to store and load a small number of task-specific parameters in addition to the pre-trained model for each task, greatly boosting the operational efficiency when deployed. However, existing techniques\\n\\n∗Equal contribution.\\n\\n0Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency.\\n\\n1While GPT-3 175B achieves non-trivial performance with few-shot learning, fine-tuning boosts its performance significantly as shown in Appendix A.\\n\\n# Figure 1: Our reparametrization.\\n\\nWe only train A and B', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LlamaParse JSON Output"
      ],
      "metadata": {
        "id": "fCfzTVX0pqhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using LlamaParse in JSON Mode for PDF Reading\n",
        "\n",
        "import glob\n",
        "pdf_files = glob.glob(\"/content/research_papers_llamaparse/*.pdf\")\n",
        "\n",
        "parser = LlamaParse(verbose=True)\n",
        "\n",
        "json_objs=[]\n",
        "\n",
        "for pdf_file in pdf_files:\n",
        "  json_objs.extend(parser.get_json_result(pdf_file))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBb4VjgPPiWq",
        "outputId": "ac9fba0f-66ce-4d27-a946-5090d34da980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started parsing the file under job_id 996fef21-4fb4-49c8-82ae-38f2cf7190c6\n",
            "Started parsing the file under job_id ef8297fd-73e6-47c3-9ea4-d58d4de7a205\n",
            "Started parsing the file under job_id 1b8b700c-031f-4dc7-b2ed-986ba3a44efc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_objs[0]['pages'][0]['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "A9HexIWZWCpV",
        "outputId": "fa5d2383-aa40-4953-e6a8-ede437c0b005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'arXiv:2404.19756v2 [cs.LG] 2 May 2024\\n\\n                                                                                     KAN: Kolmogorov–Arnold Networks\\n\\n                                                                James Halverson3,41 Massachusetts Institute of TechnologyZiming Liu1,4∗ Yixuan Wang2ci´1Sachin Vaidya1Marin Soljaˇ               c      ,4           Thomas Y. Hou2Fabian Ruehle3,4Max Tegmark1,4\\n                                                                                                                                             2 California Institute of Technology3 Northeastern University\\n                                                                          4 The NSF Institute for Artificial Intelligence and Fundamental Interactions\\n\\n                                                                                                                                                                                      Abstract\\n                                                                Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-\\n                                                                Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs).\\n                                                                While MLPs have fixed activation functions on nodes (“neurons”), KANs have learnable\\n                                                                activation functions on edges (“weights”). KANs have no linear weights at all – every\\n                                                                weight parameter is replaced by a univariate function parametrized as a spline. We show\\n                                                                that this seemingly simple change makes KANs outperform MLPs in terms of accuracy\\n                                                                and interpretability. For accuracy, much smaller KANs can achieve comparable or better\\n                                                                accuracy than much larger MLPs in data fitting and PDE solving. Theoretically and em-\\n                                                                pirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs\\n                                                                can be intuitively visualized and can easily interact with human users. Through two ex-\\n                                                                amples in mathematics and physics, KANs are shown to be useful “collaborators” helping\\n                                                                scientists (re)discover mathematical and physical laws. In summary, KANs are promising\\n                                                                alternatives for MLPs, opening opportunities for further improving today’s deep learning\\n                                                                models which rely heavily on MLPs.\\n\\n                                                                     Model                    Multi-Layer Perceptron (MLP)                                                                                  Kolmogorov-Arnold Network (KAN)\\n                                                                   Theorem                               Universal Approximation Theorem                                                                            Kolmogorov-Arnold Representation Theorem\\n                                                                                                                                   N(ϵ)                                                                                                                     2n+1        n\\n                                                                    Formula                                         f(x) ≈∑1aσ(wi ⋅ x + b)                                                                                                   f(x) =          ∑1Φq    ∑1ϕq,p(x)p\\n                                                                  (Shallow)                                                                    i                          i\\n                                                                                                                                    i=                                                                                                                       q=      p=\\n                                                                                          (a)                                                          fixed activation functions                           (b)                                                    learnable activation functions\\n                                                                                                                                                                       on nodes                                                                                                       on edges\\n                                                                     Model\\n                                                                  (Shallow)                                                                                                                                                                                               sum operation on nodes\\n                                                                                                                                                         learnable weights\\n                                                                                                                                                                  on edges\\n\\n                                                                    Formula                         MLP(x) = (W3 ∘ σ2 ∘ W2 ∘ σ1 ∘ W)(x)                                                                                                KAN(x) = (Φ3 ∘ Φ2 ∘ Φ)(x)\\n                                                                     (Deep)                                                                                                      1                                                                                                 1\\n\\n                                                                                           (c)                                                                                       MLP(x)                 (d)                                                                                   KAN(x)\\n                                                                                                                                                        W3                                                                                                                Φ3\\n                                                                     Model                                                                               σ2                            nonlinear,\\n                                                                    (Deep)                                                                                                                  fixed                                                                                                nonlinear,\\n                                                                                                                                                        W2                                                                                                                Φ2                     learnable\\n                                                                                                                                                          σ1                               linear,\\n                                                                                                                                                        W1                             learnable                                                                           Φ1\\n                                                                                                                                                                                      x                                                                                                           x\\n\\n                                                                    Figure 0.1: Multi-Layer Perceptrons (MLPs) vs. Kolmogorov-Arnold Networks (KANs)\\n\\n                                                ∗zmliu@mit.edu\\n\\n                                       Preprint. Under review.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_objs[0]['pages'][4]['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "fN075z18fAHI",
        "outputId": "338e0fad-dc26-443f-e9ce-226d8a77b753"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'As mentioned, such a network is known to be too simple to approximate any function arbitrarily\\nwell in practice with smooth splines! We therefore generalize our KAN to be wider and deeper.\\nIt is not immediately clear how to make KANs deeper, since Kolmogorov-Arnold representations\\ncorrespond to two-layer KANs. To the best of our knowledge, there is not yet a “generalized”\\nversion of the theorem that corresponds to deeper KANs.\\n\\nThe breakthrough occurs when we notice the analogy between MLPs and KANs. In MLPs, once we\\ndefine a layer (which is composed of a linear transformation and nonlinearties), we can stack more\\nlayers to make the network deeper. To build deep KANs, we should first answer: “what is a KAN\\nlayer?” It turns out that a KAN layer with nin-dimensional inputs and nout-dimensional outputs can\\nbe defined as a matrix of 1D functions\\n\\n                         Φ = {ϕq,p},            p = 1, 2, · · · , nin,         q = 1, 2 · · · , nout,                    (2.2)\\n\\nwhere the functions ϕq,p have trainable parameters, as detaild below. In the Kolmogov-Arnold theo-\\nrem, the inner functions form a KAN layer with nin = n and nout = 2n + 1, and the outer functions\\nform a KAN layer with nin = 2n + 1 and nout = 1. So the Kolmogorov-Arnold representations in\\nEq. (2.1) are simply compositions of two KAN layers. Now it becomes clear what it means to have\\ndeeper Kolmogorov-Arnold representations: simply stack more KAN layers!\\n\\nLet us introduce some notation. This paragraph will be a bit technical, but readers can refer to Fig-\\nure 2.2 (left) for a concrete example and intuitive understanding. The shape of a KAN is represented\\nby an integer array\\n\\n                                                     [n0, n1, · · · , nL],                                               (2.3)\\nwhere ni is the number of nodes in the ith layer of the computational graph. We denote the ith\\nneuron in the lth layer by (l, i), and the activation value of the (l, i)-neuron by xl,i. Between layer l\\nand layer l + 1, there are nnl+1 activation functions: the activation function that connects (l, i) andl\\n(l + 1, j) is denoted by\\n\\n                      ϕl,j,i,    l = 0, · · · , L − 1,       i = 1, · · · , n,l    j = 1, · · · , nl+1.                  (2.4)\\nThe pre-activation of ϕl,j,i is simply xl,i; the post-activation of ϕl,j,i is denoted by ˜l,j,i ≡x\\nϕl,j,i(xl,i). The activation value of the (l + 1, j) neuron is simply the sum of all incoming post-\\nactivations:                           nl              nl\\n                         xl+1,j =     X˜l,j,i =x      Xϕl,j,i(xl,i),            j = 1, · · · , nl+1.                     (2.5)\\n                                      i=1             i=1\\nIn matrix form, this reads\\n                                   \\uf8eb                                                            \\uf8f6\\n                                        ϕl,1,1(·)         ϕl,1,2(·)      · · ·    ϕl,1,nl (·)\\n                                   \\uf8ec\\n                                   \\uf8ec    ϕl,2,1(·)         ϕl,2,2(·)      · · ·    ϕl,2,nl (·)   \\uf8f7\\n                                                                                                \\uf8f7x,\\n                        xl+1 =     \\uf8ec\\n                                   \\uf8ec         ...              ...                       ...     \\uf8f7\\n                                                                                                \\uf8f7    l                   (2.6)\\n                                   \\uf8ed                                                            \\uf8f8\\n                                      ϕl,nl+1,1(·)      ϕl,nl+1,2(·)     · · ·  ϕl,nl+1,nl (·)\\n                                   |                             {zl                              }\\n                                                                 Φ\\n\\nwhere Φl is the function matrix corresponding to the ln0 , the output of KAN isth KAN layer. A general KAN network is a\\ncomposition of L layers: given an input vector x0 ∈ R\\n\\n                                 KAN(x) = (ΦL−1 ◦ ΦL−2 ◦ · · · ◦ Φ1 ◦ Φ0)x.                                              (2.7)\\n\\n                                                               5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# KAN 7th page complete extracted information\n",
        "json_objs[0]['pages'][6]['text']"
      ],
      "metadata": {
        "id": "TXGAykARXWdV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "eb429629-711f-4be4-d207-212b6c610e1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'                           Paper                             Idea                  Scaling exponent α\\n                Sharma & Kaplan [17]             Intrinsic dimensionality                (k + 1)/d\\n                   Michaud et al. [18]                maximum arity                      (k + 1)/2\\n                    Poggio et al. [14]            compositional sparsity                     m/2\\n                            Ours                    K-A representation                      k + 1\\nTable 1: Scaling exponents from different theories ℓ ∝ N −α. ℓ: test RMSE loss, N : number of model\\nparameters, d: input intrinsic dimension, k: order of piecewise polynomial, m: derivative order as in function\\nclass Wm.\\n\\n(2) with layers of equal width n0 = n1 = · · · = nL = N ,\\n(3) with each spline of order k (usually k = 3) on G intervals (for G + 1 grid points).\\nThen there are in total O(N 2L(G + k)) ∼ O(N 2LG) parameters. In contrast, an MLP with depth\\nL and width N only needs O(N 2L) parameters, which appears to be more efficient than KAN. For-\\ntunately, KANs usually require much smaller N than MLPs, which not only saves parameters, but\\nalso achieves better generalization (see e.g., Figure 3.1 and 3.3) and facilitates interpretability. We\\nremark that for 1D problems, we can take N = L = 1 and the KAN network in our implementation\\nis nothing but a spline approximation. For higher dimensions, we characterize the generalization\\nbehavior of KANs with a theorem below.\\n\\n2.3    KAN’s Approximation Abilities and Scaling Laws\\nRecall that in Eq. (2.1), the 2-Layer width-(2n + 1) representation may be non-smooth. How-\\never, deeper representations may bring the advantages of smoother activations. For example, the\\n4-variable function\\n                           f (x1, x2, x3, x4) = expsin(x1          2+ x22) + sin(x3 2+ x42)                     (2.13)\\n\\ncan be smoothly represented by a [4, 2, 1, 1] KAN which is 3-Layer, but may not admit a 2-Layer\\nKAN with smooth activations. To facilitate an approximation analysis, we still assume smoothness\\nof activations, but allow the representations to be arbitrarily wide and deep, as in Eq. (2.7). To\\nemphasize the dependence of our KAN on the finite set of grid points, we use Φl Gand Φl,i,j belowG\\nto replace the notation Φl and Φl,i,j used in Eq. (2.5) and (2.6).\\nTheorem 2.1 (Approximation theory, KAT). Let x = (x1, x2, · · · , xn). Suppose that a function\\nf (x) admits a representation\\n\\n                                     f = (ΦL−1 ◦ ΦL−2 ◦ · · · ◦ Φ1 ◦ Φ0)x ,                                     (2.14)\\n\\nas in Eq. (2.7), where each one of the Φl,i,j are (k + 1)-times continuously differentiable. Then\\nthere exists a constant C depending on f and its representation, such that we have the following\\napproximation bound in terms of the grid size G: there exist k-th order B-spline functions Φl,i,jG\\nsuch that for any 0 ≤ m ≤ k, we have the bound\\n                       ∥f − (ΦLG−1 ◦ ΦLG−2 ◦ · · · ◦ Φ1 G◦ Φ0 G)x∥Cm ≤ CG−k−1+m .                               (2.15)\\nHere we adopt the notation of Cm-norm measuring the magnitude of derivatives up to order m:\\n                                        ∥g∥Cm = max ∈[0,1]|β|≤mxsupn    Dβ g(x) .\\n\\nProof. By the classical 1D B-spline theory [19] and the fact that Φl,i,j as continuous functions can\\nbe uniformly bounded on a bounded domain, we know that there exist finite-grid B-spline functions\\n\\n                                                              7'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Table information\n",
        "json_objs[0]['pages'][6]['items'][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEnTFCn2pgB7",
        "outputId": "b5604aba-3717-4621-964b-1d11f8f8dc84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'type': 'table',\n",
              " 'rows': [['Paper', 'Idea', 'Scaling exponent α'],\n",
              "  ['Sharma & Kaplan [17]', 'Intrinsic dimensionality', '(k + 1)/d'],\n",
              "  ['Michaud et al. [18]', 'maximum arity', '(k + 1)/2'],\n",
              "  ['Poggio et al. [14]', 'compositional sparsity', 'm/2'],\n",
              "  ['Ours', 'K-A representation', 'k + 1']],\n",
              " 'md': '|Paper|Idea|Scaling exponent α|\\n|---|---|---|\\n|Sharma & Kaplan [17]|Intrinsic dimensionality|(k + 1)/d|\\n|Michaud et al. [18]|maximum arity|(k + 1)/2|\\n|Poggio et al. [14]|compositional sparsity|m/2|\\n|Ours|K-A representation|k + 1|',\n",
              " 'isPerfectTable': True,\n",
              " 'csv': '\"Paper\",\"Idea\",\"Scaling exponent α\"\\n\"Sharma & Kaplan [17]\",\"Intrinsic dimensionality\",\"(k + 1)/d\"\\n\"Michaud et al. [18]\",\"maximum arity\",\"(k + 1)/2\"\\n\"Poggio et al. [14]\",\"compositional sparsity\",\"m/2\"\\n\"Ours\",\"K-A representation\",\"k + 1\"'}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_objs[0]['pages'][3]['items'][2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2KgsfBKrxME",
        "outputId": "e508bb8c-9888-445f-86c9-c4bb5dd15efc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'type': 'heading',\n",
              " 'lvl': 1,\n",
              " 'value': 'Figure 2.2',\n",
              " 'md': '# Figure 2.2',\n",
              " 'bBox': {'x': 108, 'y': 593, 'w': 12.453249999999997, 'h': 9.962599999999952}}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KKy6jmjLyG3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pTfBbY2l2czn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}