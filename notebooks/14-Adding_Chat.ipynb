{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/towardsai/ai-tutor-rag-system/blob/main/notebooks/14-Adding_Chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zE1h0uQV7uT"
      },
      "source": [
        "# Install Packages and Setup Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "QPJzr-I9XQ7l",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install -q llama-index==0.10.57 openai==1.37.0 llama-index-finetuning llama-index-embeddings-huggingface llama-index-embeddings-cohere llama-index-readers-web cohere==5.6.2 tiktoken==0.7.0 chromadb==0.5.5 html2text sentence_transformers pydantic llama-index-vector-stores-chroma==0.1.10 kaleido==0.2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "riuXwpSPcvWC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set the following API Keys in the Python environment. Will be used later.\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_OPENAI_KEY>\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"<YOUR_API_KEY>\"\n",
        "\n",
        "# from google.colab import userdata\n",
        "# os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api_key')\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = userdata.get('Google_api_key')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "jIEeZzqLbz0J"
      },
      "outputs": [],
      "source": [
        "# Allows running asyncio in environments with an existing event loop, like Jupyter notebooks.\n",
        "\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bkgi2OrYzF7q"
      },
      "source": [
        "# Load a Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9oGT6crooSSj"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core import Settings\n",
        "\n",
        "Settings.llm = OpenAI(temperature=1, model=\"gpt-4o-mini\")\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: You can create a vector store from scratch using the code below, or you can load it from Hugging Face using the code provided in this notebook.**"
      ],
      "metadata": {
        "id": "giQakQl7pDwS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BwVuJXlzHVL"
      },
      "source": [
        "# Create a VectoreStore\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQP87lHczHKc"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "\n",
        "# create client and a new collection\n",
        "# chromadb.EphemeralClient saves data in-memory.\n",
        "chroma_client = chromadb.PersistentClient(path=\"./ai_tutor_knowledge\")\n",
        "chroma_collection = chroma_client.create_collection(\"ai_tutor_knowledge\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAaGcYMJzHAN"
      },
      "outputs": [],
      "source": [
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "\n",
        "# Define a storage context object using the created vector database.\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9JbAzFcjkpn"
      },
      "source": [
        "# Load the Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceveDuYdWCYk"
      },
      "source": [
        "### Download\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "92c130aace5f4b19b0aac1751899551c",
            "92519659dadf4aefab4ae34050cee3e2",
            "35af66cc2e6541819b0c871dbe9c4120",
            "f0d65c04c4bf475aaa636ab93c7d3cb4",
            "a1b77d4363d04939ada84f153714be1e",
            "731be95a92e44174b9d8971f02cacf86",
            "57f98df9d10e47d2bf005b909340e855",
            "75ae7d9892f94db9a54455456aada690",
            "2e8bd99c1a5e4b0382406c916b7ef6b7",
            "430432c14eae4621a9520ae66023be80",
            "58c932f0ea5f4423ab92963b4080637a"
          ]
        },
        "id": "wl_pbPvMlv1h",
        "outputId": "229b711f-d105-4c4b-8570-eab1ccd59658"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ai_tutor_knowledge.jsonl:   0%|          | 0.00/6.96M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92c130aace5f4b19b0aac1751899551c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "file_path = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"ai_tutor_knowledge.jsonl\",repo_type=\"dataset\",local_dir=\"/content\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWBLtDbUWJfA"
      },
      "source": [
        "## Read File\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Q9sxuW0g3Gd",
        "outputId": "7162ce22-6d31-481e-f009-0dca1c67e524"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "762"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import json\n",
        "with open(file_path, \"r\") as file:\n",
        "    ai_tutor_knowledge = [json.loads(line) for line in file]\n",
        "\n",
        "len(ai_tutor_knowledge)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S17g2RYOjmf2"
      },
      "source": [
        "# Convert to Document obj\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YizvmXPejkJE"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from llama_index.core import Document\n",
        "\n",
        "def create_docs_from_list(data_list: List[dict]) -> List[Document]:\n",
        "    documents = []\n",
        "    for data in data_list:\n",
        "        documents.append(\n",
        "            Document(\n",
        "                doc_id=data[\"doc_id\"],\n",
        "                text=data[\"content\"],\n",
        "                metadata={  # type: ignore\n",
        "                    \"url\": data[\"url\"],\n",
        "                    \"title\": data[\"name\"],\n",
        "                    \"tokens\": data[\"tokens\"],\n",
        "                    \"source\": data[\"source\"],\n",
        "                },\n",
        "                excluded_llm_metadata_keys=[\n",
        "                    \"title\",\n",
        "                    \"tokens\",\n",
        "                    \"source\",\n",
        "                ],\n",
        "                excluded_embed_metadata_keys=[\n",
        "                    \"url\",\n",
        "                    \"tokens\",\n",
        "                    \"source\",\n",
        "                ],\n",
        "            )\n",
        "        )\n",
        "    return documents\n",
        "\n",
        "doc = create_docs_from_list(ai_tutor_knowledge)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjuLbmFuWsyl"
      },
      "source": [
        "# Transforming\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9z3t70DGWsjO"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.text_splitter import TokenTextSplitter\n",
        "\n",
        "# Define the splitter object that split the text into segments with 512 tokens,\n",
        "# with a 128 overlap between the segments.\n",
        "text_splitter = TokenTextSplitter(separator=\" \", chunk_size=512, chunk_overlap=128)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.extractors import (\n",
        "    SummaryExtractor,\n",
        "    QuestionsAnsweredExtractor,\n",
        "    KeywordExtractor,\n",
        ")\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "\n",
        "# Create the pipeline to apply the transformation on each chunk,\n",
        "# and store the transformed text in the chroma vector store.\n",
        "pipeline = IngestionPipeline(\n",
        "    transformations=[\n",
        "        text_splitter,\n",
        "        QuestionsAnsweredExtractor(questions=3, llm=Settings.llm), # You can ignore the LLM argument\n",
        "        SummaryExtractor(summaries=[\"prev\", \"self\"], llm=Settings.llm),\n",
        "        KeywordExtractor(keywords=10, llm=Settings.llm),\n",
        "        OpenAIEmbedding(),\n",
        "    ],\n",
        "    vector_store=vector_store,\n",
        ")\n",
        "\n",
        "nodes = pipeline.run(documents=doc, show_progress=True)"
      ],
      "metadata": {
        "id": "2apdfTzAp215"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compress the vector store directory to a zip file to be able to download and use later.\n",
        "!zip -r vectorstore.zip ai_tutor_knowledge"
      ],
      "metadata": {
        "id": "IcZiELbypx2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWaT6rL7ksp8"
      },
      "source": [
        "# Load Indexes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLkmv3Yxp9mu"
      },
      "source": [
        "**Note: If you created the vector store from scratch, please comment out the three code blocks/cells below.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading Vector store from Hugging face hub\n",
        "from huggingface_hub import hf_hub_download\n",
        "vectorstore = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"vectorstore.zip\",repo_type=\"dataset\",local_dir=\"/content\")"
      ],
      "metadata": {
        "id": "60AA1iCrqApk"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "SodY2Xpf_kxg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7e33ffc-f5a1-4605-91dc-a785795921b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  vectorstore.zip\n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/length.bin  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/index_metadata.pickle  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/link_lists.bin  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/header.bin  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/data_level0.bin  \n",
            "  inflating: ai_tutor_knowledge/chroma.sqlite3  \n"
          ]
        }
      ],
      "source": [
        "!unzip vectorstore.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "mXi56KTXk2sp"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "\n",
        "# Load the vector store from the local storage.\n",
        "db = chromadb.PersistentClient(path=\"./ai_tutor_knowledge\")\n",
        "chroma_collection = db.get_or_create_collection(\"ai_tutor_knowledge\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "jKXURvLtkuTS"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "# Create the index based on the vector store.\n",
        "vector_index = VectorStoreIndex.from_vector_store(vector_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0m5rl195bcz"
      },
      "source": [
        "# Disply result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "4JpaHEmF5dSS"
      },
      "outputs": [],
      "source": [
        "# A simple function to show the response and the sources.\n",
        "def display_res(response):\n",
        "    print(\"Response:\\n\\t\", response.response.replace(\"\\n\", \"\"))\n",
        "\n",
        "    print(\"Sources:\")\n",
        "    if response.source_nodes:\n",
        "        for src in response.source_nodes:\n",
        "            print(\"\\tNode ID\\t\", src.node_id)\n",
        "            print(\"\\tText\\t\", src.text)\n",
        "            print(\"\\tScore\\t\", src.score)\n",
        "            print(\"\\t\" + \"-_\" * 20)\n",
        "    else:\n",
        "        print(\"\\tNo sources used!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbStjvUJ1cft"
      },
      "source": [
        "# Chat Engine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "kwWlDpoR1cRI"
      },
      "outputs": [],
      "source": [
        "# define the chat_engine by using the index\n",
        "chat_engine = vector_index.as_chat_engine(llm = Settings.llm)  # chat_mode=\"best\" and You can ignore the llm argument"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ER3Lb-oN46lJ",
        "outputId": "e8ff6211-5616-4bca-da86-b05f096c268c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "\t Parameter Efficient Fine Tuning (PEFT) involves optimizing the fine-tuning process of large language models (LLMs) to reduce computational intensity. Instead of adjusting every weight in a pretrained model, PEFT focuses on making slight adjustments to a select set of model weights, allowing for more efficient training.There are three main approaches within PEFT:1. **Selective**: Fine-tuning only a subset of the parameters in the model minimizes the computational load by targeting specific weights.2. **Reparameterization**: This method uses low-rank representations to reformulate model weights. Techniques like LoRA (Low Rank Adaptation) decompose weight matrices to reduce the number of trainable parameters while preserving performance.3. **Additive**: Although not detailed in the available information, this approach typically involves adding additional parameters or layers to enhance the model's capacity without requiring full finetuning.By leveraging these strategies, PEFT allows for effective fine-tuning of large models with fewer resources, making it particularly beneficial for tasks such as question answering and sentiment analysis.\n",
            "Sources:\n",
            "\tNode ID\t 6be88fa3-2f8b-43e7-aba0-d874b39809fc\n",
            "\tText\t # FourierFT: Discrete Fourier Transformation Fine-Tuning[FourierFT](https://huggingface.co/papers/2405.03003) is a parameter-efficient fine-tuning technique that leverages Discrete Fourier Transform to compress the model's tunable weights. This method outperforms LoRA in the GLUE benchmark and common ViT classification tasks using much less parameters.FourierFT currently has the following constraints:- Only `nn.Linear` layers are supported.- Quantized layers are not supported.If these constraints don't work for your use case, consider other methods instead.The abstract from the paper is:> Low-rank adaptation (LoRA) has recently gained much interest in fine-tuning foundation models. It effectively reduces the number of trainable parameters by incorporating low-rank matrices A and B to represent the weight change, i.e., Delta W=BA. Despite LoRA's progress, it faces storage challenges when handling extensive customization adaptations or larger base models. In this work, we aim to further compress trainable parameters by enjoying the powerful expressiveness of the Fourier transform. Specifically, we introduce FourierFT, which treats Delta W as a matrix in the spatial domain and learns only a small fraction of its spectral coefficients. With the trained spectral coefficients, we implement the inverse discrete Fourier transform to recover Delta W. Empirically, our FourierFT method shows comparable or better performance with fewer parameters than LoRA on various tasks, including natural language understanding, natural language generation, instruction tuning, and image classification. For example, when performing instruction tuning on the LLaMA2-7B model, FourierFT surpasses LoRA with only 0.064M trainable parameters, compared to LoRA's 33.5M.## FourierFTConfig[[autodoc]] tuners.fourierft.config.FourierFTConfig## FourierFTModel[[autodoc]] tuners.fourierft.model.FourierFTModel\n",
            "\tScore\t 0.412086796867701\n",
            "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "\tNode ID\t 2b4590b8-4c9d-4d51-a81f-9a13979593a8\n",
            "\tText\t SAMSUM is one of the datasets that FLAN T5 uses. There are several pre-trained FLAN T5 models that have been fine-tuned on SAMSUM  including Phil Schmid/flan-t5-base-samsum and jasonmcaffee/flan-t5-large-samsum on Hugging Face. If we want to fine-tune the FLAN T5 model specifically for formal dialogue conversations  we can do so using the DIALOGUESUM dataset.   Models fine-tuned on DialogSum can be applied to areas like customer support  meeting minutes generation  chatbot summarization  and more.   2. PEFT (Parameter efficient fine tuning)Training LLMs is computationally intensive. Full finetuning is computationally expensive as it might change each weight in the model. First  we start with a pretrained LLM like GPT-3. This model already has a vast amount of knowledge and understanding of language. Then we provide task-specific datasets  which could be data for question answering or sentiment analysis or any other customer dataset. During training  full finetuning process makes slight adjustments to every weight in the pretrained model. While the model weights are substantial  we have other important aspects during training like Optimizer  which adds up to the cost. For example  Optimizer States  gradients  forward activation  and temporary memory. These additional components add up to the training cost.   Three main approaches are used in PEFT: Selective / reparameterization/additive.   1. SelectiveHere  we select a subset of initial LLM parameters to fine-tune.   2. ReparameterizationWe reparameterize model weights using a low-rank representation. We will discuss LoRA in detail below.   LORA: Low Rank Representation:   Each layer in a transformer architecture has multiple weight matrices for different operations  like self-attention or feed-forward networks. These matrices can have different sizes depending on the specific layer and configuration. Let us take an example by picking a matrix of size 512 x 64 = 32 768 parameters. Let us now see LoRA with rank = 8.   Original Weight Matrix: Dimensions: 512 x 64  Parameters: 32 768 (512 x 64)Matrix A (Rank Decomposition):\n",
            "\tScore\t 0.3870042742443449\n",
            "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "# First Question:\n",
        "response = chat_engine.chat( \"Use the tool to answer, How Parameter efficient fine tuning works?\" )\n",
        "\n",
        "display_res(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RRmiJEQ5R1Q",
        "outputId": "c5d06625-8771-4709-f718-c1605bc45e18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "\t Why don't scientists trust atoms?Because they make up everything!\n",
            "Sources:\n",
            "\tNo sources used!\n"
          ]
        }
      ],
      "source": [
        "# Second Question:\n",
        "response = chat_engine.chat(\"Tell me a joke?\")\n",
        "display_res(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eOzp5Xc5Vbj",
        "outputId": "b4978a3b-54d5-47b5-da45-2cff6844ad27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "\t The first question you asked was, \"How Parameter efficient fine tuning works?\"\n",
            "Sources:\n",
            "\tNo sources used!\n"
          ]
        }
      ],
      "source": [
        "# Third Question: (check if it can recall previous interactions)\n",
        "response = chat_engine.chat(\"What was the first question I asked?\")\n",
        "display_res(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "7jfiLpru5VZT"
      },
      "outputs": [],
      "source": [
        "# Reset the session to clear the memory\n",
        "chat_engine.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jt0q8RW25VXN",
        "outputId": "c6090b58-a830-4c68-d113-f7b8b049446e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "\t The first question you asked was, \"What was the first question I asked?\"\n",
            "Sources:\n",
            "\tNo sources used!\n"
          ]
        }
      ],
      "source": [
        "# Fourth Question: (don't recall the previous interactions.)\n",
        "response = chat_engine.chat(\"What was the first question I asked?\")\n",
        "display_res(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Egsib7yPJGR"
      },
      "source": [
        "# Streaming\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zanJeMbaPJcq",
        "outputId": "2add0a5f-83cc-4ffb-eb8c-a722a0a3f2f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG (Retrieval-Augmented Generation) and PEFT (Parameter-Efficient Fine-Tuning) are two advanced techniques used to enhance the capabilities of language models. RAG works by combining a retrieval mechanism with a generative model, where the system retrieves relevant documents or information from an external knowledge base to inform and enrich the language generation process. This allows RAG to produce responses that are not only contextually relevant but also informed by the latest available data. On the other hand, PEFT focuses on optimizing pre-trained models by making efficient adjustments to a small number of parameters, enabling the model to better adapt to specific tasks without undergoing full retraining. The primary difference between the two lies in their methodology: RAG enriches the generative process with real-time information retrieval, while PEFT streamlines the adaptation of models to new tasks through minimal parameter tuning, making it more resource-efficient. This makes RAG particularly powerful for scenarios requiring accurate and up-to-date information, while PEFT is ideal for quickly tailoring models to niche applications without extensive computational overhead."
          ]
        }
      ],
      "source": [
        "# Stream the words as soon as they are available instead of waiting for the model to finish generation.\n",
        "streaming_response = chat_engine.stream_chat(\n",
        "    \"Write a paragraph explaining how RAG and PEFT work, and highlight the differences between them.\"\n",
        ")\n",
        "streaming_response.print_response_stream()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuRgOJ2AHMJh"
      },
      "source": [
        "## Condense Question\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb2Lt41jq145"
      },
      "source": [
        "Enhance the input prompt by looking at the previous chat history along with the present question. The refined prompt can then be used to fetch the nodes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "v0gmM5LGIaRl"
      },
      "outputs": [],
      "source": [
        "# Define GPT-4 model that will be used by the chat_engine to improve the query.\n",
        "gpt4 = OpenAI(temperature=0.9, model=\"gpt-4o\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "EDWsaBTBIhK7"
      },
      "outputs": [],
      "source": [
        "chat_engine = vector_index.as_chat_engine(\n",
        "    chat_mode=\"condense_question\", llm=gpt4, verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4c--hJ75VU2",
        "outputId": "eecf0a71-1b0f-4c26-cea3-78393c0b19dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying with: How Retrieval-Augmented Generation (RAG) works, and which problem does it solve?\n",
            "Response:\n",
            "\t Retrieval-Augmented Generation (RAG) works by combining the strengths of pretraining and retrieval-based models to enhance the performance of large language models (LLMs). It addresses the common issues faced by LLMs, such as producing outdated information and fabricating facts. RAG works through a series of key processing steps: query classification, retrieval, reranking, repacking, and summarization. This workflow ensures that relevant and current information is incorporated into the generated responses, thereby improving accuracy, relevance, and reducing hallucinations. The retrieval component involves indexing and searching for relevant documents based on user queries, while the generation component formulates coherent responses using this retrieved information.\n",
            "Sources:\n",
            "\tNode ID\t 2aa05360-f43a-4819-bce7-0acf7b897eab\n",
            "\tText\t Generative large language models are prone to producing outdated information or fabricating facts, although they were aligned with human preferences by reinforcement learning [1] or lightweight alternatives [2–5]. Retrieval-augmented generation (RAG) techniques address these issues by combining the strengths of pretraining and retrieval-based models, thereby providing a robust framework for enhancing model performance [6]. Furthermore, RAG enables rapid deployment of applications for specific organizations and domains without necessitating updates to the model parameters, as long as query-related documents are provided. Many RAG approaches have been proposed to enhance large language models (LLMs) through query-dependent retrievals [6–8]. A typical RAG workflow usually contains multiple intervening processing steps: query classification (determining whether retrieval is necessary for a given input query), retrieval (efficiently obtaining relevant documents for the query), reranking (refining the order of retrieved documents based on their relevance to the query), repacking (organizing the retrieved documents into a structured one for better generation), summarization (extracting key information for response generation from the repacked document and eliminating redundancies) and models. Implementing RAG also requires decisions on the ways to properly split documents into chunks, the types of embeddings to use for semantically representing these chunks, the choice of\n",
            "\tScore\t 0.5995536258251682\n",
            "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "\tNode ID\t 1c324686-fad7-4a41-bfd3-d44f9612ca91\n",
            "\tText\t Authors: Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu.Numerous studies of Retrieval-Augmented Generation (RAG) systems have emerged from various perspectives since the advent of Large Language Models (LLMs). The RAG system comprises two primary components: Retrieval and Generation. The retrieval component aims to extract relevant information from various external knowledge sources. It involves two main phases, indexing and searching. Indexing organizes documents to facilitate efficient retrieval, using either inverted indexes for sparse retrieval or dense vector encoding for dense retrieval. The searching component utilizes these indexes to fetch relevant documents based on the user's query, often incorporating the optional rerankers to refine the ranking of the retrieved documents. The generation component utilizes the retrieved content and question query to formulate coherent and contextually relevant responses with the prompting and inferencing phases. As the \"Emerging\" ability of LLMs and the breakthrough in aligning human commands, LLMs are the best performance choices model for the generation stage. Prompting methods like Chain of Thought, Tree of Thought, Rephrase and Respond guide better generation results. In the inferencing step, LLMs interpret the prompted input to generate accurate and in-depth responses that align with the query's intent and integrate the extracted information without further finetuning, such as fully finetuning or LoRA. Appendix A details the complete RAG structure. Figure 1 illustrates the structure of the RAG systems as mentioned.\n",
            "\tScore\t 0.5818868709667797\n",
            "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "response = chat_engine.chat(\n",
        "    \"How Retrieval-Augmented Generation (RAG) works, and which problem does it solve?\"\n",
        ")\n",
        "display_res(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysL9ONePOsGB"
      },
      "source": [
        "## REACT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiEFmxAtrmF-"
      },
      "source": [
        "ReAct is an agent-based chat mode that uses a loop to decide on querying a data engine during interactions, offering flexibility but relying on the Large Language Model's quality for effective responses, requiring careful management to avoid inaccurate answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "-M1jWoKXOs2t"
      },
      "outputs": [],
      "source": [
        "chat_engine = vector_index.as_chat_engine(chat_mode=\"react\", verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZkEW1SSOs0H",
        "outputId": "b3b759f7-19d7-492e-e6a1-46efb2856da1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Which company developed Claude 3.5 Sonnet, and what is its primary application?\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\":\"Which company developed Claude 3.5 Sonnet, and what is its primary application?\"}\n",
            "Got output: The information provided does not contain any details regarding Claude 3.5 Sonnet or the company that developed it, including its primary application. Therefore, an answer to the query cannot be given based on the available context.\n",
            "========================\n",
            "\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\"input\":\"Who developed Claude 3.5 Sonnet and what is it used for?\"}\n",
            "Got output: Claude 3.5 Sonnet is developed by Anthropic. It serves as a free-tier model that offers a balance between cost and features, making it suitable for tasks like creative writing and answering questions, similar to other generative AI models.\n",
            "========================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = chat_engine.chat(\n",
        "    \"Which company developed Claude 3.5 Sonnet, and what is its primary application?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eW5P1lD4Osxf",
        "outputId": "fe8265e0-24c1-4700-9d71-9fc308bdccdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "\t Claude 3.5 Sonnet was developed by Anthropic. Its primary application includes tasks such as creative writing and answering questions, functioning similarly to other generative AI models.\n",
            "Sources:\n",
            "\tNode ID\t 55740ef4-3809-4dfa-ad06-e85bac4e165f\n",
            "\tText\t seeing. Most visual perception is handled by low-level processes that merely tell your brain \"that\\'s a water droplet\" without telling you details like where the lightest and darkest points are, or \"that\\'s a bush\" without telling you the shape and position of every leaf. This is a feature of brains, not a bug. In everyday life it would be distracting to notice every leaf on every bush. But when you have to paint something, you have to look more closely, and when you do there\\'s a lot to see. You can still be noticing new things after days of trying to paint something people usually take for granted, just as you can after days of trying to write an essay about something people usually take for granted.\\n\\nThis is not the only way to paint. I\\'m not 100% sure it\\'s even a good way to paint. But it seemed a good enough bet to be worth trying.\\n\\nOur teacher, professor Ulivi, was a nice guy. He could see I worked hard, and gave me a good grade, which he wrote down in a sort of passport each student had. But the Accademia wasn\\'t teaching me anything except Italian, and my money was running out, so at the end of the first year I went back to the US.\\n\\nI wanted to go back to RISD, but I was now broke and RISD was very expensive, so I decided to get a job for a year and then return to RISD the next fall. I got one at a company called Interleaf, which made software for creating documents. You mean like Microsoft Word? Exactly. That was how I learned that low end software tends to eat high end software. But Interleaf still had a few years to live yet. [5]\\n\\nInterleaf had done something pretty bold. Inspired by Emacs, they\\'d added a scripting language, and even made the scripting language a dialect of Lisp. Now they wanted a Lisp hacker to write things in it. This was the closest thing I\\'ve had to a normal job, and I hereby apologize to my boss and coworkers, because I was a bad employee. Their Lisp was the thinnest icing on a giant C cake, and since I didn\\'t know C and didn\\'t want to learn it, I never understood most of the software.\n",
            "\tScore\t 0.2510596247298863\n",
            "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "\tNode ID\t 03b1c975-f71f-4300-bf73-bbb602b8ee69\n",
            "\tText\t and had \"hardly any plot.\"2. Programming on an IBM 1401 computer in 9th grade, using an early version of Fortran language.3. Building simple games, a program to predict the height of model rockets, and a word processor for his father.4. Reading science fiction novels, such as \"The Moon is a Harsh Mistress\" by Heinlein, which inspired him to work on AI.5. Living in Florence, Italy, and walking through the city's streets to the Accademia.Please note that these activities are mentioned in the text and are not based on prior knowledge or assumptions.</b>### Streaming Support```pythonquery_engine = index.as_query_engine(streaming=True)response = query_engine.query(\"What happened at interleaf?\")for token in response.response_gen:    print(token, end=\"\")```     Based on the context information provided, it appears that the author worked at Interleaf, a company that made software for creating and managing documents. The author mentions that Interleaf was \"on the way down\" and that the company's Release Engineering group was large compared to the group that actually wrote the software. It is inferred that Interleaf was experiencing financial difficulties and that the author was nervous about money. However, there is no explicit mention of what specifically happened at Interleaf.\n",
            "\tScore\t 0.24688669688641435\n",
            "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "\tNode ID\t 4558b7d3-7f77-4f55-b0e7-64d385820117\n",
            "\tText\t 2. Dedicated to safety and security   It is a well-known fact that Anthropic prioritizes responsible AI development the most  and it is clearly seen in Claudes design. This generative AI model is trained on a carefully curated dataset thus it minimizes biases and factual errors to a large extent. On top of that  Claude also undergoes rigorous safety checks to prevent the generation of harmful and misleading content.   3. Emphasizes Explainability   While many of the AI and LLMs currently operate as black boxes  Claude offers a high level of explainability surpassing other models. This means it can explain the reasoning and decision-making process behind all of its responses. Therefore  it helps users to use this model confidently and they can be assured about the credibility of the information provided.   Claude FamilyClaude AI comes in a family of 3 generative AI models. Users can choose from these three models based on their power requirements and budget.   1. Haiku: It is the most budget-friendly option and offers fast response times. This can be perfect for simple tasks that require short context. This is yet to be launched but users can expect it to be highly cost-effective and cheaper as compared to other models.   2. Sonnet: This is a free-tier model and serves as an excellent starting point by offering a balance between cost and features. It can effectively handle tasks like writing different creative text formats and answering questions  just like Open AIs ChatGPT.   3. Opus: This is the most powerful generative AI model by Claude AI; however  users require a premium subscription to use this AI Chatbot. It can perform complex tasks easily that require a large context window. So  if you are looking for a generative AI that can do research  summarize lengthy documents  or help with consistent lengthy conversations  then this model will be the best option.   ChatGPT vs. Claude AI: How do they differ?Claude AI and OpenAIs ChatGPT both are very powerful LLM models. But they are designed for various purposes. Lets compare.   Strengths:    Claude: It is great in performing tasks requiring long-term context as discussed above. They can maintain consistency throughout their response in extended conversations. Also  their explainability\n",
            "\tScore\t 0.30697362787655164\n",
            "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "\tNode ID\t 0e97549b-bf5a-4e4f-9f5a-d6a478126874\n",
            "\tText\t Claude AI and ChatGPT are both powerful and popular generative AI models revolutionizing various aspects of our lives. Here  let us learn more about Claude AI and its benefits   Ever since the launch of ChatGPT  many other companies have also joined the race to bring excellent generative AI models into the world that not only help users create realistic content but are also safe to use  and free from bias. While Open AIs ChatGPT and Googles Bard  now Gemini  get most of the limelight  Claude AI stands out for its impressive features and being the most reliable and ethical Large Language Model.   In this article  we will learn more about what Claude AI is and what are its unique features. We will also discuss how it differs from the most popular generative AI tool ChatGPT.   Claude AI is developed by Anthropic  an AI startup company backed by Google and Amazon  and is dedicated to developing safe and beneficial AI. Claude AI is an LLM based on the powerful transformer architecture and like OpenAIs ChatGPT  it can generate text  translate languages  as well as write different kinds of compelling content. It can interact with users like a normal AI chatbot; however  it also boasts some unique features that make it different from others.   1. Larger Context Window   One of the Claude AIs biggest capabilities is that it can process huge chunks of text as compared to ChatGPT. While ChatGPT struggles to process and keep track of information in long conversations  Claudes context window is huge (spanning up to 150 pages)  which helps users to do more coherent and consistent conversations  especially when it comes to long documents.   2. Dedicated to safety and security   It is a well-known fact that Anthropic prioritizes responsible AI development the most  and it is clearly seen in Claudes design. This generative AI model is trained on a carefully curated dataset thus it minimizes biases and factual errors to a large extent. On top of that  Claude also undergoes rigorous safety checks to prevent the generation of harmful and misleading content.   3. Emphasizes Explainability   While many of the AI and LLMs currently operate as black boxes  Claude offers a high level of explainability surpassing other models. This means it\n",
            "\tScore\t 0.2730484696300516\n",
            "\t-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "display_res(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zf6r2AmFOsca"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "92c130aace5f4b19b0aac1751899551c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92519659dadf4aefab4ae34050cee3e2",
              "IPY_MODEL_35af66cc2e6541819b0c871dbe9c4120",
              "IPY_MODEL_f0d65c04c4bf475aaa636ab93c7d3cb4"
            ],
            "layout": "IPY_MODEL_a1b77d4363d04939ada84f153714be1e"
          }
        },
        "92519659dadf4aefab4ae34050cee3e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_731be95a92e44174b9d8971f02cacf86",
            "placeholder": "​",
            "style": "IPY_MODEL_57f98df9d10e47d2bf005b909340e855",
            "value": "ai_tutor_knowledge.jsonl: 100%"
          }
        },
        "35af66cc2e6541819b0c871dbe9c4120": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75ae7d9892f94db9a54455456aada690",
            "max": 6960611,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2e8bd99c1a5e4b0382406c916b7ef6b7",
            "value": 6960611
          }
        },
        "f0d65c04c4bf475aaa636ab93c7d3cb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_430432c14eae4621a9520ae66023be80",
            "placeholder": "​",
            "style": "IPY_MODEL_58c932f0ea5f4423ab92963b4080637a",
            "value": " 6.96M/6.96M [00:01&lt;00:00, 5.82MB/s]"
          }
        },
        "a1b77d4363d04939ada84f153714be1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "731be95a92e44174b9d8971f02cacf86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57f98df9d10e47d2bf005b909340e855": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75ae7d9892f94db9a54455456aada690": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e8bd99c1a5e4b0382406c916b7ef6b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "430432c14eae4621a9520ae66023be80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58c932f0ea5f4423ab92963b4080637a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}