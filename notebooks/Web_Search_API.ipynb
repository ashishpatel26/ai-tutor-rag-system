{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JboB5VaCJUrb",
        "outputId": "4c658214-e211-40dd-a1b2-1c94572fb844"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.0/337.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.6/296.6 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.8/176.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-index==0.10.57 openai==1.37.0 tiktoken==0.7.0 llama-index-tools-google==0.1.3 newspaper4k==0.9.3.1 lxml_html_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NKAn5scN_g9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set the following API Keys in the Python environment. Will be used later.\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"[OPENAI_API_KEY]\"\n",
        "GOOGLE_SEARCH_KEY = \"GOOGLE_SEARCH_KEY\"\n",
        "GOOGLE_SEARCH_ENGINE = \"GOOGLE_SEARCH_ENGINE\" # Search Engine ID"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM and Embedding Model"
      ],
      "metadata": {
        "id": "Yp-tXudCc3Zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature= 0)\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
      ],
      "metadata": {
        "id": "dYmz-uAIc2rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex1gQVHvITMI"
      },
      "source": [
        "# Using Agents/Tools\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.core.agent import ReActAgent\n",
        "\n",
        "\n",
        "# define sample Tool\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply two integers and returns the result integer\"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
        "\n",
        "# initialize ReAct agent\n",
        "agent = ReActAgent.from_tools([multiply_tool], verbose=True)"
      ],
      "metadata": {
        "id": "OHd8WbUoZHWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = agent.chat(\"What is the multiplication of 43 and 45?\")\n",
        "\n",
        "# Final response\n",
        "res.response"
      ],
      "metadata": {
        "id": "9JGVdS6AZGve",
        "outputId": "d4bd7f24-c0c3-490e-ba6e-a3dff39da3bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Running step 1d3f5ada-78ed-4e7a-ae7a-c2fc5f152858. Step input: What is the multiplication of 43 and 45?\n",
            "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
            "Action: multiply\n",
            "Action Input: {'a': 43, 'b': 45}\n",
            "\u001b[0m\u001b[1;3;34mObservation: 1935\n",
            "\u001b[0m> Running step 02f814ea-89fb-4b30-b41b-e7a262415a05. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n",
            "Answer: The multiplication of 43 and 45 is 1935.\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The multiplication of 43 and 45 is 1935.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LMypoqUyuXq"
      },
      "source": [
        "## Define Google Search Tool\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Q7sc69nJvWI"
      },
      "outputs": [],
      "source": [
        "from llama_index.tools.google import GoogleSearchToolSpec\n",
        "\n",
        "tool_spec = GoogleSearchToolSpec(key=GOOGLE_SEARCH_KEY, engine=GOOGLE_SEARCH_ENGINE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrbuIOaMeOIf"
      },
      "outputs": [],
      "source": [
        "# Import and initialize our tool spec\n",
        "from llama_index.core.tools.tool_spec.load_and_search import LoadAndSearchToolSpec\n",
        "\n",
        "# Wrap the google search tool to create an index on top of the returned Google search\n",
        "wrapped_tool = LoadAndSearchToolSpec.from_defaults(\n",
        "    tool_spec.to_tool_list()[0],\n",
        ").to_tool_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3ENpLyBy7UL"
      },
      "source": [
        "## Create the Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_Ab47ppK8b2"
      },
      "outputs": [],
      "source": [
        "from llama_index.agent.openai import OpenAIAgent\n",
        "\n",
        "agent = OpenAIAgent.from_tools(wrapped_tool, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcUyz1-FlCQ8"
      },
      "outputs": [],
      "source": [
        "res = agent.chat(\"How many parameters LLaMA 3.2 model has?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "w4wK5sY-lOOv",
        "outputId": "6e40f9fc-76a9-4006-cd21-705c5e66bc1e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The LLaMA 3.2 model has the following parameters:\\n- Vision LLMs: 11 billion and 90 billion parameters\\n- Lightweight text-only models: 1 billion and 3 billion parameters.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "res.response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TM_cvBA1nTJM",
        "outputId": "5a4a5cc9-c630-45bb-9fb1-757e79311261",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[ToolOutput(content='Content loaded! You can now search the information using read_google_search', tool_name='google_search', raw_input={'args': (), 'kwargs': {'query': 'LLaMA 3.2 model parameters'}}, raw_output='Content loaded! You can now search the information using read_google_search', is_error=False),\n",
              " ToolOutput(content='The LLaMA 3.2 model features small and medium-sized vision LLMs with 11 billion and 90 billion parameters, as well as lightweight text-only models with 1 billion and 3 billion parameters.', tool_name='read_google_search', raw_input={'args': (), 'kwargs': {'query': 'LLaMA 3.2 model parameters'}}, raw_output='The LLaMA 3.2 model features small and medium-sized vision LLMs with 11 billion and 90 billion parameters, as well as lightweight text-only models with 1 billion and 3 billion parameters.', is_error=False)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "res.sources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "who-NM4pIhPn"
      },
      "source": [
        "# Using Tools w/ VectorStoreIndex\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9g9cTM9GI-19"
      },
      "source": [
        "A limitation of the current agent/tool in LlamaIndex is that it **relies solely on the page description from the retrieved pages** to answer questions. This approach will miss answers that are not visible in the page's description tag. To address this, a possible workaround is to fetch the page results, extract the page content using the newspaper3k library, and then create an index based on the downloaded content. Also, the previous method stacks all retrieved items from the search engine into a single document, making it **difficult to pinpoint the exact source** of the response. However, the following method will enable us to present the sources easily.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31G_fxxJIsbC"
      },
      "source": [
        "## Define Google Search Tool\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwRmj2odIHxt"
      },
      "outputs": [],
      "source": [
        "from llama_index.tools.google import GoogleSearchToolSpec\n",
        "\n",
        "tool_spec = GoogleSearchToolSpec(key=GOOGLE_SEARCH_KEY, engine=GOOGLE_SEARCH_ENGINE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVIxdj04Bsf2"
      },
      "outputs": [],
      "source": [
        "search_results = tool_spec.google_search(\"LLaMA 3.2 model details\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlYDNfg2BsdQ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "search_results = json.loads(search_results[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHALd3uhIxtQ"
      },
      "source": [
        "## Read Each URL Contents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXz3JFduBsaq",
        "outputId": "e814fa12-ad41-4723-a283-dbd9b03419ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:newspaper.network:get_html_status(): bad status code 403 on URL: https://www.reddit.com/r/LocalLLaMA/comments/1fqhjs9/is_llama_32_banned_to_use_in_eu/, html: <body class=theme-beta><div><style>.theme-light,:root{--rem360:22.5rem;--rem320:20rem;--rem192:12rem;--rem144:9rem;--rem128:8rem;--rem96:6rem;--rem90:5.625rem;--rem88:5.5rem;--rem64:4rem;--rem56:3.5re\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "import newspaper\n",
        "\n",
        "pages_content = []\n",
        "\n",
        "for item in search_results[\"items\"]:\n",
        "\n",
        "    try:\n",
        "        article = newspaper.Article(item[\"link\"])\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        if len(article.text) > 0:\n",
        "            pages_content.append(\n",
        "                {\"url\": item[\"link\"], \"text\": article.text, \"title\": item[\"title\"]}\n",
        "            )\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "print(len(pages_content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqxa_qRVI3G0"
      },
      "source": [
        "## Create the Index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4PkK8DuBsZT"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Document\n",
        "\n",
        "# Convert the texts to Document objects so the LlamaIndex framework can process them.\n",
        "documents = [\n",
        "    Document(text=row[\"text\"], metadata={\"title\": row[\"title\"], \"url\": row[\"url\"]})\n",
        "    for row in pages_content\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RtMBWpgBsWX"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "# Build index / generate embeddings using OpenAI.\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    transformations=[SentenceSplitter(chunk_size=512, chunk_overlap=128)],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xV_ibEZ_BsM4"
      },
      "outputs": [],
      "source": [
        "# Define a query engine that is responsible for retrieving related pieces of text,\n",
        "# and using a LLM to formulate the final answer.\n",
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nziwu27MI6ih"
      },
      "source": [
        "## Query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xea7ZeidH27i",
        "outputId": "f77e4f9c-46bf-4d68-b61b-a9e909ecc5eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Llama 3.2 models have the following parameter sizes: 1B, 3B, 11B, and 90B.\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\"How many parameters LLaMA 3.2 model has? list exact sizes of the models\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QpGPD5nHORP",
        "outputId": "ab09485b-dfd8-4573-cdcd-ef827bafa2f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title\t Introducing Llama 3.2 models from Meta in Amazon Bedrock: A new ...\n",
            "Source\t https://aws.amazon.com/blogs/aws/introducing-llama-3-2-models-from-meta-in-amazon-bedrock-a-new-generation-of-multimodal-vision-and-lightweight-models/\n",
            "Score\t 0.5448207816546468\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Title\t meta-llama/Llama-3.2-90B-Vision-Instruct - Demo - DeepInfra\n",
            "Source\t https://deepinfra.com/meta-llama/Llama-3.2-90B-Vision-Instruct\n",
            "Score\t 0.5350659161754009\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "# Show the retrieved nodes\n",
        "for src in response.source_nodes:\n",
        "    print(\"Title\\t\", src.metadata[\"title\"])\n",
        "    print(\"Source\\t\", src.metadata[\"url\"])\n",
        "    print(\"Score\\t\", src.score)\n",
        "    print(\"-_\" * 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5b4nZ-qHpdP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}