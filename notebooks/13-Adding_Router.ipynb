{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/towardsai/ai-tutor-rag-system/blob/main/notebooks/13-Adding_Router.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zE1h0uQV7uT"
      },
      "source": [
        "# Install Packages and Setup Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPJzr-I9XQ7l",
        "outputId": "3b178458-8322-456f-e036-2ba0c2c6df79",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.0/337.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.3/584.3 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m679.1/679.1 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.3/176.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for html2text (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for durationpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-index==0.10.57 openai==1.37.0 cohere==5.6.2 tiktoken==0.7.0 chromadb==0.5.5 html2text sentence_transformers pydantic llama-index-vector-stores-chroma==0.1.10 kaleido==0.2.1 llama-index-llms-gemini==0.1.11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "riuXwpSPcvWC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set the following API Keys in the Python environment. Will be used later.\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_OPENAI_API_KEY>\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"<YOUR_GOOGLE_API_KEY>\"\n",
        "\n",
        "# from google.colab import userdata\n",
        "# os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api_key')\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = userdata.get('Google_api_key')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jIEeZzqLbz0J"
      },
      "outputs": [],
      "source": [
        "# Allows running asyncio in environments with an existing event loop, like Jupyter notebooks.\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bkgi2OrYzF7q"
      },
      "source": [
        "# Load a Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9oGT6crooSSj"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core import Settings\n",
        "\n",
        "Settings.llm = OpenAI(temperature=1, model=\"gpt-4o-mini\")\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: You can create a vector store from scratch using the code below, or you can load it from Hugging Face using the code provided in this notebook.**"
      ],
      "metadata": {
        "id": "HKWdt7Ig3ZKh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BwVuJXlzHVL"
      },
      "source": [
        "# Create a VectoreStore\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SQP87lHczHKc",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "\n",
        "# create client and a new collection\n",
        "# chromadb.EphemeralClient saves data in-memory.\n",
        "\n",
        "chroma_client = chromadb.PersistentClient(path=\"./ai_tutor_knowledge\")\n",
        "chroma_collection = chroma_client.create_collection(\"ai_tutor_knowledge\")\n",
        "\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9JbAzFcjkpn"
      },
      "source": [
        "# Load the Dataset (JSON)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceveDuYdWCYk"
      },
      "source": [
        "### Download\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZwf6pv7WFmD"
      },
      "source": [
        "The dataset includes several articles from the TowardsAI blog, Research paper contents and documentation from various resources. Which includes the various GenAI Concepts like RAG, PEFT, BERT Model, and Other concepts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "535244513b504cf0ae900cbb9873ca0a",
            "7a0fb7e522e040e7b3d218006c090201",
            "4f2d401e824845d9b23b848421fa95e0",
            "02e0480193574ebc97d082d429615fc6",
            "cc6df9ddbe004f4d901ac4a35dd49c91",
            "24cbfe06ba4c4c9fb010ab74485c92fa",
            "1daa2d514e4848a7ac602be83fdd6d0e",
            "ad02a9bfcd3642fb834f3fb452efaf10",
            "d24b90385f884b8b921685f4fe1f6742",
            "cd77c85e0680449caeabbf4c5febaf24",
            "b2edcdbabf3c424788745bdc72504d59"
          ]
        },
        "id": "wl_pbPvMlv1h",
        "outputId": "d61e9faf-1e9f-4d3d-df77-4d9e529b425c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ai_tutor_knowledge.jsonl:   0%|          | 0.00/6.96M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "535244513b504cf0ae900cbb9873ca0a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "file_path = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"ai_tutor_knowledge.jsonl\",repo_type=\"dataset\",local_dir=\"/content\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWBLtDbUWJfA"
      },
      "source": [
        "## Read File\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Q9sxuW0g3Gd",
        "outputId": "219dcedf-8698-4937-bf69-8cea8836a225"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "762"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import json\n",
        "with open(file_path, \"r\") as file:\n",
        "    ai_tutor_knowledge = [json.loads(line) for line in file]\n",
        "\n",
        "len(ai_tutor_knowledge)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S17g2RYOjmf2"
      },
      "source": [
        "# Convert to Document obj\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YizvmXPejkJE"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from llama_index.core import Document\n",
        "\n",
        "def create_docs_from_list(data_list: List[dict]) -> List[Document]:\n",
        "    documents = []\n",
        "    for data in data_list:\n",
        "        documents.append(\n",
        "            Document(\n",
        "                doc_id=data[\"doc_id\"],\n",
        "                text=data[\"content\"],\n",
        "                metadata={  # type: ignore\n",
        "                    \"url\": data[\"url\"],\n",
        "                    \"title\": data[\"name\"],\n",
        "                    \"tokens\": data[\"tokens\"],\n",
        "                    \"source\": data[\"source\"],\n",
        "                },\n",
        "                excluded_llm_metadata_keys=[\n",
        "                    \"title\",\n",
        "                    \"tokens\",\n",
        "                    \"source\",\n",
        "                ],\n",
        "                excluded_embed_metadata_keys=[\n",
        "                    \"url\",\n",
        "                    \"tokens\",\n",
        "                    \"source\",\n",
        "                ],\n",
        "            )\n",
        "        )\n",
        "    return documents\n",
        "\n",
        "doc = create_docs_from_list(ai_tutor_knowledge)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjuLbmFuWsyl"
      },
      "source": [
        "# Transforming\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9z3t70DGWsjO"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.text_splitter import TokenTextSplitter\n",
        "\n",
        "# Define the splitter object that split the text into segments with 512 tokens,\n",
        "# with a 128 overlap between the segments.\n",
        "text_splitter = TokenTextSplitter(separator=\" \", chunk_size=512, chunk_overlap=128)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.extractors import (\n",
        "    SummaryExtractor,\n",
        "    QuestionsAnsweredExtractor,\n",
        "    KeywordExtractor,\n",
        ")\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "\n",
        "pipeline = IngestionPipeline(\n",
        "    transformations=[\n",
        "        text_splitter,\n",
        "        QuestionsAnsweredExtractor(questions=3, llm = Settings.llm),\n",
        "        SummaryExtractor(summaries=[\"prev\", \"self\"], llm = Settings.llm),\n",
        "        KeywordExtractor(keywords=10, llm = Settings.llm),\n",
        "        OpenAIEmbedding(model=\"text-embedding-3-small\"),\n",
        "    ],\n",
        "    vector_store=vector_store,\n",
        ")\n",
        "\n",
        "nodes = pipeline.run(documents=doc, show_progress=True)"
      ],
      "metadata": {
        "id": "KO-Piomu4DH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compress the vector store directory to a zip file to be able to download and use later.\n",
        "!zip -r vectorstore.zip ai_tutor_knowledge"
      ],
      "metadata": {
        "id": "lhioZf2m4GwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWaT6rL7ksp8"
      },
      "source": [
        "# Load Indexes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: If you created the vector store from scratch, please comment out the three code blocks/cells below.**"
      ],
      "metadata": {
        "id": "Fr-ytVUA4OAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading Vector store from Hugging face hub\n",
        "from huggingface_hub import hf_hub_download\n",
        "vectorstore = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"vectorstore.zip\",repo_type=\"dataset\",local_dir=\"/content\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "50945377703b431484721a78f514068a",
            "653db9bdad1345ab8fee6ef552b12743",
            "7cb14142a86f4243b95be4a9bb6e850d",
            "9b689571805240aeac1cd279edf8cd85",
            "3fe1860885934c779056586b05cd17b5",
            "9c08f60329ec41d2a065beea014b7c75",
            "6e0e7f99b04f4ebdbaf43c03ad17be69",
            "768f382510194b339510d3c84963f198",
            "ac28593124da4fbba2f920b6791e426a",
            "bd46095fa65a414cbb847889566311a7",
            "a07a41f9d13949aa9e2adb64715b84f1"
          ]
        },
        "id": "qyuq48p_4KWu",
        "outputId": "9d6f19a8-6cce-4579-9cb9-039959503b58"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vectorstore.zip:   0%|          | 0.00/97.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50945377703b431484721a78f514068a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SodY2Xpf_kxg",
        "outputId": "b235fc88-ad35-44eb-8be8-6c2090f8dd92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  vectorstore.zip\n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/length.bin  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/index_metadata.pickle  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/link_lists.bin  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/header.bin  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/data_level0.bin  \n",
            "  inflating: ai_tutor_knowledge/chroma.sqlite3  \n"
          ]
        }
      ],
      "source": [
        "!unzip vectorstore.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mXi56KTXk2sp"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "\n",
        "# Create your index\n",
        "db = chromadb.PersistentClient(path=\"./ai_tutor_knowledge\")\n",
        "chroma_collection = db.get_or_create_collection(\"ai_tutor_knowledge\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "jKXURvLtkuTS"
      },
      "outputs": [],
      "source": [
        "# Create your index\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "vector_index = VectorStoreIndex.from_vector_store(vector_store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "lgpzgnHFV_MH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4470510-1d38-4abd-ede8-dea915ef11ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty Response\n"
          ]
        }
      ],
      "source": [
        "# Query Engine\n",
        "ai_tutor_knowledge_query_engine = vector_index.as_query_engine(similarity_top_k=3)\n",
        "\n",
        "res = ai_tutor_knowledge_query_engine.query(\"How Retrieval Augmented Generation (RAG) Works?\")\n",
        "print(res.response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "cKN0brIPV_MJ"
      },
      "outputs": [],
      "source": [
        "for src in res.source_nodes:\n",
        "    print(\"Node ID\\t\", src.node_id)\n",
        "    print(\"Title\\t\", src.metadata[\"title\"])\n",
        "    print(\"Text\\t\", src.text)\n",
        "    print(\"Score\\t\", src.score)\n",
        "    print(\"Metadata\\t\", src.metadata)\n",
        "    print(\"-_\" * 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXqjul4MV_MJ"
      },
      "source": [
        "# Router\n",
        "\n",
        "Routers are modules that take in a user query and a set of “choices” (defined by metadata), and returns one or more selected choices.\n",
        "\n",
        "They can be used for the following use cases and more:\n",
        "\n",
        "- Selecting the right data source among a diverse range of data sources\n",
        "\n",
        "- Deciding whether to do summarization (e.g. using summary index query engine) or semantic search (e.g. using vector index query engine)\n",
        "\n",
        "- Deciding whether to “try” out a bunch of choices at once and combine the results (using multi-routing capabilities).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uunnC6XjV_Mb"
      },
      "source": [
        "## Lets create a different query engine with Mistral AI information\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import requests\n",
        "\n",
        "wiki_titles = [\n",
        "    \"Mistral AI\",\n",
        "    \"Llama (language model)\",\n",
        "    \"Claude AI\",\n",
        "    \"OpenAI\",\n",
        "    \"Gemini AI\",\n",
        "]\n",
        "\n",
        "data_path = Path(\"llm_data_wiki\")\n",
        "\n",
        "if not data_path.exists():\n",
        "    data_path.mkdir()\n",
        "\n",
        "for title in wiki_titles:\n",
        "    response = requests.get(\n",
        "        \"https://en.wikipedia.org/w/api.php\",\n",
        "        params={\n",
        "            \"action\": \"query\",\n",
        "            \"format\": \"json\",\n",
        "            \"titles\": title,\n",
        "            \"prop\": \"extracts\",\n",
        "            \"explaintext\": True,\n",
        "        },\n",
        "    ).json()\n",
        "\n",
        "    page = next(iter(response[\"query\"][\"pages\"].values()))\n",
        "\n",
        "    if \"extract\" in page:\n",
        "        wiki_text = page[\"extract\"]\n",
        "\n",
        "        with open(data_path / \"llm_data_wiki.txt\", \"a\") as fp:\n",
        "            fp.write(f\"Title: {title}\\n{wiki_text}\\n\\n\")\n",
        "    else:\n",
        "        print(f\"No extract found for '{title}'\")\n"
      ],
      "metadata": {
        "id": "AXnP6NfcBCjA"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "OsSsh4_bV_Mb",
        "outputId": "a82cec8e-ce1d-41c4-d9bc-3dc544834197",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 46/46 [00:25<00:00,  1.83it/s]\n",
            "100%|██████████| 46/46 [00:58<00:00,  1.26s/it]\n",
            "100%|██████████| 46/46 [00:09<00:00,  4.61it/s]\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.core.text_splitter import TokenTextSplitter\n",
        "from llama_index.core.extractors import (\n",
        "    SummaryExtractor,\n",
        "    QuestionsAnsweredExtractor,\n",
        "    KeywordExtractor,\n",
        ")\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "\n",
        "# Assuming you have prepared a directory for llm data\n",
        "documents = SimpleDirectoryReader(\"llm_data_wiki\").load_data()\n",
        "\n",
        "text_splitter = TokenTextSplitter(separator=\" \", chunk_size=512, chunk_overlap=128)\n",
        "\n",
        "transformations = [\n",
        "    text_splitter,\n",
        "    QuestionsAnsweredExtractor(questions=2),\n",
        "    SummaryExtractor(summaries=[\"prev\", \"self\"]),\n",
        "    KeywordExtractor(keywords=10),\n",
        "    OpenAIEmbedding(model=\"text-embedding-3-small\"),\n",
        "]\n",
        "\n",
        "llm_index = VectorStoreIndex.from_documents( documents= documents, transformations = transformations )\n",
        "\n",
        "llm_query_engine = llm_index.as_query_engine( similarity_top_k=2,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "p3hDxvMuV_Mc"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import PydanticSingleSelector, LLMSingleSelector\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "from llama_index.core import VectorStoreIndex, SummaryIndex\n",
        "\n",
        "# initialize tools\n",
        "ai_tutor_knowledge_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=ai_tutor_knowledge_query_engine,\n",
        "    description=\"Useful for questions about the General Generative AI Concepts\",\n",
        ")\n",
        "llm_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=llm_query_engine,\n",
        "    description=\"Useful for questions about Particular LLMs like Mistral, Claude, OpenAI, Gemini\",\n",
        ")\n",
        "\n",
        "# initialize router query engine (single selection, pydantic)\n",
        "query_engine = RouterQueryEngine(\n",
        "    selector=PydanticSingleSelector.from_defaults(),\n",
        "    query_engine_tools=[\n",
        "        ai_tutor_knowledge_tool,\n",
        "        llm_tool,\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9nnZaOPuV_Mc",
        "outputId": "a3692962-4ebf-416c-cda4-77ac0260b4e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Llama model, developed by Meta AI, is a series of large language models initially released in February 2023. These models are available in varying sizes, ranging from 1 billion to 405 billion parameters, and include foundational models as well as instruction fine-tuned versions. The Llama models have undergone several updates, with Llama 2 introducing enhanced accessibility and some commercial use, while Llama 3 expanded its features to include capabilities for virtual assistants on platforms like Facebook and WhatsApp. The focus of the Llama models is on improving language fluency and performance through various training techniques, including reinforcement learning with human feedback.\n"
          ]
        }
      ],
      "source": [
        "res = query_engine.query(\n",
        "    \"What is the LLama model?\",\n",
        ")\n",
        "print(res.response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "zLVDAW7hV_Md",
        "outputId": "d8a01cf7-4d4c-42ad-a36a-61eb3f51de5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node ID\t ac838436-b773-4136-b618-0bbbac7e0f3f\n",
            "Text\t of Llama were made available to the research community under a non-commercial license, and access was granted on a case-by-case basis. Unauthorized copies of the model were shared via BitTorrent. In response, Meta AI issued DMCA takedown requests against repositories sharing the link on GitHub. Subsequent versions of Llama were made accessible outside academia and released under licenses that permitted some commercial use. Llama models are trained at different parameter sizes, ranging between 1B and 405B. Originally, Llama was only available as a foundation model. Starting with Llama 2, Meta AI started releasing instruction fine-tuned versions alongside foundation models.\n",
            "Alongside the release of Llama 3, Meta added virtual assistant features to Facebook and WhatsApp in select regions, and a standalone website. Both services use a Llama 3 model.\n",
            "\n",
            "\n",
            "== Background ==\n",
            "After the release of large language models such as GPT-3, a focus of research was up-scaling models which in some instances showed major increases in emergent capabilities. The release of ChatGPT and its surprise success caused an increase in attention to large language models.\n",
            "Compared with other responses to ChatGPT, Meta's Chief AI scientist Yann LeCun stated that large language models are best for aiding with writing.\n",
            "An empirical investigation of the Llama series was the scaling laws. It was observed that the Llama 3 models showed that when a model is trained on data that is more than the \"Chinchilla-optimal\" amount, the performance continues to scale log-linearly. For example, the Chinchilla-optimal dataset for Llama 3 8B is 200 billion tokens, but performance continued to scale log-linearly to the 75-times larger dataset of 15 trillion tokens.\n",
            "\n",
            "\n",
            "== Initial release ==\n",
            "LLaMA was announced on February 24, 2023, via a blog post and a paper describing the model's training, architecture, and performance. The inference code used to run the model was publicly released under the open-source GPLv3 license. Access to the model's weights was managed by an application process, with access to be granted \"on a case-by-case basis to academic researchers; those affiliated with organizations in government, civil society, and academia; and industry research laboratories around the world\".\n",
            "Llama was trained on only publicly available information, and was trained at various model sizes, with the intention to make it more accessible to different\n",
            "Score\t 0.5959119219351083\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t 15192b3d-f00b-4395-a103-c0f40ff629de\n",
            "Text\t Optimization (PPO) for RLHF – a new technique based on Rejection sampling was used, followed by PPO.\n",
            "Multi-turn consistency in dialogs was targeted for improvement, to make sure that \"system messages\" (initial instructions, such as \"speak in French\" and \"act like Napoleon\") are respected during the dialog. This was accomplished using the new \"Ghost attention\" technique during training, which concatenates relevant instructions to each new user message but zeros out the loss function for tokens in the prompt (earlier parts of the dialog).\n",
            "\n",
            "\n",
            "== Applications ==\n",
            "The Stanford University Institute for Human-Centered Artificial Intelligence (HAI) Center for Research on Foundation Models (CRFM) released Alpaca, a training recipe based on the LLaMA 7B model that uses the \"Self-Instruct\" method of instruction tuning to acquire capabilities comparable to the OpenAI GPT-3 series text-davinci-003 model at a modest cost. The model files were officially removed on March 21, 2023, over hosting costs and safety concerns, though the code and paper remain online for reference.\n",
            "Meditron is a family of Llama-based finetuned on a corpus of clinical guidelines, PubMed papers, and articles. It was created by researchers at École Polytechnique Fédérale de Lausanne School of Computer and Communication Sciences, and the Yale School of Medicine. It shows increased performance on medical-related benchmarks such as MedQA and MedMCQA.\n",
            "Zoom used Meta Llama 2 to create an AI Companion that can summarize meetings, provide helpful presentation tips, and assist with message responses. This AI Companion is powered by multiple models, including Meta Llama 2.\n",
            "\n",
            "\n",
            "=== llama.cpp ===\n",
            "\n",
            "Software developer Georgi Gerganov released llama.cpp as open-source on March 10, 2023. It's a re-implementation of LLaMA in C++, allowing systems without a powerful GPU to run the model locally. The llama.cpp project introduced the GGUF file format, a binary format that stores both tensors and metadata. The format focuses on supporting different quantization types, which can reduce memory usage, and increase speed at the expense of lower model precision.\n",
            "llamafile created by Justine Tunney is an open-source tool that bundles llama.cpp with the model into a single executable file. Tunney et al. introduced new optimized matrix multiplication kernels for x86 and ARM CPUs, improving prompt\n",
            "Score\t 0.5948661836923508\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "for src in res.source_nodes:\n",
        "    print(\"Node ID\\t\", src.node_id)\n",
        "    print(\"Text\\t\", src.text)\n",
        "    print(\"Score\\t\", src.score)\n",
        "    print(\"-_\" * 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "nvqkYeF9V_Md",
        "outputId": "cdcbb064-811b-42b9-c380-d9696655838f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty Response\n"
          ]
        }
      ],
      "source": [
        "res = query_engine.query(\"Explain parameter Efficient Fine tuning method\")\n",
        "print(res.response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "yNKJdJhzV_Md"
      },
      "outputs": [],
      "source": [
        "for src in res.source_nodes:\n",
        "    print(\"Node ID\\t\", src.node_id)\n",
        "    print(\"Text\\t\", src.text)\n",
        "    print(\"Score\\t\", src.score)\n",
        "    print(\"-_\" * 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcDZ5Dn8V_Md"
      },
      "source": [
        "# OpenAI Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "8G8dyrsFV_Me"
      },
      "outputs": [],
      "source": [
        "from llama_index.agent.openai import OpenAIAgent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "z5d0WpDeV_Me"
      },
      "outputs": [],
      "source": [
        "system_message_openai_agent = \"\"\"You are an AI teacher, answering questions from students of an applied AI course on Large Language Models (LLMs or llm) and Retrieval Augmented Generation (RAG) for LLMs. Topics covered include training models, fine-tuning models, giving memory to LLMs, prompting tips, hallucinations and bias, vector databases, transformer architectures, embeddings, RAG frameworks, Langchain, LlamaIndex, making LLMs interact with tools, AI agents, reinforcement learning with human feedback. Questions should be understood in this context.\n",
        "\n",
        "Your answers are aimed to teach students, so they should be complete, clear, and easy to understand.\n",
        "\n",
        "Use the available tools to gather insights pertinent to the field of AI. Always use two tools at the same time. These tools accept a string (a user query rewritten as a statement) and return informative content regarding the domain of AI.\n",
        "e.g:\n",
        "User question: 'How can I fine-tune an LLM?'\n",
        "Input to the tool: 'Fine-tuning an LLM'\n",
        "\n",
        "User question: How can quantize an LLM?\n",
        "Input to the tool: 'Quantization for LLMs'\n",
        "\n",
        "User question: 'Teach me how to build an AI agent\"'\n",
        "Input to the tool: 'Building an AI Agent'\n",
        "\n",
        "Only some information returned by the tools might be relevant to the question, so ignore the irrelevant part and answer the question with what you have.\n",
        "\n",
        "Your responses are exclusively based on the output provided by the tools. Refrain from incorporating information not directly obtained from the tool's responses.\n",
        "\n",
        "When the conversation deepens or shifts focus within a topic, adapt your input to the tools to reflect these nuances. This means if a user requests further elaboration on a specific aspect of a previously discussed topic, you should reformulate your input to the tool to capture this new angle or more profound layer of inquiry.\n",
        "\n",
        "Provide comprehensive answers, ideally structured in multiple paragraphs, drawing from the tool's variety of relevant details. The depth and breadth of your responses should align with the scope and specificity of the information retrieved.\n",
        "\n",
        "Should the tools repository lack information on the queried topic, politely inform the user that the question transcends the bounds of your current knowledge base, citing the absence of relevant content in the tool's documentation.\n",
        "\n",
        "At the end of your answers, always invite the students to ask deeper questions about the topic if they have any. Make sure to reformulate the question to the tool to capture this new angle or more profound layer of inquiry.\n",
        "\n",
        "Do not refer to the documentation directly, but use the information provided within it to answer questions.\n",
        "\n",
        "If code is provided in the information, share it with the students. It's important to provide complete code blocks so they can execute the code when they copy and paste them.\n",
        "\n",
        "Make sure to format your answers in Markdown format, including code blocks and snippets.\n",
        "\n",
        "Politely reject questions not related to AI, while being cautious not to reject unfamiliar terms or acronyms too quickly.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "C54LMqwdV_Me"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model=\"gpt-4o\")\n",
        "\n",
        "agent = OpenAIAgent.from_tools(\n",
        "    llm=llm,\n",
        "    tools=[ai_tutor_knowledge_tool,llm_tool,],\n",
        "    system_prompt=system_message_openai_agent,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "aKqbYIlNV_Me",
        "outputId": "a760d8f2-175d-4ad5-8798-400bceceeecb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The LLaMA (Large Language Model Meta AI) series, developed by Meta, represents a collection of large language models designed to advance natural language processing capabilities. Here is an overview of the different versions and their key features:\n",
            "\n",
            "### LLaMA (Original)\n",
            "- **Release Date**: February 24, 2023\n",
            "- **Key Features**: \n",
            "  - Despite having fewer parameters, it demonstrated strong performance across various benchmarks, outperforming larger models like GPT-3.\n",
            "  - The model's weights were accessible through an application process, allowing academic researchers, government organizations, and industry labs to use it.\n",
            "\n",
            "### Llama 2\n",
            "- **Release Date**: July 18, 2023\n",
            "- **Key Features**:\n",
            "  - **Sizes**: Available in 7B, 13B, and 70B parameter versions.\n",
            "  - **Training Data**: Trained with 40% more data than the original LLaMA.\n",
            "  - **Fine-Tuning**: Includes models fine-tuned for chat-specific tasks.\n",
            "  - **Commercial Use**: Available for certain commercial use cases, although its open-source status has been debated.\n",
            "  - **Collaboration**: Developed in collaboration with Microsoft.\n",
            "\n",
            "### Code Llama\n",
            "- **Release Date**: Later in 2023\n",
            "- **Key Features**:\n",
            "  - **Sizes**: Available in 7B, 13B, 34B, and 70B parameter versions.\n",
            "  - **Specialization**: Tailored for code generation tasks.\n",
            "  - **Training**: Utilized extensive datasets designed for programming tasks, with a particular emphasis on Python.\n",
            "  - **Instruction-Following**: Enhanced capabilities for following instructions related to coding.\n",
            "\n",
            "### Llama 3\n",
            "- **Release Date**: April 18, 2024\n",
            "- **Key Features**:\n",
            "  - **Sizes**: Available in 8B and 70B parameter versions.\n",
            "  - **Advancements**: Reflects ongoing improvements in training processes, model accessibility, and the ability to run on less powerful hardware.\n",
            "  - **Applications**: Continued focus on applications across various domains, including chat assistance and medical benchmarks.\n",
            "\n",
            "The LLaMA series emphasizes improving training processes, model accessibility, and performance across various benchmarks, making it a significant player in the field of large language models.\n",
            "\n",
            "Feel free to ask more specific questions about any of these models or their applications!\n"
          ]
        }
      ],
      "source": [
        "response = agent.chat(\"What is the LLama model?\")\n",
        "print(response.response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "WqGWXuPDV_Me",
        "outputId": "87861ff4-68c0-4e1e-8b5b-8cd107886b23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter Efficient Fine-Tuning (PEFT) methods aim to optimize the fine-tuning process of large language models (LLMs) by reducing the number of parameters that need to be updated, thereby making the process more efficient in terms of computational resources and time. Here are some key techniques and strategies used in efficient fine-tuning:\n",
            "\n",
            "### Supervised Learning with Autoregressive Loss Functions\n",
            "- **Approach**: This method involves training the model using a supervised learning approach where the model learns to predict the next token in a sequence based on the previous tokens.\n",
            "- **Batch Size**: For example, during the fine-tuning of Llama 2 models, a batch size of 64 was utilized.\n",
            "- **Loss Functions**: Autoregressive loss functions are used to optimize specific components of the training process.\n",
            "\n",
            "### Reinforcement Learning with Human Feedback (RLHF)\n",
            "- **AI Alignment**: RLHF is used to align the AI's behavior with human expectations and improve dialog consistency.\n",
            "- **Human Feedback**: Human annotators compare model outputs and provide feedback, which is used to train reward models that guide the fine-tuning process.\n",
            "- **Training Examples**: A large number of training examples are used to enhance the model's safety and helpfulness.\n",
            "\n",
            "### Ghost Attention\n",
            "- **Multi-Turn Dialog Consistency**: This technique is used to improve the consistency of multi-turn dialogs by managing system message compliance.\n",
            "- **Instruction Concatenation**: Relevant instructions are concatenated to each new user message to ensure compliance.\n",
            "- **Loss Function Control**: The loss function is effectively controlled for specific tokens to enhance performance.\n",
            "\n",
            "### Parameter Efficiency\n",
            "- **Selective Updating**: Only a subset of the model's parameters is updated during fine-tuning, reducing the computational load.\n",
            "- **Layer-Wise Adaptation**: Techniques like adapter layers or low-rank adaptation (LoRA) can be used to introduce additional parameters that are fine-tuned while keeping the original model parameters mostly frozen.\n",
            "\n",
            "These strategies collectively contribute to more effective and efficient fine-tuning of LLMs, allowing for improved performance without the need for extensive computational resources.\n",
            "\n",
            "Feel free to ask more specific questions about any of these techniques or their applications!\n"
          ]
        }
      ],
      "source": [
        "response = agent.chat(\"Explain parameter Efficient Fine tuning method\")\n",
        "print(response.response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "TSPTAruzV_Me",
        "outputId": "352dbedf-9a3e-41d4-988b-b21060ab993e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm here to help with questions related to AI, Large Language Models, and related topics. If you have any questions about these subjects, feel free to ask!\n"
          ]
        }
      ],
      "source": [
        "response = agent.chat(\"Write the recipe for a chocolate cake.\")\n",
        "print(response.response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWb1I1WvV_Mf"
      },
      "source": [
        "# Code related questions to GPT-4o, the remaining questions to Gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "tY82urjtV_Mf"
      },
      "outputs": [],
      "source": [
        "from llama_index.agent.openai import OpenAIAgent\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from llama_index.core.query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import PydanticSingleSelector\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "# initialize LLMs\n",
        "gpt_4o_llm = OpenAI(model=\"gpt-4o\")\n",
        "gemini_llm = Gemini(model=\"models/gemini-1.5-flash\", temperature=1, max_tokens=512)\n",
        "\n",
        "# define query engines\n",
        "llama_query_engine_code = vector_index.as_query_engine(\n",
        "    llm=gpt_4o_llm,\n",
        "    similarity_top_k=3,\n",
        "    embed_model=OpenAIEmbedding(model=\"text-embedding-3-small\", mode=\"text_search\"),\n",
        ")\n",
        "llama_query_engine_rest = vector_index.as_query_engine(\n",
        "    llm=gemini_llm,\n",
        "    similarity_top_k=3,\n",
        "    embed_model=OpenAIEmbedding(model=\"text-embedding-3-small\", mode=\"text_search\"),\n",
        ")\n",
        "\n",
        "# define tools for LLM\n",
        "llm_tool_code = QueryEngineTool.from_defaults(\n",
        "    query_engine=llama_query_engine_code,\n",
        "    description=\"Useful for code-related questions about the Large Language Models\",\n",
        "    name=\"LLMCodeTool\",\n",
        ")\n",
        "llm_tool_rest = QueryEngineTool.from_defaults(\n",
        "    query_engine=llama_query_engine_rest,\n",
        "    description=\"Useful for non-code-related questions about Large Language Models\",\n",
        "    name=\"LLMGeneralTool\",\n",
        ")\n",
        "\n",
        "# Initialize OpenAIAgent with the system message and the router query engine\n",
        "agent = OpenAIAgent.from_tools(\n",
        "    llm=gpt_4o_llm,  # The base LLM, used only if no other tools apply\n",
        "    tools=[llm_tool_code, llm_tool_rest],\n",
        "    system_prompt=system_message_openai_agent,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "h3NMpGxYV_Mf",
        "outputId": "42f8d9f9-df46-4f3b-a350-f0f3d2e84397",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLMGeneralTool\n",
            "LLMCodeTool\n"
          ]
        }
      ],
      "source": [
        "# Test the agent with a code-related question\n",
        "response = agent.chat(\"How do I fine-tune the LLama model? Write the code for it.\")\n",
        "for source in response.sources:\n",
        "    print(source.tool_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ysB0X7D5V_Mf",
        "outputId": "c7438148-cb0c-4b9b-d1c4-126b27b93d55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLMGeneralTool\n"
          ]
        }
      ],
      "source": [
        "# Test the agent with a code-related question\n",
        "response = agent.chat(\"What is the relationship between Llama and Meta?\")\n",
        "for source in response.sources:\n",
        "    print(source.tool_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e5PeNM0sG3Qd"
      },
      "execution_count": 35,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "535244513b504cf0ae900cbb9873ca0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7a0fb7e522e040e7b3d218006c090201",
              "IPY_MODEL_4f2d401e824845d9b23b848421fa95e0",
              "IPY_MODEL_02e0480193574ebc97d082d429615fc6"
            ],
            "layout": "IPY_MODEL_cc6df9ddbe004f4d901ac4a35dd49c91"
          }
        },
        "7a0fb7e522e040e7b3d218006c090201": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24cbfe06ba4c4c9fb010ab74485c92fa",
            "placeholder": "​",
            "style": "IPY_MODEL_1daa2d514e4848a7ac602be83fdd6d0e",
            "value": "ai_tutor_knowledge.jsonl: 100%"
          }
        },
        "4f2d401e824845d9b23b848421fa95e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad02a9bfcd3642fb834f3fb452efaf10",
            "max": 6960611,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d24b90385f884b8b921685f4fe1f6742",
            "value": 6960611
          }
        },
        "02e0480193574ebc97d082d429615fc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd77c85e0680449caeabbf4c5febaf24",
            "placeholder": "​",
            "style": "IPY_MODEL_b2edcdbabf3c424788745bdc72504d59",
            "value": " 6.96M/6.96M [00:00&lt;00:00, 36.9MB/s]"
          }
        },
        "cc6df9ddbe004f4d901ac4a35dd49c91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24cbfe06ba4c4c9fb010ab74485c92fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1daa2d514e4848a7ac602be83fdd6d0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad02a9bfcd3642fb834f3fb452efaf10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d24b90385f884b8b921685f4fe1f6742": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd77c85e0680449caeabbf4c5febaf24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2edcdbabf3c424788745bdc72504d59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50945377703b431484721a78f514068a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_653db9bdad1345ab8fee6ef552b12743",
              "IPY_MODEL_7cb14142a86f4243b95be4a9bb6e850d",
              "IPY_MODEL_9b689571805240aeac1cd279edf8cd85"
            ],
            "layout": "IPY_MODEL_3fe1860885934c779056586b05cd17b5"
          }
        },
        "653db9bdad1345ab8fee6ef552b12743": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c08f60329ec41d2a065beea014b7c75",
            "placeholder": "​",
            "style": "IPY_MODEL_6e0e7f99b04f4ebdbaf43c03ad17be69",
            "value": "vectorstore.zip: 100%"
          }
        },
        "7cb14142a86f4243b95be4a9bb6e850d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_768f382510194b339510d3c84963f198",
            "max": 97198458,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ac28593124da4fbba2f920b6791e426a",
            "value": 97198458
          }
        },
        "9b689571805240aeac1cd279edf8cd85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd46095fa65a414cbb847889566311a7",
            "placeholder": "​",
            "style": "IPY_MODEL_a07a41f9d13949aa9e2adb64715b84f1",
            "value": " 97.2M/97.2M [00:05&lt;00:00, 40.3MB/s]"
          }
        },
        "3fe1860885934c779056586b05cd17b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c08f60329ec41d2a065beea014b7c75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e0e7f99b04f4ebdbaf43c03ad17be69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "768f382510194b339510d3c84963f198": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac28593124da4fbba2f920b6791e426a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bd46095fa65a414cbb847889566311a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a07a41f9d13949aa9e2adb64715b84f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}