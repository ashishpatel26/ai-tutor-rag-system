{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/towardsai/ai-tutor-rag-system/blob/main/notebooks/13-Adding_Router.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zE1h0uQV7uT"
      },
      "source": [
        "# Install Packages and Setup Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPJzr-I9XQ7l",
        "outputId": "7e0a939a-0a66-40fa-8ac6-2b389b06b9dd",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.0/337.0 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.3/584.3 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.2/255.2 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m679.1/679.1 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.8/173.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.6/82.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for html2text (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-index==0.10.57 openai==1.37.0 cohere==5.6.2 tiktoken==0.7.0 chromadb==0.5.5 html2text sentence_transformers pydantic llama-index-vector-stores-chroma==0.1.10 kaleido==0.2.1 llama-index-llms-gemini==0.1.11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "riuXwpSPcvWC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set the following API Keys in the Python environment. Will be used later.\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_OPENAI_API_KEY>\"\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = \"<YOUR_GOOGLE_API_KEY>\"\n",
        "\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api_key')\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('Google_api_key')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jIEeZzqLbz0J"
      },
      "outputs": [],
      "source": [
        "# Allows running asyncio in environments with an existing event loop, like Jupyter notebooks.\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bkgi2OrYzF7q"
      },
      "source": [
        "# Load a Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9oGT6crooSSj"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core import Settings\n",
        "\n",
        "Settings.llm = OpenAI(temperature=1, model=\"gpt-4o-mini\")\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: You can create a vector store from scratch using the code below, or you can load it from Hugging Face using the code provided in this notebook.**"
      ],
      "metadata": {
        "id": "HKWdt7Ig3ZKh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BwVuJXlzHVL"
      },
      "source": [
        "# Create a VectoreStore\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQP87lHczHKc",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "\n",
        "# create client and a new collection\n",
        "# chromadb.EphemeralClient saves data in-memory.\n",
        "\n",
        "chroma_client = chromadb.PersistentClient(path=\"./ai_tutor_knowledge\")\n",
        "chroma_collection = chroma_client.create_collection(\"ai_tutor_knowledge\")\n",
        "\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9JbAzFcjkpn"
      },
      "source": [
        "# Load the Dataset (JSON)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceveDuYdWCYk"
      },
      "source": [
        "### Download\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZwf6pv7WFmD"
      },
      "source": [
        "The dataset includes several articles from the TowardsAI blog, Research paper contents and documentation from various resources. Which includes the various GenAI Concepts like RAG, PEFT, BERT Model, and Other concepts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "535244513b504cf0ae900cbb9873ca0a",
            "7a0fb7e522e040e7b3d218006c090201",
            "4f2d401e824845d9b23b848421fa95e0",
            "02e0480193574ebc97d082d429615fc6",
            "cc6df9ddbe004f4d901ac4a35dd49c91",
            "24cbfe06ba4c4c9fb010ab74485c92fa",
            "1daa2d514e4848a7ac602be83fdd6d0e",
            "ad02a9bfcd3642fb834f3fb452efaf10",
            "d24b90385f884b8b921685f4fe1f6742",
            "cd77c85e0680449caeabbf4c5febaf24",
            "b2edcdbabf3c424788745bdc72504d59"
          ]
        },
        "id": "wl_pbPvMlv1h",
        "outputId": "d61e9faf-1e9f-4d3d-df77-4d9e529b425c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ai_tutor_knowledge.jsonl:   0%|          | 0.00/6.96M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "535244513b504cf0ae900cbb9873ca0a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "file_path = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"ai_tutor_knowledge.jsonl\",repo_type=\"dataset\",local_dir=\"/content\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWBLtDbUWJfA"
      },
      "source": [
        "## Read File\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Q9sxuW0g3Gd",
        "outputId": "219dcedf-8698-4937-bf69-8cea8836a225"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "762"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import json\n",
        "with open(file_path, \"r\") as file:\n",
        "    ai_tutor_knowledge = [json.loads(line) for line in file]\n",
        "\n",
        "len(ai_tutor_knowledge)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S17g2RYOjmf2"
      },
      "source": [
        "# Convert to Document obj\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YizvmXPejkJE"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from llama_index.core import Document\n",
        "\n",
        "def create_docs_from_list(data_list: List[dict]) -> List[Document]:\n",
        "    documents = []\n",
        "    for data in data_list:\n",
        "        documents.append(\n",
        "            Document(\n",
        "                doc_id=data[\"doc_id\"],\n",
        "                text=data[\"content\"],\n",
        "                metadata={  # type: ignore\n",
        "                    \"url\": data[\"url\"],\n",
        "                    \"title\": data[\"name\"],\n",
        "                    \"tokens\": data[\"tokens\"],\n",
        "                    \"source\": data[\"source\"],\n",
        "                },\n",
        "                excluded_llm_metadata_keys=[\n",
        "                    \"title\",\n",
        "                    \"tokens\",\n",
        "                    \"source\",\n",
        "                ],\n",
        "                excluded_embed_metadata_keys=[\n",
        "                    \"url\",\n",
        "                    \"tokens\",\n",
        "                    \"source\",\n",
        "                ],\n",
        "            )\n",
        "        )\n",
        "    return documents\n",
        "\n",
        "doc = create_docs_from_list(ai_tutor_knowledge)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjuLbmFuWsyl"
      },
      "source": [
        "# Transforming\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9z3t70DGWsjO"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.text_splitter import TokenTextSplitter\n",
        "\n",
        "# Define the splitter object that split the text into segments with 512 tokens,\n",
        "# with a 128 overlap between the segments.\n",
        "text_splitter = TokenTextSplitter(separator=\" \", chunk_size=512, chunk_overlap=128)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.extractors import (\n",
        "    SummaryExtractor,\n",
        "    QuestionsAnsweredExtractor,\n",
        "    KeywordExtractor,\n",
        ")\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "\n",
        "pipeline = IngestionPipeline(\n",
        "    transformations=[\n",
        "        text_splitter,\n",
        "        QuestionsAnsweredExtractor(questions=3, llm = Settings.llm),\n",
        "        SummaryExtractor(summaries=[\"prev\", \"self\"], llm = Settings.llm),\n",
        "        KeywordExtractor(keywords=10, llm = Settings.llm),\n",
        "        OpenAIEmbedding(model=\"text-embedding-3-small\"),\n",
        "    ],\n",
        "    vector_store=vector_store,\n",
        ")\n",
        "\n",
        "nodes = pipeline.run(documents=doc, show_progress=True)"
      ],
      "metadata": {
        "id": "KO-Piomu4DH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compress the vector store directory to a zip file to be able to download and use later.\n",
        "!zip -r vectorstore.zip ai_tutor_knowledge"
      ],
      "metadata": {
        "id": "lhioZf2m4GwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWaT6rL7ksp8"
      },
      "source": [
        "# Load Indexes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: If you created the vector store from scratch, please comment out the three code blocks/cells below.**"
      ],
      "metadata": {
        "id": "Fr-ytVUA4OAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading Vector store from Hugging face hub\n",
        "from huggingface_hub import hf_hub_download\n",
        "vectorstore = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"vectorstore.zip\",repo_type=\"dataset\",local_dir=\"/content\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "925ad382a16d4761bde196bdd5134366",
            "574b186993104a4a95f64a93c4c48cd3",
            "d6bb6f844e034f99a0bda181fccd2a8e",
            "3db013c960004990bf3011f911a683fc",
            "d719a2c329804d908bb1aa831cd3dfbb",
            "5f48aba326984fa09159bd1b16a97bae",
            "59a47e7061af49dba9a5f54a72ef2d25",
            "7ad3b4cf967e4168bd140b42e673e4b8",
            "ec9f64527bcd4917aebb453ed6ab4d30",
            "8398f3bafb244529a6f6c4edc8354dbc",
            "0635c26c2deb495a82aaac8d0aa5f073"
          ]
        },
        "id": "qyuq48p_4KWu",
        "outputId": "191d6e62-3914-4834-9417-e9fd768139e8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vectorstore.zip:   0%|          | 0.00/97.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "925ad382a16d4761bde196bdd5134366"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SodY2Xpf_kxg",
        "outputId": "555793fa-1170-49c9-ac0f-baf5471a9e8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  vectorstore.zip\n",
            "   creating: ai_tutor_knowledge/\n",
            "   creating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/\n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/length.bin  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/index_metadata.pickle  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/link_lists.bin  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/header.bin  \n",
            "  inflating: ai_tutor_knowledge/684af133-f877-4230-bde4-575cf53b6688/data_level0.bin  \n",
            "  inflating: ai_tutor_knowledge/chroma.sqlite3  \n"
          ]
        }
      ],
      "source": [
        "!unzip vectorstore.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mXi56KTXk2sp"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "\n",
        "# Create your index\n",
        "db = chromadb.PersistentClient(path=\"./ai_tutor_knowledge\")\n",
        "chroma_collection = db.get_or_create_collection(\"ai_tutor_knowledge\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jKXURvLtkuTS"
      },
      "outputs": [],
      "source": [
        "# Create your index\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "vector_index = VectorStoreIndex.from_vector_store(vector_store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lgpzgnHFV_MH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c83e6bc-f1ab-4ebb-e3e7-e53153a736fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieval Augmented Generation (RAG) combines the strengths of retrieval-based and generative models to enhance response quality in natural language processing tasks. The process typically consists of several key steps:\n",
            "\n",
            "1. **Query Classification**: The system determines if the input query necessitates retrieval of additional information.\n",
            "\n",
            "2. **Retrieval**: Relevant documents are fetched from external knowledge sources, which can be organized using indexing methods for efficient access.\n",
            "\n",
            "3. **Reranking**: The initially retrieved documents are refined in order based on their relevance to the query.\n",
            "\n",
            "4. **Repacking**: The selected documents are structured in a way that facilitates better generation of responses.\n",
            "\n",
            "5. **Summarization**: Key information is extracted from the organized documents, eliminating redundancies to provide concise content for the final response.\n",
            "\n",
            "In the generation stage, retrieved data is utilized by large language models to produce coherent responses. Techniques such as prompting can further enhance the output, enabling the models to generate contextually relevant and accurate answers based on the augmented information. This synergy between retrieval and generation techniques helps mitigate issues like outdated information and fact fabrication, ultimately improving the reliability and accuracy of generated content.\n"
          ]
        }
      ],
      "source": [
        "# Query Engine\n",
        "ai_tutor_knowledge_query_engine = vector_index.as_query_engine(similarity_top_k=3)\n",
        "\n",
        "res = ai_tutor_knowledge_query_engine.query(\"How Retrieval Augmented Generation (RAG) Works?\")\n",
        "print(res.response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cKN0brIPV_MJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ca24a96-392a-45a8-fbd8-cb72c5a820d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node ID\t 2aa05360-f43a-4819-bce7-0acf7b897eab\n",
            "Title\t Searching for Best Practices in Retrieval-Augmented Generation:1 Introduction\n",
            "Text\t Generative large language models are prone to producing outdated information or fabricating facts, although they were aligned with human preferences by reinforcement learning [1] or lightweight alternatives [2–5]. Retrieval-augmented generation (RAG) techniques address these issues by combining the strengths of pretraining and retrieval-based models, thereby providing a robust framework for enhancing model performance [6]. Furthermore, RAG enables rapid deployment of applications for specific organizations and domains without necessitating updates to the model parameters, as long as query-related documents are provided. Many RAG approaches have been proposed to enhance large language models (LLMs) through query-dependent retrievals [6–8]. A typical RAG workflow usually contains multiple intervening processing steps: query classification (determining whether retrieval is necessary for a given input query), retrieval (efficiently obtaining relevant documents for the query), reranking (refining the order of retrieved documents based on their relevance to the query), repacking (organizing the retrieved documents into a structured one for better generation), summarization (extracting key information for response generation from the repacked document and eliminating redundancies) and models. Implementing RAG also requires decisions on the ways to properly split documents into chunks, the types of embeddings to use for semantically representing these chunks, the choice of\n",
            "Score\t 0.6077727402448877\n",
            "Metadata\t {'url': 'http://arxiv.org/pdf/2407.01219v1', 'title': 'Searching for Best Practices in Retrieval-Augmented Generation:1 Introduction', 'tokens': 263, 'source': 'Arxiv: 2407.01219', 'questions_this_excerpt_can_answer': 'Based on the provided context, here are two specific questions that can be answered:\\n\\n1. **What are the key processing steps involved in a typical retrieval-augmented generation (RAG) workflow?**\\n   - This question focuses on the specific steps mentioned in the context, such as query classification, retrieval, reranking, repacking, and summarization, which are integral to the RAG process.\\n\\n2. **How does retrieval-augmented generation (RAG) enhance the performance of large language models (LLMs)?**\\n   - This question seeks to understand the advantages of RAG techniques in improving the reliability and accuracy of information generated by LLMs, particularly in addressing issues of outdated information and fact fabrication.\\n\\nThese questions are tailored to extract detailed insights from the context that may not be readily available in other sources.', 'prev_section_summary': 'The section discusses retrieval-augmented generation (RAG) techniques, highlighting their effectiveness in integrating current information, reducing hallucinations, and improving response quality, especially in specialized fields. It notes the challenges of complex implementation and slow response times associated with many RAG approaches. The excerpt emphasizes the investigation of existing RAG methods and their combinations to identify optimal practices for balancing performance and efficiency. Additionally, it mentions the role of multimodal retrieval techniques in enhancing question-answering capabilities related to visual inputs and expediting the generation of multimodal content through a \"retrieval as generation\" strategy. Resources for further exploration are provided via a GitHub link.', 'section_summary': 'The section discusses the concept of retrieval-augmented generation (RAG) as a method to enhance the performance of generative large language models (LLMs). It highlights the challenges faced by LLMs, such as producing outdated information and fabricating facts, and explains how RAG techniques address these issues by integrating pretraining and retrieval-based models. The text outlines a typical RAG workflow, which includes several key processing steps: query classification, retrieval, reranking, repacking, and summarization. Additionally, it notes that RAG allows for the rapid deployment of applications tailored to specific organizations and domains without needing to update model parameters, provided that relevant documents are available. The section also mentions considerations for document chunking and embedding types for effective semantic representation. \\n\\nKey topics include:\\n- Retrieval-augmented generation (RAG)\\n- Large language models (LLMs)\\n- Challenges of LLMs (outdated information, fact fabrication)\\n- RAG workflow steps (query classification, retrieval, reranking, repacking, summarization)\\n- Deployment of RAG applications\\n- Document chunking and embedding types\\n\\nEntities mentioned include:\\n- Generative large language models\\n- RAG techniques\\n- Pretraining and retrieval-based models', 'excerpt_keywords': 'Keywords: retrieval-augmented generation, large language models, outdated information, fact fabrication, query classification, document retrieval, reranking, repacking, summarization, semantic representation'}\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t 1c324686-fad7-4a41-bfd3-d44f9612ca91\n",
            "Title\t Evaluation of Retrieval-Augmented Generation: A Survey:Research Paper Information\n",
            "Text\t Authors: Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu.Numerous studies of Retrieval-Augmented Generation (RAG) systems have emerged from various perspectives since the advent of Large Language Models (LLMs). The RAG system comprises two primary components: Retrieval and Generation. The retrieval component aims to extract relevant information from various external knowledge sources. It involves two main phases, indexing and searching. Indexing organizes documents to facilitate efficient retrieval, using either inverted indexes for sparse retrieval or dense vector encoding for dense retrieval. The searching component utilizes these indexes to fetch relevant documents based on the user's query, often incorporating the optional rerankers to refine the ranking of the retrieved documents. The generation component utilizes the retrieved content and question query to formulate coherent and contextually relevant responses with the prompting and inferencing phases. As the \"Emerging\" ability of LLMs and the breakthrough in aligning human commands, LLMs are the best performance choices model for the generation stage. Prompting methods like Chain of Thought, Tree of Thought, Rephrase and Respond guide better generation results. In the inferencing step, LLMs interpret the prompted input to generate accurate and in-depth responses that align with the query's intent and integrate the extracted information without further finetuning, such as fully finetuning or LoRA. Appendix A details the complete RAG structure. Figure 1 illustrates the structure of the RAG systems as mentioned.\n",
            "Score\t 0.5936512371171832\n",
            "Metadata\t {'url': 'http://arxiv.org/pdf/2405.07437v2', 'title': 'Evaluation of Retrieval-Augmented Generation: A Survey:Research Paper Information', 'tokens': 305, 'source': 'Arxiv: 2405.07437', 'questions_this_excerpt_can_answer': '1. What are the two primary components of Retrieval-Augmented Generation (RAG) systems, and what roles do they play in the overall process?\\n\\n2. How do prompting methods like Chain of Thought and Tree of Thought contribute to the generation stage in Retrieval-Augmented Generation systems?', 'prev_section_summary': 'The excerpt discusses a survey conducted by Hao Yu and colleagues on Retrieval-Augmented Generation (RAG) systems, which have become increasingly important in natural language processing. The authors highlight the unique challenges associated with evaluating RAG systems due to their hybrid nature and dependence on dynamic knowledge sources. They aim to provide a comprehensive overview of the evaluation processes and benchmarks for RAG systems, focusing on quantifiable metrics such as relevance, accuracy, and faithfulness. The survey also critiques current benchmarks, identifying their limitations, and proposes potential directions for advancing RAG evaluation methodologies. The authors are affiliated with Tencent Company, McGill University, and the University of Science and Technology of China. The paper emphasizes the importance of RAG in enhancing generative models by reducing factual inaccuracies and improving the reliability of generated content.', 'section_summary': \"The section discusses Retrieval-Augmented Generation (RAG) systems, highlighting their two primary components: Retrieval and Generation. \\n\\n1. **Retrieval Component**: \\n   - **Purpose**: Extracts relevant information from external knowledge sources.\\n   - **Phases**: \\n     - **Indexing**: Organizes documents for efficient retrieval using inverted indexes (sparse retrieval) or dense vector encoding (dense retrieval).\\n     - **Searching**: Utilizes the indexes to fetch relevant documents based on user queries, often enhanced by rerankers to improve document ranking.\\n\\n2. **Generation Component**: \\n   - **Function**: Uses the retrieved content and user queries to generate coherent and contextually relevant responses.\\n   - **Methods**: Employs prompting techniques such as Chain of Thought and Tree of Thought to enhance generation quality. \\n   - **Inferencing**: Involves interpreting prompted input to produce accurate responses that align with the query's intent, integrating extracted information without additional fine-tuning.\\n\\nThe authors of the paper are Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu, and the document is sourced from Arxiv with the identifier 2405.07437. The section emphasizes the significance of Large Language Models (LLMs) in the generation stage of RAG systems.\", 'excerpt_keywords': 'Keywords: Retrieval-Augmented Generation, RAG systems, Large Language Models, information retrieval, document indexing, prompting methods, Chain of Thought, Tree of Thought, generation component, evaluation methodologies'}\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t e67de210-5d8c-4bbc-87e9-b72c66cac0dc\n",
            "Title\t Adaptive Retrieval-Augmented Generation for Conversational Systems:Abstract\n",
            "Text\t Despite the success of integrating large language models into the development of conversational systems, many studies have shown the effectiveness of retrieving and augmenting external knowledge for informative responses. Hence, many existing studies commonly assume the always need for Retrieval Augmented Generation (RAG) in a conversational system without explicit control. This raises a research question about such a necessity. In this study, we propose to investigate the need for each turn of system response to be augmented with external knowledge. In particular, by leveraging human judgements on the binary choice of adaptive augmentation, we develop RAGate, a gating model, which models conversation context and relevant inputs to predict if a conversational system requires RAG for improved responses. We conduct extensive experiments on devising and applying RAGate to conversational models and well-rounded analyses of different conversational scenarios. Our experimental results and analysis indicate the effective application of RAGate in RAG-based conversational systems in identifying system responses for appropriate RAG with high-quality responses and a high generation confidence. This study also identifies the correlation between the generation’s confidence level and the relevance of the augmented knowledge.\n",
            "Score\t 0.5819083749234663\n",
            "Metadata\t {'url': 'http://arxiv.org/pdf/2407.21712v1', 'title': 'Adaptive Retrieval-Augmented Generation for Conversational Systems:Abstract', 'tokens': 226, 'source': 'Arxiv: 2407.21712', 'questions_this_excerpt_can_answer': '1. What is the primary research question addressed by the study regarding the necessity of Retrieval Augmented Generation (RAG) in conversational systems?\\n\\n2. How does the RAGate model determine whether a conversational system requires external knowledge augmentation for improved responses?', 'prev_section_summary': 'The section discusses the inference stage in the Retrieval-Augmented Generation (RaR) pipeline, highlighting its role in processing the final input string prepared during the prompting step. The inference stage involves the Large Language Model (LLM) generating output based on the input derived from previous stages, ultimately providing an answer to the initial query for downstream tasks. Additionally, it mentions the possibility of implementing post-processing steps after inference to format the output or extract specific information, such as through named entity recognition or parsing, particularly in tasks like classification or information extraction. Key topics include the inference stage, LLM output generation, and post-processing techniques.', 'section_summary': \"The section discusses the integration of Retrieval Augmented Generation (RAG) in conversational systems, highlighting the research question regarding the necessity of RAG for each response. It introduces the RAGate model, which determines whether external knowledge augmentation is needed based on conversation context and relevant inputs. The study emphasizes the importance of human judgments in this adaptive augmentation process and presents experimental results demonstrating RAGate's effectiveness in improving response quality and generation confidence in RAG-based conversational systems. Additionally, it notes a correlation between the confidence level of generated responses and the relevance of the augmented knowledge. Key topics include RAG, conversational systems, adaptive augmentation, and the RAGate model.\", 'excerpt_keywords': 'Keywords: Retrieval Augmented Generation, conversational systems, RAGate model, adaptive augmentation, external knowledge, human judgments, response quality, generation confidence, large language models, experimental analysis'}\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "for src in res.source_nodes:\n",
        "    print(\"Node ID\\t\", src.node_id)\n",
        "    print(\"Title\\t\", src.metadata[\"title\"])\n",
        "    print(\"Text\\t\", src.text)\n",
        "    print(\"Score\\t\", src.score)\n",
        "    print(\"Metadata\\t\", src.metadata)\n",
        "    print(\"-_\" * 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXqjul4MV_MJ"
      },
      "source": [
        "# Router\n",
        "\n",
        "Routers are modules that take in a user query and a set of “choices” (defined by metadata), and returns one or more selected choices.\n",
        "\n",
        "They can be used for the following use cases and more:\n",
        "\n",
        "- Selecting the right data source among a diverse range of data sources\n",
        "\n",
        "- Deciding whether to do summarization (e.g. using summary index query engine) or semantic search (e.g. using vector index query engine)\n",
        "\n",
        "- Deciding whether to “try” out a bunch of choices at once and combine the results (using multi-routing capabilities).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uunnC6XjV_Mb"
      },
      "source": [
        "## Lets create a different query engine with Mistral AI information\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import requests\n",
        "\n",
        "wiki_titles = [\n",
        "    \"Mistral AI\",\n",
        "    \"Llama (language model)\",\n",
        "    \"Claude AI\",\n",
        "    \"OpenAI\",\n",
        "    \"Gemini AI\",\n",
        "]\n",
        "\n",
        "data_path = Path(\"llm_data_wiki\")\n",
        "\n",
        "if not data_path.exists():\n",
        "    data_path.mkdir()\n",
        "\n",
        "for title in wiki_titles:\n",
        "    response = requests.get(\n",
        "        \"https://en.wikipedia.org/w/api.php\",\n",
        "        params={\n",
        "            \"action\": \"query\",\n",
        "            \"format\": \"json\",\n",
        "            \"titles\": title,\n",
        "            \"prop\": \"extracts\",\n",
        "            \"explaintext\": True,\n",
        "        },\n",
        "    ).json()\n",
        "\n",
        "    page = next(iter(response[\"query\"][\"pages\"].values()))\n",
        "\n",
        "    if \"extract\" in page:\n",
        "        wiki_text = page[\"extract\"]\n",
        "\n",
        "        with open(data_path / \"llm_data_wiki.txt\", \"a\") as fp:\n",
        "            fp.write(f\"Title: {title}\\n{wiki_text}\\n\\n\")\n",
        "    else:\n",
        "        print(f\"No extract found for '{title}'\")\n"
      ],
      "metadata": {
        "id": "AXnP6NfcBCjA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OsSsh4_bV_Mb",
        "outputId": "c9a9abc4-dc1d-4bd9-afbe-2b78f9d92a26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 46/46 [00:24<00:00,  1.86it/s]\n",
            "100%|██████████| 46/46 [00:46<00:00,  1.01s/it]\n",
            "100%|██████████| 46/46 [00:10<00:00,  4.29it/s]\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.core.text_splitter import TokenTextSplitter\n",
        "from llama_index.core.extractors import (\n",
        "    SummaryExtractor,\n",
        "    QuestionsAnsweredExtractor,\n",
        "    KeywordExtractor,\n",
        ")\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "\n",
        "# Assuming you have prepared a directory for llm data\n",
        "documents = SimpleDirectoryReader(\"llm_data_wiki\").load_data()\n",
        "\n",
        "text_splitter = TokenTextSplitter(separator=\" \", chunk_size=512, chunk_overlap=128)\n",
        "\n",
        "transformations = [\n",
        "    text_splitter,\n",
        "    QuestionsAnsweredExtractor(questions=2),\n",
        "    SummaryExtractor(summaries=[\"prev\", \"self\"]),\n",
        "    KeywordExtractor(keywords=10),\n",
        "    OpenAIEmbedding(model=\"text-embedding-3-small\"),\n",
        "]\n",
        "\n",
        "llm_index = VectorStoreIndex.from_documents( documents= documents, transformations = transformations )\n",
        "\n",
        "llm_query_engine = llm_index.as_query_engine( similarity_top_k=2,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "p3hDxvMuV_Mc"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import PydanticSingleSelector, LLMSingleSelector\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "from llama_index.core import VectorStoreIndex, SummaryIndex\n",
        "\n",
        "# initialize tools\n",
        "ai_tutor_knowledge_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=ai_tutor_knowledge_query_engine,\n",
        "    description=\"Useful for questions about the General Generative AI Concepts\",\n",
        ")\n",
        "llm_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=llm_query_engine,\n",
        "    description=\"Useful for questions about Particular LLMs like Mistral, Claude, OpenAI, Gemini\",\n",
        ")\n",
        "\n",
        "# initialize router query engine (single selection, pydantic)\n",
        "query_engine = RouterQueryEngine(\n",
        "    selector=PydanticSingleSelector.from_defaults(),\n",
        "    query_engine_tools=[\n",
        "        ai_tutor_knowledge_tool,\n",
        "        llm_tool,\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9nnZaOPuV_Mc",
        "outputId": "823cb762-94cc-4ce5-879b-de67b16425ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The LLaMA model is a family of autoregressive language models developed by Meta AI, initially released in February 2023. The models vary in size, ranging from 1 billion to 405 billion parameters. LLaMA was designed to provide researchers with accessible language modeling capabilities under a non-commercial license. The series began with the foundational model, and subsequent versions, starting with LLaMA 2, included instruction-tuned variants to enhance their usability. The models have undergone various training methodologies and have been integrated into applications, such as virtual assistant features in platforms like Facebook and WhatsApp.\n"
          ]
        }
      ],
      "source": [
        "res = query_engine.query(\n",
        "    \"What is the LLama model?\",\n",
        ")\n",
        "print(res.response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zLVDAW7hV_Md",
        "outputId": "fcfb8584-6ec6-418d-ce25-126b248ee4c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node ID\t 53556ef6-f9d5-4bf6-a9da-b86d38879e4c\n",
            "Text\t Optimization (PPO) for RLHF – a new technique based on Rejection sampling was used, followed by PPO.\n",
            "Multi-turn consistency in dialogs was targeted for improvement, to make sure that \"system messages\" (initial instructions, such as \"speak in French\" and \"act like Napoleon\") are respected during the dialog. This was accomplished using the new \"Ghost attention\" technique during training, which concatenates relevant instructions to each new user message but zeros out the loss function for tokens in the prompt (earlier parts of the dialog).\n",
            "\n",
            "\n",
            "== Applications ==\n",
            "The Stanford University Institute for Human-Centered Artificial Intelligence (HAI) Center for Research on Foundation Models (CRFM) released Alpaca, a training recipe based on the LLaMA 7B model that uses the \"Self-Instruct\" method of instruction tuning to acquire capabilities comparable to the OpenAI GPT-3 series text-davinci-003 model at a modest cost. The model files were officially removed on March 21, 2023, over hosting costs and safety concerns, though the code and paper remain online for reference.\n",
            "Meditron is a family of Llama-based finetuned on a corpus of clinical guidelines, PubMed papers, and articles. It was created by researchers at École Polytechnique Fédérale de Lausanne School of Computer and Communication Sciences, and the Yale School of Medicine. It shows increased performance on medical-related benchmarks such as MedQA and MedMCQA.\n",
            "Zoom used Meta Llama 2 to create an AI Companion that can summarize meetings, provide helpful presentation tips, and assist with message responses. This AI Companion is powered by multiple models, including Meta Llama 2.\n",
            "\n",
            "\n",
            "=== llama.cpp ===\n",
            "\n",
            "Software developer Georgi Gerganov released llama.cpp as open-source on March 10, 2023. It's a re-implementation of LLaMA in C++, allowing systems without a powerful GPU to run the model locally. The llama.cpp project introduced the GGUF file format, a binary format that stores both tensors and metadata. The format focuses on supporting different quantization types, which can reduce memory usage, and increase speed at the expense of lower model precision.\n",
            "llamafile created by Justine Tunney is an open-source tool that bundles llama.cpp with the model into a single executable file. Tunney et al. introduced new optimized matrix multiplication kernels for x86 and ARM CPUs, improving prompt\n",
            "Score\t 0.5919300232347018\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t b0adc188-ce12-488a-816f-54ac0f2fb41f\n",
            "Text\t of Llama were made available to the research community under a non-commercial license, and access was granted on a case-by-case basis. Unauthorized copies of the model were shared via BitTorrent. In response, Meta AI issued DMCA takedown requests against repositories sharing the link on GitHub. Subsequent versions of Llama were made accessible outside academia and released under licenses that permitted some commercial use. Llama models are trained at different parameter sizes, ranging between 1B and 405B. Originally, Llama was only available as a foundation model. Starting with Llama 2, Meta AI started releasing instruction fine-tuned versions alongside foundation models.\n",
            "Alongside the release of Llama 3, Meta added virtual assistant features to Facebook and WhatsApp in select regions, and a standalone website. Both services use a Llama 3 model.\n",
            "\n",
            "\n",
            "== Background ==\n",
            "After the release of large language models such as GPT-3, a focus of research was up-scaling models which in some instances showed major increases in emergent capabilities. The release of ChatGPT and its surprise success caused an increase in attention to large language models.\n",
            "Compared with other responses to ChatGPT, Meta's Chief AI scientist Yann LeCun stated that large language models are best for aiding with writing.\n",
            "An empirical investigation of the Llama series was the scaling laws. It was observed that the Llama 3 models showed that when a model is trained on data that is more than the \"Chinchilla-optimal\" amount, the performance continues to scale log-linearly. For example, the Chinchilla-optimal dataset for Llama 3 8B is 200 billion tokens, but performance continued to scale log-linearly to the 75-times larger dataset of 15 trillion tokens.\n",
            "\n",
            "\n",
            "== Initial release ==\n",
            "LLaMA was announced on February 24, 2023, via a blog post and a paper describing the model's training, architecture, and performance. The inference code used to run the model was publicly released under the open-source GPLv3 license. Access to the model's weights was managed by an application process, with access to be granted \"on a case-by-case basis to academic researchers; those affiliated with organizations in government, civil society, and academia; and industry research laboratories around the world\".\n",
            "Llama was trained on only publicly available information, and was trained at various model sizes, with the intention to make it more accessible to different\n",
            "Score\t 0.5685236842780711\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "for src in res.source_nodes:\n",
        "    print(\"Node ID\\t\", src.node_id)\n",
        "    print(\"Text\\t\", src.text)\n",
        "    print(\"Score\\t\", src.score)\n",
        "    print(\"-_\" * 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nvqkYeF9V_Md",
        "outputId": "57ffedc5-cbbe-4c54-8516-0edbda8419b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter Efficient Fine Tuning (PEFT) is a technique used to fine-tune large language models (LLMs) with reduced computational cost. Instead of adjusting every weight in the model, PEFT involves making slight adjustments to a subset of parameters. This is beneficial for maintaining the integrity of the pretrained model's knowledge while adapting it to specific tasks.\n",
            "\n",
            "Three main approaches are utilized within PEFT:\n",
            "\n",
            "1. **Selective Tuning**: This method focuses on fine-tuning only a specific subset of the model's parameters rather than all weights, which reduces the computational burden.\n",
            "\n",
            "2. **Reparameterization**: This involves using low-rank representations to adjust model weights, allowing for a significant reduction in the number of trainable parameters. An example of this is the Low Rank Adaptation (LoRA) technique, which approximates original weight matrices with smaller, rank-decomposed matrices.\n",
            "\n",
            "3. **Additive Approaches**: This includes methods like adapter modules, which are small, trainable neural network modules added to specific layers of the model, and prompt tuning, which involves crafting specific prompts to optimize the model's responses without altering its architecture.\n",
            "\n",
            "Overall, PEFT provides a means to fine-tune large models efficiently while minimizing the associated computational costs.\n"
          ]
        }
      ],
      "source": [
        "res = query_engine.query(\"Explain parameter Efficient Fine tuning method\")\n",
        "print(res.response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "yNKJdJhzV_Md",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dbe12e1-f67f-49a0-f29f-708632420ee2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node ID\t 6be88fa3-2f8b-43e7-aba0-d874b39809fc\n",
            "Text\t # FourierFT: Discrete Fourier Transformation Fine-Tuning[FourierFT](https://huggingface.co/papers/2405.03003) is a parameter-efficient fine-tuning technique that leverages Discrete Fourier Transform to compress the model's tunable weights. This method outperforms LoRA in the GLUE benchmark and common ViT classification tasks using much less parameters.FourierFT currently has the following constraints:- Only `nn.Linear` layers are supported.- Quantized layers are not supported.If these constraints don't work for your use case, consider other methods instead.The abstract from the paper is:> Low-rank adaptation (LoRA) has recently gained much interest in fine-tuning foundation models. It effectively reduces the number of trainable parameters by incorporating low-rank matrices A and B to represent the weight change, i.e., Delta W=BA. Despite LoRA's progress, it faces storage challenges when handling extensive customization adaptations or larger base models. In this work, we aim to further compress trainable parameters by enjoying the powerful expressiveness of the Fourier transform. Specifically, we introduce FourierFT, which treats Delta W as a matrix in the spatial domain and learns only a small fraction of its spectral coefficients. With the trained spectral coefficients, we implement the inverse discrete Fourier transform to recover Delta W. Empirically, our FourierFT method shows comparable or better performance with fewer parameters than LoRA on various tasks, including natural language understanding, natural language generation, instruction tuning, and image classification. For example, when performing instruction tuning on the LLaMA2-7B model, FourierFT surpasses LoRA with only 0.064M trainable parameters, compared to LoRA's 33.5M.## FourierFTConfig[[autodoc]] tuners.fourierft.config.FourierFTConfig## FourierFTModel[[autodoc]] tuners.fourierft.model.FourierFTModel\n",
            "Score\t 0.4105498114037279\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t 2b4590b8-4c9d-4d51-a81f-9a13979593a8\n",
            "Text\t SAMSUM is one of the datasets that FLAN T5 uses. There are several pre-trained FLAN T5 models that have been fine-tuned on SAMSUM  including Phil Schmid/flan-t5-base-samsum and jasonmcaffee/flan-t5-large-samsum on Hugging Face. If we want to fine-tune the FLAN T5 model specifically for formal dialogue conversations  we can do so using the DIALOGUESUM dataset.   Models fine-tuned on DialogSum can be applied to areas like customer support  meeting minutes generation  chatbot summarization  and more.   2. PEFT (Parameter efficient fine tuning)Training LLMs is computationally intensive. Full finetuning is computationally expensive as it might change each weight in the model. First  we start with a pretrained LLM like GPT-3. This model already has a vast amount of knowledge and understanding of language. Then we provide task-specific datasets  which could be data for question answering or sentiment analysis or any other customer dataset. During training  full finetuning process makes slight adjustments to every weight in the pretrained model. While the model weights are substantial  we have other important aspects during training like Optimizer  which adds up to the cost. For example  Optimizer States  gradients  forward activation  and temporary memory. These additional components add up to the training cost.   Three main approaches are used in PEFT: Selective / reparameterization/additive.   1. SelectiveHere  we select a subset of initial LLM parameters to fine-tune.   2. ReparameterizationWe reparameterize model weights using a low-rank representation. We will discuss LoRA in detail below.   LORA: Low Rank Representation:   Each layer in a transformer architecture has multiple weight matrices for different operations  like self-attention or feed-forward networks. These matrices can have different sizes depending on the specific layer and configuration. Let us take an example by picking a matrix of size 512 x 64 = 32 768 parameters. Let us now see LoRA with rank = 8.   Original Weight Matrix: Dimensions: 512 x 64  Parameters: 32 768 (512 x 64)Matrix A (Rank Decomposition):\n",
            "Score\t 0.3805887199501794\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t b5152174-64c5-4a94-af55-d87c2b0e6cc3\n",
            "Text\t representation. We will discuss LoRA in detail below.   LORA: Low Rank Representation:   Each layer in a transformer architecture has multiple weight matrices for different operations  like self-attention or feed-forward networks. These matrices can have different sizes depending on the specific layer and configuration. Let us take an example by picking a matrix of size 512 x 64 = 32 768 parameters. Let us now see LoRA with rank = 8.   Original Weight Matrix: Dimensions: 512 x 64  Parameters: 32 768 (512 x 64)Matrix A (Rank Decomposition): Dimensions: 8 x 64 (rank x original dimension)  Parameters: 512 (8 x 64)Matrix B (Rank Decomposition): Dimensions: 8 x 512 (rank x original dimension)  Parameters: 4 096 (8 x 512)Total LORA Parameters: 512 (A) + 4 096 (B) = 4 608Approximation:   The original weight matrix (W) is approximated by the product of A and B:   Z  W  A * B   Reasoning Behind the Dimensions:   The dimensions of A and B are chosen to capture the essence of the original weight matrix (W) with fewer parameters.The rank (here  8) controls the trade-off between efficiency and accuracy. A lower rank leads to fewer parameters but might result in a slightly less accurate approximation.We can also create task-specific decomposition matrices.In the example we discussed  LORA achieves a reduction of approximately 86% in the number of trainable parameters needed for fine-tuning. Heres the summary.   Original Weight Matrix: 32 768 parameters (512 x 64)Total LORA Parameters: 4 608 parameters (512 + 4 096)3. AdditiveWe add trainable layers or parameters to the model in the form of adapter modules.   The two main additive approaches are:   Adapter Modules: These are small  trainable neural network modules strategically inserted into specific layers of the pre-trained LLM. They help the LLM learn task-specific information without drastically changing its underlying knowledge.Prompt Tuning: This approach doesnt involve adding any new modules to the model itself. Instead  it focuses on crafting specific prompts (essentially\n",
            "Score\t 0.37723002698107644\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "for src in res.source_nodes:\n",
        "    print(\"Node ID\\t\", src.node_id)\n",
        "    print(\"Text\\t\", src.text)\n",
        "    print(\"Score\\t\", src.score)\n",
        "    print(\"-_\" * 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcDZ5Dn8V_Md"
      },
      "source": [
        "# OpenAI Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8G8dyrsFV_Me"
      },
      "outputs": [],
      "source": [
        "from llama_index.agent.openai import OpenAIAgent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "z5d0WpDeV_Me"
      },
      "outputs": [],
      "source": [
        "system_message_openai_agent = \"\"\"You are an AI teacher, answering questions from students of an applied AI course on Large Language Models (LLMs or llm) and Retrieval Augmented Generation (RAG) for LLMs. Topics covered include training models, fine-tuning models, giving memory to LLMs, prompting tips, hallucinations and bias, vector databases, transformer architectures, embeddings, RAG frameworks, Langchain, LlamaIndex, making LLMs interact with tools, AI agents, reinforcement learning with human feedback. Questions should be understood in this context.\n",
        "\n",
        "Your answers are aimed to teach students, so they should be complete, clear, and easy to understand.\n",
        "\n",
        "Use the available tools to gather insights pertinent to the field of AI. Always use two tools at the same time. These tools accept a string (a user query rewritten as a statement) and return informative content regarding the domain of AI.\n",
        "e.g:\n",
        "User question: 'How can I fine-tune an LLM?'\n",
        "Input to the tool: 'Fine-tuning an LLM'\n",
        "\n",
        "User question: How can quantize an LLM?\n",
        "Input to the tool: 'Quantization for LLMs'\n",
        "\n",
        "User question: 'Teach me how to build an AI agent\"'\n",
        "Input to the tool: 'Building an AI Agent'\n",
        "\n",
        "Only some information returned by the tools might be relevant to the question, so ignore the irrelevant part and answer the question with what you have.\n",
        "\n",
        "Your responses are exclusively based on the output provided by the tools. Refrain from incorporating information not directly obtained from the tool's responses.\n",
        "\n",
        "When the conversation deepens or shifts focus within a topic, adapt your input to the tools to reflect these nuances. This means if a user requests further elaboration on a specific aspect of a previously discussed topic, you should reformulate your input to the tool to capture this new angle or more profound layer of inquiry.\n",
        "\n",
        "Provide comprehensive answers, ideally structured in multiple paragraphs, drawing from the tool's variety of relevant details. The depth and breadth of your responses should align with the scope and specificity of the information retrieved.\n",
        "\n",
        "Should the tools repository lack information on the queried topic, politely inform the user that the question transcends the bounds of your current knowledge base, citing the absence of relevant content in the tool's documentation.\n",
        "\n",
        "At the end of your answers, always invite the students to ask deeper questions about the topic if they have any. Make sure to reformulate the question to the tool to capture this new angle or more profound layer of inquiry.\n",
        "\n",
        "Do not refer to the documentation directly, but use the information provided within it to answer questions.\n",
        "\n",
        "If code is provided in the information, share it with the students. It's important to provide complete code blocks so they can execute the code when they copy and paste them.\n",
        "\n",
        "Make sure to format your answers in Markdown format, including code blocks and snippets.\n",
        "\n",
        "Politely reject questions not related to AI, while being cautious not to reject unfamiliar terms or acronyms too quickly.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "C54LMqwdV_Me"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model=\"gpt-4o\")\n",
        "\n",
        "agent = OpenAIAgent.from_tools(\n",
        "    llm=llm,\n",
        "    tools=[ai_tutor_knowledge_tool,llm_tool,],\n",
        "    system_prompt=system_message_openai_agent,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "aKqbYIlNV_Me",
        "outputId": "f15e9ffc-2ee1-4130-d342-f5fc3f3ae8e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The LLaMA model, developed by Meta AI, is a series of language models introduced on February 24, 2023. It was designed to provide accessible and efficient language modeling capabilities. The model comes in various parameter sizes, ranging from 1 billion to 405 billion parameters, with specific versions tailored for different hardware configurations.\n",
            "\n",
            "One of the notable achievements of the LLaMA model is its 13 billion parameter version, which outperformed the much larger GPT-3 model (175 billion parameters) on various natural language processing (NLP) benchmarks. This highlights the model's efficiency and effectiveness in handling language tasks. The LLaMA series has been updated with subsequent versions, such as LLaMA 2 and LLaMA 3, which introduced advancements like fine-tuning with Reinforcement Learning from Human Feedback (RLHF) and new techniques such as \"Ghost attention\" to improve consistency in multi-turn dialogues.\n",
            "\n",
            "The model was initially released under a non-commercial license, with access to its weights granted selectively to researchers and institutions. However, a significant leak of the model's weights on March 3, 2023, led to widespread distribution, sparking mixed reactions regarding its accessibility and potential misuse.\n",
            "\n",
            "Efforts have been made to facilitate the use of LLaMA on less powerful systems through projects like llama.cpp, an open-source re-implementation that introduced an optimized file format for efficient execution. Overall, LLaMA continues to influence ongoing research and development in AI language models, offering a balance between accessibility and performance.\n",
            "\n",
            "If you have more questions about the LLaMA model or its applications, feel free to ask!\n"
          ]
        }
      ],
      "source": [
        "response = agent.chat(\"What is the LLama model?\")\n",
        "print(response.response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "WqGWXuPDV_Me",
        "outputId": "e8909da9-0218-4aa4-a334-dde920692747",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter Efficient Fine-Tuning (PEFT) methods are designed to optimize the fine-tuning process of large language models (LLMs) by focusing on improving performance and efficiency without the need to retrain all model parameters. These methods are particularly useful for adapting models to specific tasks or improving their performance with limited computational resources.\n",
            "\n",
            "Here are some key techniques used in parameter efficient fine-tuning:\n",
            "\n",
            "1. **Supervised Fine-Tuning**: This involves using a large set of labeled data to train models. The goal is to minimize the loss on user prompts using autoregressive loss functions, which helps the model learn from specific examples and improve its performance on similar tasks.\n",
            "\n",
            "2. **Reinforcement Learning from Human Feedback (RLHF)**: This approach enhances model responses by incorporating human feedback, focusing on preferences for safety and helpfulness. Techniques like Rejection Sampling and Proximal Policy Optimization (PPO) are used to improve training efficiency and ensure that the model aligns with human expectations.\n",
            "\n",
            "3. **Ghost Attention**: This is a unique technique applied during training to enhance multi-turn consistency in dialogues. It involves concatenating relevant instructions to user messages and modifying the loss function to prioritize system messages, thereby improving the model's ability to maintain coherent conversations over multiple turns.\n",
            "\n",
            "4. **Text-Quality Classifiers**: These classifiers are used to filter training data, ensuring that only high-quality input is used. This improves the model's robustness and reliability by preventing it from learning from poor-quality or irrelevant data.\n",
            "\n",
            "5. **Task-Specific Fine-Tuning**: Models like Alpaca use specialized training recipes, such as the \"Self-Instruct\" method, to adapt foundational models to specific tasks effectively and at a lower cost. This approach allows models to be fine-tuned for particular applications without extensive retraining.\n",
            "\n",
            "These techniques highlight the importance of data quality and feedback mechanisms in achieving efficient fine-tuning for LLMs. They enable models to leverage their existing architecture more effectively, facilitating better performance even with limited computational resources.\n",
            "\n",
            "If you have further questions about parameter efficient fine-tuning or any specific technique, feel free to ask!\n"
          ]
        }
      ],
      "source": [
        "response = agent.chat(\"Explain parameter Efficient Fine tuning method\")\n",
        "print(response.response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "TSPTAruzV_Me",
        "outputId": "b073ea0f-a500-4258-98b3-98e682a2257a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm here to assist with questions related to AI and large language models. If you have any questions about those topics, feel free to ask!\n"
          ]
        }
      ],
      "source": [
        "response = agent.chat(\"Write the recipe for a chocolate cake.\")\n",
        "print(response.response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWb1I1WvV_Mf"
      },
      "source": [
        "# Code related questions to GPT-4o, the remaining questions to Gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "tY82urjtV_Mf"
      },
      "outputs": [],
      "source": [
        "from llama_index.agent.openai import OpenAIAgent\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from llama_index.core.query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import PydanticSingleSelector\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "# initialize LLMs\n",
        "gpt_4o_llm = OpenAI(model=\"gpt-4o\")\n",
        "gemini_llm = Gemini(model=\"models/gemini-1.5-flash\", temperature=1, max_tokens=512)\n",
        "\n",
        "# define query engines\n",
        "llm_query_engine_code = vector_index.as_query_engine(\n",
        "    llm=gpt_4o_llm,\n",
        "    similarity_top_k=3,\n",
        "    embed_model=OpenAIEmbedding(model=\"text-embedding-3-small\", mode=\"text_search\"),\n",
        ")\n",
        "\n",
        "llm_query_engine_rest = vector_index.as_query_engine(\n",
        "    llm=gemini_llm,\n",
        "    similarity_top_k=3,\n",
        "    embed_model=OpenAIEmbedding(model=\"text-embedding-3-small\", mode=\"text_search\"),\n",
        ")\n",
        "\n",
        "# define tools for LLM\n",
        "llm_tool_code = QueryEngineTool.from_defaults(\n",
        "    query_engine=llm_query_engine_code,\n",
        "    description=\"Ideal for handling code-related queries, technical implementations, and troubleshooting involving Large Language Models.\",\n",
        "    name=\"LLMCodeTool\",\n",
        ")\n",
        "\n",
        "llm_tool_rest = QueryEngineTool.from_defaults(\n",
        "    query_engine=llm_query_engine_rest,\n",
        "    description=\"Best suited for answering conceptual, theoretical, and general questions about Large Language Models.\",\n",
        "    name=\"LLMGeneralTool\",\n",
        ")\n",
        "\n",
        "# Initialize OpenAIAgent with the system message and the router query engine\n",
        "agent = OpenAIAgent.from_tools(\n",
        "    llm=gpt_4o_llm,  # The base LLM, used only if no other tools apply\n",
        "    tools=[llm_tool_code, llm_tool_rest],\n",
        "    system_prompt=system_message_openai_agent,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "h3NMpGxYV_Mf",
        "outputId": "58b3b1a4-51f6-475c-b940-43e282e2ccd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLMGeneralTool\n",
            "LLMCodeTool\n"
          ]
        }
      ],
      "source": [
        "# Test the agent with a code-related question\n",
        "response = agent.chat(\"How do I fine-tune the LLama model? Write the code for it.\")\n",
        "for source in response.sources:\n",
        "    print(source.tool_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "ysB0X7D5V_Mf",
        "outputId": "aa54be93-2582-4157-9d42-de3d74f6ac98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLMGeneralTool\n"
          ]
        }
      ],
      "source": [
        "# Test the agent with a Non code-related question\n",
        "response1 = agent.chat(\"What is the relationship between Llama and Meta?\")\n",
        "for source in response1.sources:\n",
        "    print(source.tool_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pdYfpsHg60MJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "535244513b504cf0ae900cbb9873ca0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7a0fb7e522e040e7b3d218006c090201",
              "IPY_MODEL_4f2d401e824845d9b23b848421fa95e0",
              "IPY_MODEL_02e0480193574ebc97d082d429615fc6"
            ],
            "layout": "IPY_MODEL_cc6df9ddbe004f4d901ac4a35dd49c91"
          }
        },
        "7a0fb7e522e040e7b3d218006c090201": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24cbfe06ba4c4c9fb010ab74485c92fa",
            "placeholder": "​",
            "style": "IPY_MODEL_1daa2d514e4848a7ac602be83fdd6d0e",
            "value": "ai_tutor_knowledge.jsonl: 100%"
          }
        },
        "4f2d401e824845d9b23b848421fa95e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad02a9bfcd3642fb834f3fb452efaf10",
            "max": 6960611,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d24b90385f884b8b921685f4fe1f6742",
            "value": 6960611
          }
        },
        "02e0480193574ebc97d082d429615fc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd77c85e0680449caeabbf4c5febaf24",
            "placeholder": "​",
            "style": "IPY_MODEL_b2edcdbabf3c424788745bdc72504d59",
            "value": " 6.96M/6.96M [00:00&lt;00:00, 36.9MB/s]"
          }
        },
        "cc6df9ddbe004f4d901ac4a35dd49c91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24cbfe06ba4c4c9fb010ab74485c92fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1daa2d514e4848a7ac602be83fdd6d0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad02a9bfcd3642fb834f3fb452efaf10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d24b90385f884b8b921685f4fe1f6742": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd77c85e0680449caeabbf4c5febaf24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2edcdbabf3c424788745bdc72504d59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "925ad382a16d4761bde196bdd5134366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_574b186993104a4a95f64a93c4c48cd3",
              "IPY_MODEL_d6bb6f844e034f99a0bda181fccd2a8e",
              "IPY_MODEL_3db013c960004990bf3011f911a683fc"
            ],
            "layout": "IPY_MODEL_d719a2c329804d908bb1aa831cd3dfbb"
          }
        },
        "574b186993104a4a95f64a93c4c48cd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f48aba326984fa09159bd1b16a97bae",
            "placeholder": "​",
            "style": "IPY_MODEL_59a47e7061af49dba9a5f54a72ef2d25",
            "value": "vectorstore.zip: 100%"
          }
        },
        "d6bb6f844e034f99a0bda181fccd2a8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ad3b4cf967e4168bd140b42e673e4b8",
            "max": 97198458,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ec9f64527bcd4917aebb453ed6ab4d30",
            "value": 97198458
          }
        },
        "3db013c960004990bf3011f911a683fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8398f3bafb244529a6f6c4edc8354dbc",
            "placeholder": "​",
            "style": "IPY_MODEL_0635c26c2deb495a82aaac8d0aa5f073",
            "value": " 97.2M/97.2M [00:01&lt;00:00, 88.8MB/s]"
          }
        },
        "d719a2c329804d908bb1aa831cd3dfbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f48aba326984fa09159bd1b16a97bae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59a47e7061af49dba9a5f54a72ef2d25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ad3b4cf967e4168bd140b42e673e4b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec9f64527bcd4917aebb453ed6ab4d30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8398f3bafb244529a6f6c4edc8354dbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0635c26c2deb495a82aaac8d0aa5f073": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}