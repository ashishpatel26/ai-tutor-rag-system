{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/towardsai/ai-tutor-rag-system/blob/main/notebooks/13-Adding_Router.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zE1h0uQV7uT"
   },
   "source": [
    "# Install Packages and Setup Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "QPJzr-I9XQ7l",
    "outputId": "7e0a939a-0a66-40fa-8ac6-2b389b06b9dd"
   },
   "outputs": [],
   "source": [
    "!pip install -q llama-index==0.10.57 openai==1.37.0 cohere==5.6.2 tiktoken==0.7.0 chromadb==0.5.5 html2text sentence_transformers pydantic llama-index-vector-stores-chroma==0.1.10 kaleido==0.2.1 llama-index-llms-gemini==0.1.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riuXwpSPcvWC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the following API Keys in the Python environment. Will be used later.\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_OPENAI_API_KEY>\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"<YOUR_GOOGLE_API_KEY>\"\n",
    "\n",
    "#from google.colab import userdata\n",
    "#os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api_key')\n",
    "#os.environ[\"GOOGLE_API_KEY\"] = userdata.get('Google_api_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jIEeZzqLbz0J"
   },
   "outputs": [],
   "source": [
    "# Allows running asyncio in environments with an existing event loop, like Jupyter notebooks.\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bkgi2OrYzF7q"
   },
   "source": [
    "# Load a Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9oGT6crooSSj"
   },
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = OpenAI(temperature=1, model=\"gpt-4o-mini\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWaT6rL7ksp8"
   },
   "source": [
    "# Load Indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "925ad382a16d4761bde196bdd5134366",
      "574b186993104a4a95f64a93c4c48cd3",
      "d6bb6f844e034f99a0bda181fccd2a8e",
      "3db013c960004990bf3011f911a683fc",
      "d719a2c329804d908bb1aa831cd3dfbb",
      "5f48aba326984fa09159bd1b16a97bae",
      "59a47e7061af49dba9a5f54a72ef2d25",
      "7ad3b4cf967e4168bd140b42e673e4b8",
      "ec9f64527bcd4917aebb453ed6ab4d30",
      "8398f3bafb244529a6f6c4edc8354dbc",
      "0635c26c2deb495a82aaac8d0aa5f073"
     ]
    },
    "id": "qyuq48p_4KWu",
    "outputId": "191d6e62-3914-4834-9417-e9fd768139e8"
   },
   "outputs": [],
   "source": [
    "# Downloading Vector store from Hugging face hub\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "vectorstore = hf_hub_download(repo_id=\"jaiganesan/ai_tutor_knowledge\", filename=\"vectorstore.zip\", repo_type=\"dataset\", local_dir=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SodY2Xpf_kxg",
    "outputId": "555793fa-1170-49c9-ac0f-baf5471a9e8d"
   },
   "outputs": [],
   "source": [
    "!unzip vectorstore.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXi56KTXk2sp"
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Create your index\n",
    "db = chromadb.PersistentClient(path=\"./ai_tutor_knowledge\")\n",
    "chroma_collection = db.get_or_create_collection(\"ai_tutor_knowledge\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "vector_index = VectorStoreIndex.from_vector_store(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lgpzgnHFV_MH",
    "outputId": "1c83e6bc-f1ab-4ebb-e3e7-e53153a736fb"
   },
   "outputs": [],
   "source": [
    "# Query Engine\n",
    "ai_tutor_knowledge_query_engine = vector_index.as_query_engine(similarity_top_k=3)\n",
    "\n",
    "res = ai_tutor_knowledge_query_engine.query(\"How does Retrieval Augmented Generation (RAG) work?\")\n",
    "print(res.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cKN0brIPV_MJ",
    "outputId": "4ca24a96-392a-45a8-fbd8-cb72c5a820d4"
   },
   "outputs": [],
   "source": [
    "for src in res.source_nodes:\n",
    "    print(\"Node ID\\t\", src.node_id)\n",
    "    print(\"Title\\t\", src.metadata[\"title\"])\n",
    "    print(\"Text\\t\", src.text)\n",
    "    print(\"Score\\t\", src.score)\n",
    "    print(\"Metadata\\t\", src.metadata)\n",
    "    print(\"-_\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXqjul4MV_MJ"
   },
   "source": [
    "# Router\n",
    "\n",
    "Routers are modules that take in a user query and a set of “choices” (defined by metadata), and returns one or more selected choices.\n",
    "\n",
    "They can be used for the following use cases and more:\n",
    "\n",
    "- Selecting the right data source among a diverse range of data sources\n",
    "\n",
    "- Deciding whether to do summarization (e.g. using summary index query engine) or semantic search (e.g. using vector index query engine)\n",
    "\n",
    "- Deciding whether to “try” out a bunch of choices at once and combine the results (using multi-routing capabilities).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uunnC6XjV_Mb"
   },
   "source": [
    "## Lets create a different query engine with Mistral AI information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AXnP6NfcBCjA"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "wiki_titles = [\n",
    "    \"Mistral AI\",\n",
    "    \"Llama (language model)\",\n",
    "    \"Claude AI\",\n",
    "    \"OpenAI\",\n",
    "    \"Gemini AI\",\n",
    "]\n",
    "\n",
    "data_path = Path(\"llm_data_wiki\")\n",
    "\n",
    "if not data_path.exists():\n",
    "    data_path.mkdir()\n",
    "\n",
    "for title in wiki_titles:\n",
    "    response = requests.get(\n",
    "        \"https://en.wikipedia.org/w/api.php\",\n",
    "        params={\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": title,\n",
    "            \"prop\": \"extracts\",\n",
    "            \"explaintext\": True,\n",
    "        },\n",
    "    ).json()\n",
    "\n",
    "    page = next(iter(response[\"query\"][\"pages\"].values()))\n",
    "\n",
    "    if \"extract\" in page:\n",
    "        wiki_text = page[\"extract\"]\n",
    "\n",
    "        with open(data_path / \"llm_data_wiki.txt\", \"a\") as fp:\n",
    "            fp.write(f\"Title: {title}\\n{wiki_text}\\n\\n\")\n",
    "    else:\n",
    "        print(f\"No extract found for '{title}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OsSsh4_bV_Mb",
    "outputId": "c9a9abc4-dc1d-4bd9-afbe-2b78f9d92a26"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.text_splitter import TokenTextSplitter\n",
    "from llama_index.core.extractors import (\n",
    "    SummaryExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    "    KeywordExtractor,\n",
    ")\n",
    "\n",
    "# Assuming you have prepared a directory for llm data\n",
    "documents = SimpleDirectoryReader(\"llm_data_wiki\").load_data()\n",
    "\n",
    "text_splitter = TokenTextSplitter(separator=\" \", chunk_size=512, chunk_overlap=128)\n",
    "\n",
    "transformations = [\n",
    "    text_splitter,\n",
    "    QuestionsAnsweredExtractor(questions=2),\n",
    "    SummaryExtractor(summaries=[\"prev\", \"self\"]),\n",
    "    KeywordExtractor(keywords=10),\n",
    "    OpenAIEmbedding(model=\"text-embedding-3-small\"),\n",
    "]\n",
    "\n",
    "llm_index = VectorStoreIndex.from_documents(documents=documents, transformations=transformations)\n",
    "\n",
    "llm_query_engine = llm_index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3hDxvMuV_Mc"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import PydanticSingleSelector\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "# initialize tools\n",
    "ai_tutor_knowledge_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=ai_tutor_knowledge_query_engine,\n",
    "    description=\"Useful for questions about general generative AI concepts\",\n",
    ")\n",
    "llm_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=llm_query_engine,\n",
    "    description=\"Useful for questions about particular LLMs like Mistral, Claude, OpenAI, Gemini\",\n",
    ")\n",
    "\n",
    "# initialize router query engine (single selection, pydantic)\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=PydanticSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        ai_tutor_knowledge_tool,\n",
    "        llm_tool,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9nnZaOPuV_Mc",
    "outputId": "823cb762-94cc-4ce5-879b-de67b16425ed"
   },
   "outputs": [],
   "source": [
    "res = query_engine.query(\n",
    "    \"What is the LLama model?\",\n",
    ")\n",
    "print(res.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zLVDAW7hV_Md",
    "outputId": "fcfb8584-6ec6-418d-ce25-126b248ee4c3"
   },
   "outputs": [],
   "source": [
    "for src in res.source_nodes:\n",
    "    print(\"Node ID\\t\", src.node_id)\n",
    "    print(\"Text\\t\", src.text)\n",
    "    print(\"Score\\t\", src.score)\n",
    "    print(\"-_\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nvqkYeF9V_Md",
    "outputId": "57ffedc5-cbbe-4c54-8516-0edbda8419b2"
   },
   "outputs": [],
   "source": [
    "res = query_engine.query(\"Explain parameter-efficient finetuning methods\")\n",
    "print(res.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yNKJdJhzV_Md",
    "outputId": "5dbe12e1-f67f-49a0-f29f-708632420ee2"
   },
   "outputs": [],
   "source": [
    "for src in res.source_nodes:\n",
    "    print(\"Node ID\\t\", src.node_id)\n",
    "    print(\"Text\\t\", src.text)\n",
    "    print(\"Score\\t\", src.score)\n",
    "    print(\"-_\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcDZ5Dn8V_Md"
   },
   "source": [
    "# OpenAI Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8G8dyrsFV_Me"
   },
   "outputs": [],
   "source": [
    "from llama_index.agent.openai import OpenAIAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5d0WpDeV_Me"
   },
   "outputs": [],
   "source": [
    "system_message_openai_agent = \"\"\"You are an AI teacher, answering questions from students of an applied AI course on Large Language Models (LLMs or llm) and Retrieval Augmented Generation (RAG) for LLMs. Topics covered include training models, fine-tuning models, giving memory to LLMs, prompting tips, hallucinations and bias, vector databases, transformer architectures, embeddings, RAG frameworks, Langchain, LlamaIndex, making LLMs interact with tools, AI agents, reinforcement learning with human feedback. Questions should be understood in this context.\n",
    "\n",
    "Your answers are aimed to teach students, so they should be complete, clear, and easy to understand.\n",
    "\n",
    "Use the available tools to gather insights pertinent to the field of AI. Always use two tools at the same time. These tools accept a string (a user query rewritten as a statement) and return informative content regarding the domain of AI.\n",
    "e.g:\n",
    "User question: 'How can I fine-tune an LLM?'\n",
    "Input to the tool: 'Fine-tuning an LLM'\n",
    "\n",
    "User question: How can quantize an LLM?\n",
    "Input to the tool: 'Quantization for LLMs'\n",
    "\n",
    "User question: 'Teach me how to build an AI agent\"'\n",
    "Input to the tool: 'Building an AI Agent'\n",
    "\n",
    "Only some information returned by the tools might be relevant to the question, so ignore the irrelevant part and answer the question with what you have.\n",
    "\n",
    "Your responses are exclusively based on the output provided by the tools. Refrain from incorporating information not directly obtained from the tool's responses.\n",
    "\n",
    "When the conversation deepens or shifts focus within a topic, adapt your input to the tools to reflect these nuances. This means if a user requests further elaboration on a specific aspect of a previously discussed topic, you should reformulate your input to the tool to capture this new angle or more profound layer of inquiry.\n",
    "\n",
    "Provide comprehensive answers, ideally structured in multiple paragraphs, drawing from the tool's variety of relevant details. The depth and breadth of your responses should align with the scope and specificity of the information retrieved.\n",
    "\n",
    "Should the tools repository lack information on the queried topic, politely inform the user that the question transcends the bounds of your current knowledge base, citing the absence of relevant content in the tool's documentation.\n",
    "\n",
    "At the end of your answers, always invite the students to ask deeper questions about the topic if they have any. Make sure to reformulate the question to the tool to capture this new angle or more profound layer of inquiry.\n",
    "\n",
    "Do not refer to the documentation directly, but use the information provided within it to answer questions.\n",
    "\n",
    "If code is provided in the information, share it with the students. It's important to provide complete code blocks so they can execute the code when they copy and paste them.\n",
    "\n",
    "Make sure to format your answers in Markdown format, including code blocks and snippets.\n",
    "\n",
    "Politely reject questions not related to AI, while being cautious not to reject unfamiliar terms or acronyms too quickly.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C54LMqwdV_Me"
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o\")\n",
    "\n",
    "agent = OpenAIAgent.from_tools(\n",
    "    llm=llm,\n",
    "    tools=[ai_tutor_knowledge_tool, llm_tool],\n",
    "    system_prompt=system_message_openai_agent,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aKqbYIlNV_Me",
    "outputId": "f15e9ffc-2ee1-4130-d342-f5fc3f3ae8e6"
   },
   "outputs": [],
   "source": [
    "response = agent.chat(\"What is the LLama model?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WqGWXuPDV_Me",
    "outputId": "e8909da9-0218-4aa4-a334-dde920692747"
   },
   "outputs": [],
   "source": [
    "response = agent.chat(\"Explain parameter-efficient finetuning methods\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TSPTAruzV_Me",
    "outputId": "b073ea0f-a500-4258-98b3-98e682a2257a"
   },
   "outputs": [],
   "source": [
    "response = agent.chat(\"Write the recipe for a chocolate cake.\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWb1I1WvV_Mf"
   },
   "source": [
    "# Code related questions to GPT-4o, the remaining questions to Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tY82urjtV_Mf"
   },
   "outputs": [],
   "source": [
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import PydanticSingleSelector\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "# initialize LLMs\n",
    "gpt_4o_llm = OpenAI(model=\"gpt-4o\")\n",
    "gemini_llm = Gemini(model=\"models/gemini-1.5-flash\", temperature=1, max_tokens=512)\n",
    "\n",
    "# define query engines\n",
    "llm_query_engine_code = vector_index.as_query_engine(\n",
    "    llm=gpt_4o_llm,\n",
    "    similarity_top_k=3,\n",
    "    embed_model=OpenAIEmbedding(model=\"text-embedding-3-small\", mode=\"text_search\"),\n",
    ")\n",
    "\n",
    "llm_query_engine_rest = vector_index.as_query_engine(\n",
    "    llm=gemini_llm,\n",
    "    similarity_top_k=3,\n",
    "    embed_model=OpenAIEmbedding(model=\"text-embedding-3-small\", mode=\"text_search\"),\n",
    ")\n",
    "\n",
    "# define tools for LLM\n",
    "llm_tool_code = QueryEngineTool.from_defaults(\n",
    "    query_engine=llm_query_engine_code,\n",
    "    description=\"Ideal for handling code-related queries, technical implementations, and troubleshooting involving Large Language Models.\",\n",
    "    name=\"LLMCodeTool\",\n",
    ")\n",
    "\n",
    "llm_tool_rest = QueryEngineTool.from_defaults(\n",
    "    query_engine=llm_query_engine_rest,\n",
    "    description=\"Best suited for answering conceptual, theoretical, and general questions about Large Language Models.\",\n",
    "    name=\"LLMGeneralTool\",\n",
    ")\n",
    "\n",
    "# Initialize OpenAIAgent with the system message and the router query engine\n",
    "agent = OpenAIAgent.from_tools(\n",
    "    llm=gpt_4o_llm,  # The base LLM, used only if no other tools apply\n",
    "    tools=[llm_tool_code, llm_tool_rest],\n",
    "    system_prompt=system_message_openai_agent,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "h3NMpGxYV_Mf",
    "outputId": "58b3b1a4-51f6-475c-b940-43e282e2ccd4"
   },
   "outputs": [],
   "source": [
    "# Test the agent with a code-related question\n",
    "response = agent.chat(\"How do I fine-tune the LLama model? Write the code for it.\")\n",
    "for source in response.sources:\n",
    "    print(source.tool_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ysB0X7D5V_Mf",
    "outputId": "aa54be93-2582-4157-9d42-de3d74f6ac98"
   },
   "outputs": [],
   "source": [
    "# Test the agent with a Non code-related question\n",
    "response1 = agent.chat(\"What is the relationship between Llama and Meta?\")\n",
    "for source in response1.sources:\n",
    "    print(source.tool_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pdYfpsHg60MJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "llamaindexkernel",
   "language": "python",
   "name": "llamaindexkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02e0480193574ebc97d082d429615fc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd77c85e0680449caeabbf4c5febaf24",
      "placeholder": "​",
      "style": "IPY_MODEL_b2edcdbabf3c424788745bdc72504d59",
      "value": " 6.96M/6.96M [00:00&lt;00:00, 36.9MB/s]"
     }
    },
    "0635c26c2deb495a82aaac8d0aa5f073": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1daa2d514e4848a7ac602be83fdd6d0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "24cbfe06ba4c4c9fb010ab74485c92fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3db013c960004990bf3011f911a683fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8398f3bafb244529a6f6c4edc8354dbc",
      "placeholder": "​",
      "style": "IPY_MODEL_0635c26c2deb495a82aaac8d0aa5f073",
      "value": " 97.2M/97.2M [00:01&lt;00:00, 88.8MB/s]"
     }
    },
    "4f2d401e824845d9b23b848421fa95e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad02a9bfcd3642fb834f3fb452efaf10",
      "max": 6960611,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d24b90385f884b8b921685f4fe1f6742",
      "value": 6960611
     }
    },
    "535244513b504cf0ae900cbb9873ca0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7a0fb7e522e040e7b3d218006c090201",
       "IPY_MODEL_4f2d401e824845d9b23b848421fa95e0",
       "IPY_MODEL_02e0480193574ebc97d082d429615fc6"
      ],
      "layout": "IPY_MODEL_cc6df9ddbe004f4d901ac4a35dd49c91"
     }
    },
    "574b186993104a4a95f64a93c4c48cd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5f48aba326984fa09159bd1b16a97bae",
      "placeholder": "​",
      "style": "IPY_MODEL_59a47e7061af49dba9a5f54a72ef2d25",
      "value": "vectorstore.zip: 100%"
     }
    },
    "59a47e7061af49dba9a5f54a72ef2d25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5f48aba326984fa09159bd1b16a97bae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a0fb7e522e040e7b3d218006c090201": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_24cbfe06ba4c4c9fb010ab74485c92fa",
      "placeholder": "​",
      "style": "IPY_MODEL_1daa2d514e4848a7ac602be83fdd6d0e",
      "value": "ai_tutor_knowledge.jsonl: 100%"
     }
    },
    "7ad3b4cf967e4168bd140b42e673e4b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8398f3bafb244529a6f6c4edc8354dbc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "925ad382a16d4761bde196bdd5134366": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_574b186993104a4a95f64a93c4c48cd3",
       "IPY_MODEL_d6bb6f844e034f99a0bda181fccd2a8e",
       "IPY_MODEL_3db013c960004990bf3011f911a683fc"
      ],
      "layout": "IPY_MODEL_d719a2c329804d908bb1aa831cd3dfbb"
     }
    },
    "ad02a9bfcd3642fb834f3fb452efaf10": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2edcdbabf3c424788745bdc72504d59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cc6df9ddbe004f4d901ac4a35dd49c91": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd77c85e0680449caeabbf4c5febaf24": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d24b90385f884b8b921685f4fe1f6742": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d6bb6f844e034f99a0bda181fccd2a8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7ad3b4cf967e4168bd140b42e673e4b8",
      "max": 97198458,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ec9f64527bcd4917aebb453ed6ab4d30",
      "value": 97198458
     }
    },
    "d719a2c329804d908bb1aa831cd3dfbb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ec9f64527bcd4917aebb453ed6ab4d30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
