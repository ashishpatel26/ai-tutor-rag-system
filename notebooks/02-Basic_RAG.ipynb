{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyND3B7W2jhm8NF7/+wlqCAN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/towardsai/ai-tutor-rag-system/blob/main/notebooks/02-Basic_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaB4G9zr0BYm",
        "outputId": "03c7161a-e3e2-4bf6-e148-ef94ddca73a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m978.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.7/51.7 kB\u001b[0m \u001b[31m963.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q openai==1.6.0 cohere==4.39 tiktoken==0.5.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_OPENAI_KEY>\""
      ],
      "metadata": {
        "id": "MYvUA6CF2Le6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_file = \"./index_with_embedding.json\""
      ],
      "metadata": {
        "id": "0ViVXXIqXBai"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "D8Nzx-cN_bDz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Paul Graham Essay (JSON)"
      ],
      "metadata": {
        "id": "5JpI7GiZ--Gw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/run-llama/llama_index/main/examples/paul_graham_essay/index.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6NEJT9S2OoH",
        "outputId": "b7b4eb4c-77e2-439f-ed10-d987dbce77f0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-20 16:11:36--  https://raw.githubusercontent.com/run-llama/llama_index/main/examples/paul_graham_essay/index.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 176588 (172K) [text/plain]\n",
            "Saving to: ‘index.json’\n",
            "\n",
            "\rindex.json            0%[                    ]       0  --.-KB/s               \rindex.json          100%[===================>] 172.45K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-12-20 16:11:36 (7.10 MB/s) - ‘index.json’ saved [176588/176588]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read JSON"
      ],
      "metadata": {
        "id": "oYDd03Qn_clh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('./index.json', 'r') as file:\n",
        "    data = json.load(file)"
      ],
      "metadata": {
        "id": "XjW10Cnz_DVZ"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data['index_struct']['all_nodes']"
      ],
      "metadata": {
        "id": "A2U5PnB2CnUO"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len( data )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcQ7Ge_XCuXa",
        "outputId": "669be133-a912-4d74-83c2-3ee109a47ed7"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "63"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(data).transpose()"
      ],
      "metadata": {
        "id": "YNk1hxGVDqTu"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKdFSOb0NXjx",
        "outputId": "0b16e381-fea1-4c04-f656-03488705f770"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['text', 'doc_id', 'index', 'child_indices', 'embedding', 'ref_doc_id'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply Embedding"
      ],
      "metadata": {
        "id": "21pFDgNdW9rO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "def get_embedding(text):\n",
        "  try:\n",
        "    # Remove newlines\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    res = client.embeddings.create(input = [text], model=\"text-embedding-ada-002\")\n",
        "\n",
        "    return res.data[0].embedding\n",
        "\n",
        "  except:\n",
        "        return None"
      ],
      "metadata": {
        "id": "AfS9w9eQAKyu"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "not bool( embedding_file )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p010q7cqXWi3",
        "outputId": "8fb409b1-e0ce-4412-f5a5-46cb475f5ce1"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "if not embedding_file:\n",
        "  print(\"Generating embeddings...\")\n",
        "  for index, row in tqdm( df.iterrows() ):\n",
        "    row['embedding'] = get_embedding( row['text'] )\n",
        "\n",
        "  df.to_json(\"./index_with_embedding.json\")\n",
        "\n",
        "else:\n",
        "  print(\"Loaded the embedding file.\")\n",
        "  with open('./index_with_embedding.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "  df = pd.DataFrame(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qC6aeFr3Rmi2",
        "outputId": "7f531fd3-ae10-4ff1-f42e-1a52e90b71b7"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded the embedding file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# User Question"
      ],
      "metadata": {
        "id": "E_qrXwImXrXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "QUESTION = \"How much budget Paul Graham had to spend each day when he was in Florence?\""
      ],
      "metadata": {
        "id": "mHt4bZjYRaiY"
      },
      "execution_count": 293,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QUESTION_emb = get_embedding( QUESTION )"
      ],
      "metadata": {
        "id": "OMQG6DtaVKGl"
      },
      "execution_count": 294,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len( QUESTION_emb )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGTa7cqCX97q",
        "outputId": "d38fbd8e-61f7-4603-d2b2-d1b5ff706cbd"
      },
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1536"
            ]
          },
          "metadata": {},
          "execution_count": 295
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate Cosine Similarities"
      ],
      "metadata": {
        "id": "BXNzNWrJYWhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BAD_SOURCE_emb = get_embedding( \"The sky is blue.\" )\n",
        "GOOD_SOURCE_emb = get_embedding( \"Paul Graham had budget of $1 per day in Florence.\" )"
      ],
      "metadata": {
        "id": "LqDWcPd4b-ZI"
      },
      "execution_count": 296,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# A sample that how a good piece of text can achieve high similarity score compared\n",
        "# to a completely unrelated text.\n",
        "print(\"> Bad Response Score:\", cosine_similarity([QUESTION_emb], [BAD_SOURCE_emb]) )\n",
        "print(\"> Good Response Score:\", cosine_similarity([QUESTION_emb], [GOOD_SOURCE_emb]) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OI00eN86YZKB",
        "outputId": "cb4c84a8-97ea-4def-d28a-2fea16514bd7"
      },
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Bad Response Score: [[0.71542785]]\n",
            "> Good Response Score: [[0.94895731]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarities = cosine_similarity( [QUESTION_emb], df['embedding'].tolist() )"
      ],
      "metadata": {
        "id": "iYzyEU6-bwSz"
      },
      "execution_count": 298,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarities"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNPN7OAXemmH",
        "outputId": "6f1bccd1-9b49-41c6-b660-1b679cceae53"
      },
      "execution_count": 299,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.74283738, 0.72700095, 0.75178514, 0.72451674, 0.71790563,\n",
              "        0.71921944, 0.75048449, 0.74530984, 0.75329709, 0.76653264,\n",
              "        0.76974562, 0.72420627, 0.73249944, 0.76877011, 0.79248341,\n",
              "        0.73697747, 0.71247212, 0.78187566, 0.76031472, 0.73767647,\n",
              "        0.72234783, 0.74282612, 0.75314902, 0.74521852, 0.75741529,\n",
              "        0.7501051 , 0.75077242, 0.76393937, 0.76099337, 0.76104132,\n",
              "        0.74024353, 0.71837665, 0.76223564, 0.71520494, 0.72417841,\n",
              "        0.76740974, 0.74417228, 0.7428031 , 0.74380591, 0.76662179,\n",
              "        0.76371986, 0.74027053, 0.72705364, 0.76053316, 0.72927648,\n",
              "        0.72150454, 0.7451773 , 0.69488923, 0.73433034, 0.77833212,\n",
              "        0.74265443, 0.76117176, 0.7494365 , 0.71624952, 0.72440837,\n",
              "        0.73051577, 0.72781133, 0.74202563, 0.75499305, 0.7157058 ,\n",
              "        0.72145234, 0.76054956, 0.71896691]])"
            ]
          },
          "metadata": {},
          "execution_count": 299
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sort the scores\n",
        "highest_index = np.argmax( cosine_similarities )"
      ],
      "metadata": {
        "id": "g0cBfePFaayw"
      },
      "execution_count": 300,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_chunks_to_retrieve = 3\n",
        "\n",
        "indices = np.argsort(cosine_similarities[0])[::-1][:number_of_chunks_to_retrieve]"
      ],
      "metadata": {
        "id": "vBbHJ7uihfKO"
      },
      "execution_count": 301,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-XI1_7mhlw4",
        "outputId": "ac5a55ad-bfc7-409e-cf2e-5cc1dce75544"
      },
      "execution_count": 302,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([14, 17, 49])"
            ]
          },
          "metadata": {},
          "execution_count": 302
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the highest scored pieces of text\n",
        "for idx, item in enumerate( df.text[indices] ):\n",
        "  print(f\"> Chunk {idx+1}\")\n",
        "  print(item)\n",
        "  print(\"----\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPmhCb9kfB0w",
        "outputId": "aafb01ef-65e1-4630-f849-96a2eaf1ee06"
      },
      "execution_count": 303,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Chunk 1\n",
            "of thinking, but at the time it caused a lot of friction. Toward the end of the year I spent much of my time surreptitiously working on On Lisp, which I had by this time gotten a contract to publish.\n",
            "\n",
            "The good part was that I got paid huge amounts of money, especially by art student standards. In Florence, after paying my part of the rent, my budget for everything else had been $7 a day. Now I was getting paid more than 4 times that every hour, even when I was just sitting in a meeting. By living cheaply I not only managed to save enough to go back to RISD, but also paid off my college loans.\n",
            "\n",
            "I learned some useful things at Interleaf, though they were mostly about what not to do. I learned that it's better for technology companies to be run by product people than sales people (though sales is a real skill and people who are good at it are really good at it), that it leads to bugs when code is edited by too many people, that cheap office space is no bargain if it's depressing, that planned meetings are inferior to corridor conversations, that big, bureaucratic customers are a dangerous source of money, and that there's not much overlap between conventional office hours and the optimal time for hacking, or conventional offices and the optimal place for it.\n",
            "\n",
            "But the most important thing I learned, and which I used in both Viaweb and Y Combinator, is that the low end eats the high end: that it's good to\n",
            "----\n",
            "> Chunk 2\n",
            "I took at RISD, but otherwise I was basically teaching myself to paint, and I could do that for free. So in 1993 I dropped out. I hung around Providence for a bit, and then my college friend Nancy Parmet did me a big favor. A rent-controlled apartment in a building her mother owned in New York was becoming vacant. Did I want it? It wasn't much more than my current place, and New York was supposed to be where the artists were. So yes, I wanted it! [7]\n",
            "\n",
            "Asterix comics begin by zooming in on a tiny corner of Roman Gaul that turns out not to be controlled by the Romans. You can do something similar on a map of New York City: if you zoom in on the Upper East Side, there's a tiny corner that's not rich, or at least wasn't in 1993. It's called Yorkville, and that was my new home. Now I was a New York artist — in the strictly technical sense of making paintings and living in New York.\n",
            "\n",
            "I was nervous about money, because I could sense that Interleaf was on the way down. Freelance Lisp hacking work was very rare, and I didn't want to have to program in another language, which in those days would have meant C++ if I was lucky. So with my unerring nose for financial opportunity, I decided to write another book on Lisp. This would be a popular book, the sort of book that could be used as a textbook. I imagined\n",
            "----\n",
            "> Chunk 3\n",
            "from writing essays during most of this time, or I'd never have finished. In late 2015 I spent 3 months writing essays, and when I went back to working on Bel I could barely understand the code. Not so much because it was badly written as because the problem is so convoluted. When you're working on an interpreter written in itself, it's hard to keep track of what's happening at what level, and errors can be practically encrypted by the time you get them.\n",
            "\n",
            "So I said no more essays till Bel was done. But I told few people about Bel while I was working on it. So for years it must have seemed that I was doing nothing, when in fact I was working harder than I'd ever worked on anything. Occasionally after wrestling for hours with some gruesome bug I'd check Twitter or HN and see someone asking \"Does Paul Graham still code?\"\n",
            "\n",
            "Working on Bel was hard but satisfying. I worked on it so intensively that at any given time I had a decent chunk of the code in my head and could write more there. I remember taking the boys to the coast on a sunny day in 2015 and figuring out how to deal with some problem involving continuations while I watched them play in the tide pools. It felt like I was doing life right. I remember that because I was slightly dismayed at how novel it felt. The good news is that I had more moments like this over the next few years.\n",
            "\n",
            "In the summer of 2016 we moved to England. We wanted our kids to see what it was like living in another\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Augment the Prompt"
      ],
      "metadata": {
        "id": "7uvQACqAkHg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Formulating the system prompt\n",
        "    system_prompt = (\n",
        "      \"You are an assistant and expert in answering questions from a chunks of content. \"\n",
        "      \"Only answer AI-related question, else say that you cannot answer this question.\"\n",
        "    )\n",
        "\n",
        "    # Combining the system prompt with the user's question\n",
        "    prompt = (\n",
        "      \"Read the following informations that might contain the context you require to answer the question. You can use the informations starting from the <START_OF_CONTEXT> tag and end with the <END_OF_CONTEXT> tag. Here is the content:\\n\\n<START_OF_CONTEXT>\\n{}\\n<END_OF_CONTEXT>\\n\\n\"\n",
        "      \"Please provide an informative and accurate answer to the following question based on the avaiable context. Be concise and take your time. \\nQuestion: {}\\nAnswer:\"\n",
        "    )\n",
        "    prompt = prompt.format( \"\".join( df.text[indices] ), QUESTION )\n",
        "\n",
        "    # Call the OpenAI API\n",
        "    response = client.chat.completions.create(\n",
        "      model='gpt-3.5-turbo-16k',\n",
        "      temperature=0.0,\n",
        "      messages=[\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "      ]\n",
        "    )\n",
        "\n",
        "    # Return the AI's response\n",
        "    res = response.choices[0].message.content.strip()\n",
        "\n",
        "except Exception as e:\n",
        "    print( f\"An error occurred: {e}\" )"
      ],
      "metadata": {
        "id": "MXRdzta5kJ3V"
      },
      "execution_count": 305,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print( res )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tBvJ8oMucha",
        "outputId": "2f4e9f03-a694-41d3-b3fe-7ac14a63b440"
      },
      "execution_count": 306,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paul Graham had a budget of $7 a day when he was in Florence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Without Augmentation"
      ],
      "metadata": {
        "id": "pW-BNCAC2JzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Formulating the system prompt\n",
        "system_prompt = (\n",
        "  \"You are an assistant and expert in answering questions.\"\n",
        ")\n",
        "\n",
        "# Combining the system prompt with the user's question\n",
        "prompt = (\n",
        "  \"Be concise and take your time to answer the following question. \\nQuestion: {}\\nAnswer:\"\n",
        ")\n",
        "prompt = prompt.format( QUESTION )\n",
        "\n",
        "# Call the OpenAI API\n",
        "response = client.chat.completions.create(\n",
        "  model='gpt-3.5-turbo-16k',\n",
        "  temperature=.9,\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "  ]\n",
        ")\n",
        "\n",
        "# Return the AI's response\n",
        "res = response.choices[0].message.content.strip()"
      ],
      "metadata": {
        "id": "RuyXjzZyuecE"
      },
      "execution_count": 307,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print( res )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAy34tPTzGbh",
        "outputId": "a6eb8efc-ce40-46fc-e307-b8e9772f9657"
      },
      "execution_count": 308,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There is no specific information available about Paul Graham's daily budget when he was in Florence.\n"
          ]
        }
      ]
    }
  ]
}