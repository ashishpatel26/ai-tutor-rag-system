{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/towardsai/ai-tutor-rag-system/blob/main/notebooks/Crawl_a_Website.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CW8ux1RSdem",
        "outputId": "83a191f1-615d-477c-f842-ddab8e6d55c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/7.4 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/7.4 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m6.6/7.4 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.0/337.0 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m679.1/679.1 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.8/176.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-index==0.10.57 llama-index-llms-openai openai==1.37.0 google-generativeai==0.5.4 newspaper3k==0.2.8 lxml_html_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wxDPsVXSAj6_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set the following API Keys in the Python environment. Will be used later.\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"[OPENAI_API_KEY]\"\n",
        "USESCRAPER_API_KEY = \"[USESCRAPER_API_KEY]\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSc7-1mljmrp"
      },
      "source": [
        "There are two primary methods for extracting webpage content. The first method involves having a list of URLs; one can iterate through this list to retrieve the content of each page. The second method, web crawling, requires using a script or service to extract page URLs from a sitemap or manually following links on the page to access all the content. Initially, we will explore web scraping techniques before discussing how to use a service like usescraper.com to perform web crawling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3r2tYHgeIK9"
      },
      "source": [
        "# 1. Scraping using `newspaper` Library\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it43ZQf8jatw"
      },
      "source": [
        "## Define URLs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "x74PqfQ7eIzD"
      },
      "outputs": [],
      "source": [
        "urls = [\n",
        "    \"https://docs.llamaindex.ai/en/stable/understanding\",\n",
        "    \"https://docs.llamaindex.ai/en/stable/understanding/using_llms/using_llms/\",\n",
        "    \"https://docs.llamaindex.ai/en/stable/understanding/indexing/indexing/\",\n",
        "    \"https://docs.llamaindex.ai/en/stable/understanding/querying/querying/\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgxfpfSsjcMC"
      },
      "source": [
        "## Get Page Contents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Q6Xs1OhUfVQV"
      },
      "outputs": [],
      "source": [
        "import newspaper\n",
        "\n",
        "pages_content = []\n",
        "\n",
        "# Retrieve the Content\n",
        "for url in urls:\n",
        "    try:\n",
        "        article = newspaper.Article(url)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        if len(article.text) > 0:\n",
        "            pages_content.append(\n",
        "                {\"url\": url, \"title\": article.title, \"text\": article.text}\n",
        "            )\n",
        "    except:\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cNdJNi2g1ly",
        "outputId": "99de4fdf-7cd5-4e36-c26b-7a9a07bc9d5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'url': 'https://docs.llamaindex.ai/en/stable/understanding',\n",
              " 'title': 'Building an LLM Application',\n",
              " 'text': \"Building an LLM application#\\n\\nWelcome to the beginning of Understanding LlamaIndex. This is a series of short, bite-sized tutorials on every stage of building an LLM application to get you acquainted with how to use LlamaIndex before diving into more advanced and subtle strategies. If you're an experienced programmer new to LlamaIndex, this is the place to start.\\n\\nKey steps in building an LLM application#\\n\\nTip If you've already read our high-level concepts page you'll recognize several of these steps.\\n\\nThis tutorial has three main parts: Building a RAG pipeline, Building an agent, and Building Workflows, with some smaller sections before and after. Here's what to expect:\\n\\nUsing LLMs : hit the ground running by getting started working with LLMs. We'll show you how to use any of our dozens of supported LLMs, whether via remote API calls or running locally on your machine.\\n\\nBuilding a RAG pipeline : Retrieval-Augmented Generation (RAG) is a key technique for getting your data into an LLM, and a component of more sophisticated agentic systems. We'll show you how to build a full-featured RAG pipeline that can answer questions about your data. This includes: Loading & Ingestion : Getting your data from wherever it lives, whether that's unstructured text, PDFs, databases, or APIs to other applications. LlamaIndex has hundreds of connectors to every data source over at LlamaHub. Indexing and Embedding : Once you've got your data there are an infinite number of ways to structure access to that data to ensure your applications is always working with the most relevant data. LlamaIndex has a huge number of these strategies built-in and can help you select the best ones. Storing : You will probably find it more efficient to store your data in indexed form, or pre-processed summaries provided by an LLM, often in a specialized database known as a Vector Store (see below). You can also store your indexes, metadata and more. Querying : Every indexing strategy has a corresponding querying strategy and there are lots of ways to improve the relevance, speed and accuracy of what you retrieve and what the LLM does with it before returning it to you, including turning it into structured responses such as an API.\\n\\nBuilding an agent : agents are LLM-powered knowledge workers that can interact with the world via a set of tools. Those tools can be RAG engines such as you learned how to build in the previous section, or any arbitrary code. This tutorial includes: Building a basic agent : We show you how to build a simple agent that can interact with the world via a set of tools. Using local models with agents : Agents can be built to use local models, which can be important for performance or privacy reasons. Adding RAG to an agent : The RAG pipelines you built in the previous tutorial can be used as a tool by an agent, giving your agent powerful information-retrieval capabilities. Adding other tools : Let's add more sophisticated tools to your agent, such as API integrations.\\n\\nBuilding Workflows : Workflows are a low-level, event-driven abstraction for building agentic applications. They're the base layer you should be using to build any custom, advanced RAG/agent system. You can use the pre-built abstractions you learned above, or build completely agentic applications from scratch. Get started here.\\n\\nPutting it all together : whether you are building question & answering, chatbots, an API, or an autonomous agent, we show you how to get your application into production.\\n\\nTracing and debugging : also called observability , it's especially important with LLM applications to be able to look into the inner workings of what's going on to help you debug problems and spot places to improve.\\n\\nEvaluating: every strategy has pros and cons and a key part of building, shipping and evolving your application is evaluating whether your change has improved your application in terms of accuracy, performance, clarity, cost and more. Reliably evaluating your changes is a crucial part of LLM application development.\\n\\nReady to dive in? Head to using LLMs.\"}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pages_content[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WleP60A3gkQM",
        "outputId": "c138349f-db29-46a9-e427-a12359593eb0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(pages_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5mCiRfGjfNx"
      },
      "source": [
        "## Convert to Document\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TOJ3K-CBfVDR"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.schema import Document\n",
        "\n",
        "# Convert the chunks to Document objects so the LlamaIndex framework can process them.\n",
        "documents = [\n",
        "    Document(text=row[\"text\"], metadata={\"title\": row[\"title\"], \"url\": row[\"url\"]})\n",
        "    for row in pages_content\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkjEyEmkJevT"
      },
      "source": [
        "# 2. Submit the Crawler Job\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYpchBo5-brp",
        "outputId": "ffe2a1bc-1926-4831-ecf8-57fc878c33a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'org': '2492', 'id': '7YDMC0KP6JPFYG0XH5SDADW0WV', 'urls': ['https://docs.llamaindex.ai/en/stable/understanding/'], 'exclude_globs': [], 'exclude_elements': 'nav, header, footer, script, style, noscript, svg, [role=\"alert\"], [role=\"banner\"], [role=\"dialog\"], [role=\"alertdialog\"], [role=\"region\"][aria-label*=\"skip\" i], [aria-modal=\"true\"]', 'output_format': 'markdown', 'output_expiry': 604800, 'min_length': 50, 'page_limit': 10000, 'force_crawling_mode': 'link', 'block_resources': True, 'include_linked_files': False, 'createdAt': 1730468521773, 'status': 'starting', 'use_browser': True, 'sitemapPageCount': 0, 'notices': []}\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "urls_to_crawl = [\n",
        "    \"https://docs.llamaindex.ai/en/stable/understanding/\", # add your URLs here, e.g. \"https://docs.llamaindex.ai/en/stable/understanding/\"\n",
        "]\n",
        "\n",
        "payload = {\n",
        "    \"urls\": urls_to_crawl,  # list of urls to crawl\n",
        "    \"output_format\": \"markdown\",  # text, html, markdown\n",
        "    \"output_expiry\": 604800,  # Automatically delete after X seconds\n",
        "    \"min_length\": 50,  # Skip pages with less than X characters\n",
        "    \"page_limit\": 3,  # Maximum number of pages to crawl\n",
        "    \"force_crawling_mode\": \"link\",  # \"link\" follows links in the page reccursively, or \"sitemap\" to find pages from website's sitemap\n",
        "    \"block_resources\": True,  # skip loading images, stylesheets, or scripts\n",
        "    \"include_linked_files\": False,  # include files (PDF, text, ...) in output\n",
        "}\n",
        "headers = {\n",
        "    \"Authorization\": \"Bearer \" + USESCRAPER_API_KEY,\n",
        "    \"Content-Type\": \"application/json\",\n",
        "}\n",
        "\n",
        "response = requests.request(\n",
        "    \"POST\", \"https://api.usescraper.com/crawler/jobs\", json=payload, headers=headers\n",
        ")\n",
        "\n",
        "response = json.loads(response.text)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx_4MjHxJgxh"
      },
      "source": [
        "## Get the Status\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLJ0BUR8c1a8",
        "outputId": "25b5c576-588a-4401-9dc4-528b29d89505"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running\n",
            "{'scraped': 9, 'discarded': 0, 'failed': 1}\n"
          ]
        }
      ],
      "source": [
        "url = \"https://api.usescraper.com/crawler/jobs/{}\".format(response[\"id\"])\n",
        "\n",
        "status_res = requests.request(\"GET\", url, headers=headers)\n",
        "\n",
        "status_res = json.loads(status_res.text)\n",
        "\n",
        "print(status_res[\"status\"])\n",
        "print(status_res[\"progress\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHcRJIDsJh2i"
      },
      "source": [
        "## Get the Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4dUn4cmGGab",
        "outputId": "57af9f22-67dc-4a2d-b56e-bc48a670a914"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'data': [{'meta': {'url': 'https://docs.llamaindex.ai/en/stable/understanding/evaluating/cost_analysis/usage_pattern/', 'fetchedUrl': 'https://docs.llamaindex.ai/en/stable/understanding/evaluating/cost_analysis/usage_pattern/', 'fetchedUrlStatusCode': 200, 'meta': {'title': 'Usage Pattern - LlamaIndex'}}, 'text': ' \\n[ Skip to content ](https://docs.llamaindex.ai/en/stable/understanding/evaluating/cost_analysis/usage_pattern/#usage-pattern)\\n#Usage Pattern[#](https://docs.llamaindex.ai/en/stable/understanding/evaluating/cost_analysis/usage_pattern/#usage-pattern)\\n##Estimating LLM and Embedding Token Counts[#](https://docs.llamaindex.ai/en/stable/understanding/evaluating/cost_analysis/usage_pattern/#estimating-llm-and-embedding-token-counts)\\nIn order to measure LLM and Embedding token counts, you\\'ll need to\\n\\n- Setup MockLLM and MockEmbedding objects\\nfrom llama_index.core.llms import MockLLM\\nfrom llama_index.core import MockEmbedding\\n\\nllm = MockLLM(max_tokens=256)\\nembed_model = MockEmbedding(embed_dim=1536)\\n- Setup the TokenCountingCallback handler\\nimport tiktoken\\nfrom llama_index.core.callbacks import CallbackManager, TokenCountingHandler\\n\\ntoken_counter = TokenCountingHandler(\\n    tokenizer=tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\\n)\\n\\ncallback_manager = CallbackManager([token_counter])\\n- Add them to the global Settings\\nfrom llama_index.core import Settings\\n\\nSettings.llm = llm\\nSettings.embed_model = embed_model\\nSettings.callback_manager = callback_manager\\n- Construct an Index\\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\\n\\ndocuments = SimpleDirectoryReader(\\n    \"./docs/examples/data/paul_graham\"\\n).load_data()\\n\\nindex = VectorStoreIndex.from_documents(documents)\\n- Measure the counts!\\nprint(\\n    \"Embedding Tokens: \",\\n    token_counter.total_embedding_token_count,\\n    \"\\\\n\",\\n    \"LLM Prompt Tokens: \",\\n    token_counter.prompt_llm_token_count,\\n    \"\\\\n\",\\n    \"LLM Completion Tokens: \",\\n    token_counter.completion_llm_token_count,\\n    \"\\\\n\",\\n    \"Total LLM Token Count: \",\\n    token_counter.total_llm_token_count,\\n    \"\\\\n\",\\n)\\n\\n# reset counts\\ntoken_counter.reset_counts()\\n- Run a query, measure again\\nquery_engine = index.as_query_engine()\\n\\nresponse = query_engine.query(\"query\")\\n\\nprint(\\n    \"Embedding Tokens: \",\\n    token_counter.total_embedding_token_count,\\n    \"\\\\n\",\\n    \"LLM Prompt Tokens: \",\\n    token_counter.prompt_llm_token_count,\\n    \"\\\\n\",\\n    \"LLM Completion Tokens: \",\\n    token_counter.completion_llm_token_count,\\n    \"\\\\n\",\\n    \"Total LLM Token Count: \",\\n    token_counter.total_llm_token_count,\\n    \"\\\\n\",\\n)\\n\\n🦙'}, {'meta': {'url': 'https://docs.llamaindex.ai/en/stable/understanding/agent/llamaparse/', 'fetchedUrl': 'https://docs.llamaindex.ai/en/stable/understanding/agent/llamaparse/', 'fetchedUrlStatusCode': 200, 'meta': {'title': 'Enhancing with LlamaParse - LlamaIndex'}}, 'text': ' \\n[ Skip to content ](https://docs.llamaindex.ai/en/stable/understanding/agent/llamaparse/#enhancing-with-llamaparse)\\n#Enhancing with LlamaParse[#](https://docs.llamaindex.ai/en/stable/understanding/agent/llamaparse/#enhancing-with-llamaparse)\\nIn the previous example we asked a very basic question of our document, about the total amount of the budget. Let\\'s instead ask a more complicated question about a specific fact in the document:\\n\\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\\nindex = VectorStoreIndex.from_documents(documents)\\nquery_engine = index.as_query_engine()\\n\\nresponse = query_engine.query(\\n    \"How much exactly was allocated to a tax credit to promote investment in green technologies in the 2023 Canadian federal budget?\"\\n)\\nprint(response)\\nWe unfortunately get an unhelpful answer:\\n\\nThe budget allocated funds to a new green investments tax credit, but the exact amount was not specified in the provided context information.\\nThis is bad, because we happen to know the exact number is in the document! But the PDF is complicated, with tables and multi-column layout, and the LLM is missing the answer. Luckily, we can use LlamaParse to help us out.\\n\\nFirst, you need a LlamaCloud API key. You can [get one for free](https://cloud.llamaindex.ai/) by signing up for LlamaCloud. Then put it in your .env file just like your OpenAI key:\\n\\nLLAMA_CLOUD_API_KEY=llx-xxxxx\\nNow you\\'re ready to use LlamaParse in your code. Let\\'s bring it in as as import:\\n\\nfrom llama_parse import LlamaParse\\nAnd let\\'s put in a second attempt to parse and query the file (note that this uses documents2, index2, etc.) and see if we get a better answer to the exact same question:\\n\\ndocuments2 = LlamaParse(result_type=\"markdown\").load_data(\\n    \"./data/2023_canadian_budget.pdf\"\\n)\\nindex2 = VectorStoreIndex.from_documents(documents2)\\nquery_engine2 = index2.as_query_engine()\\n\\nresponse2 = query_engine2.query(\\n    \"How much exactly was allocated to a tax credit to promote investment in green technologies in the 2023 Canadian federal budget?\"\\n)\\nprint(response2)\\nWe do!\\n\\n$20 billion was allocated to a tax credit to promote investment in green technologies in the 2023 Canadian federal budget.\\nYou can always check [the repo](https://github.com/run-llama/python-agents-tutorial/blob/main/4_llamaparse.py) to what this code looks like.\\n\\nAs you can see, parsing quality makes a big difference to what the LLM can understand, even for relatively simple questions. Next let\\'s see how [memory](https://docs.llamaindex.ai/en/stable/understanding/agent/memory/) can help us with more complex questions.\\n\\n\\nHi, how can I help you?\\n\\n🦙'}, {'meta': {'url': 'https://docs.llamaindex.ai/en/stable/understanding/agent/memory/', 'fetchedUrl': 'https://docs.llamaindex.ai/en/stable/understanding/agent/memory/', 'fetchedUrlStatusCode': 200, 'meta': {'title': 'Memory - LlamaIndex'}}, 'text': ' \\n[ Skip to content ](https://docs.llamaindex.ai/en/stable/understanding/agent/memory/#memory)\\n#Memory[#](https://docs.llamaindex.ai/en/stable/understanding/agent/memory/#memory)\\nWe\\'ve now made several additions and subtractions to our code. To make it clear what we\\'re using, you can see [the current code for our agent](https://github.com/run-llama/python-agents-tutorial/blob/main/5_memory.py) in the repo. It\\'s using OpenAI for the LLM and LlamaParse to enhance parsing.\\n\\nWe\\'ve also added 3 questions in a row. Let\\'s see how the agent handles them:\\n\\nresponse = agent.chat(\\n    \"How much exactly was allocated to a tax credit to promote investment in green technologies in the 2023 Canadian federal budget?\"\\n)\\n\\nprint(response)\\n\\nresponse = agent.chat(\\n    \"How much was allocated to a implement a means-tested dental care program in the 2023 Canadian federal budget?\"\\n)\\n\\nprint(response)\\n\\nresponse = agent.chat(\\n    \"How much was the total of those two allocations added together? Use a tool to answer any questions.\"\\n)\\n\\nprint(response)\\nThis is demonstrating a powerful feature of agents in LlamaIndex: memory. Let\\'s see what the output looks like:\\n\\nStarted parsing the file under job_id cac11eca-45e0-4ea9-968a-25f1ac9b8f99\\nThought: The current language of the user is English. I need to use a tool to help me answer the question.\\nAction: canadian_budget_2023\\nAction Input: {\\'input\\': \\'How much was allocated to a tax credit to promote investment in green technologies in the 2023 Canadian federal budget?\\'}\\nObservation: $20 billion was allocated to a tax credit to promote investment in green technologies in the 2023 Canadian federal budget.\\nThought: I can answer without using any more tools. I\\'ll use the user\\'s language to answer\\nAnswer: $20 billion was allocated to a tax credit to promote investment in green technologies in the 2023 Canadian federal budget.\\n$20 billion was allocated to a tax credit to promote investment in green technologies in the 2023 Canadian federal budget.\\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\\nAction: canadian_budget_2023\\nAction Input: {\\'input\\': \\'How much was allocated to implement a means-tested dental care program in the 2023 Canadian federal budget?\\'}\\nObservation: $13 billion was allocated to implement a means-tested dental care program in the 2023 Canadian federal budget.\\nThought: I can answer without using any more tools. I\\'ll use the user\\'s language to answer\\nAnswer: $13 billion was allocated to implement a means-tested dental care program in the 2023 Canadian federal budget.\\n$13 billion was allocated to implement a means-tested dental care program in the 2023 Canadian federal budget.\\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\\nAction: add\\nAction Input: {\\'a\\': 20, \\'b\\': 13}\\nObservation: 33\\nThought: I can answer without using any more tools. I\\'ll use the user\\'s language to answer\\nAnswer: The total of the allocations for the tax credit to promote investment in green technologies and the means-tested dental care program in the 2023 Canadian federal budget is $33 billion.\\nThe total of the allocations for the tax credit to promote investment in green technologies and the means-tested dental care program in the 2023 Canadian federal budget is $33 billion.\\nThe agent remembers that it already has the budget allocations from previous questions, and can answer a contextual question like \"add those two allocations together\" without needing to specify which allocations exactly. It even correctly uses the other addition tool to sum up the numbers.\\n\\nHaving demonstrated how memory helps, let\\'s [add some more complex tools](https://docs.llamaindex.ai/en/stable/understanding/agent/tools/) to our agent.\\n\\n\\n🦙'}, {'meta': {'url': 'https://docs.llamaindex.ai/en/stable/understanding/querying/querying/', 'fetchedUrl': 'https://docs.llamaindex.ai/en/stable/understanding/querying/querying/', 'fetchedUrlStatusCode': 200, 'meta': {'title': 'Querying - LlamaIndex'}}, 'text': ' \\n[ Skip to content ](https://docs.llamaindex.ai/en/stable/understanding/querying/querying/#querying)\\n#Querying[#](https://docs.llamaindex.ai/en/stable/understanding/querying/querying/#querying)\\nNow you\\'ve loaded your data, built an index, and stored that index for later, you\\'re ready to get to the most significant part of an LLM application: querying.\\n\\nAt its simplest, querying is just a prompt call to an LLM: it can be a question and get an answer, or a request for summarization, or a much more complex instruction.\\n\\nMore complex querying could involve repeated/chained prompt + LLM calls, or even a reasoning loop across multiple components.\\n\\n##Getting started[#](https://docs.llamaindex.ai/en/stable/understanding/querying/querying/#getting-started)\\nThe basis of all querying is the QueryEngine. The simplest way to get a QueryEngine is to get your index to create one for you, like this:\\n\\nquery_engine = index.as_query_engine()\\nresponse = query_engine.query(\\n    \"Write an email to the user given their background information.\"\\n)\\nprint(response)\\n##Stages of querying[#](https://docs.llamaindex.ai/en/stable/understanding/querying/querying/#stages-of-querying)\\nHowever, there is more to querying than initially meets the eye. Querying consists of three distinct stages:\\n\\n- Retrieval is when you find and return the most relevant documents for your query from your Index. As previously discussed in [indexing](https://docs.llamaindex.ai/en/stable/understanding/indexing/indexing/), the most common type of retrieval is \"top-k\" semantic retrieval, but there are many other retrieval strategies.\\n- Postprocessing is when the Nodes retrieved are optionally reranked, transformed, or filtered, for instance by requiring that they have specific metadata such as keywords attached.\\n- Response synthesis is when your query, your most-relevant data and your prompt are combined and sent to your LLM to return a response.\\nTip\\n\\nYou can find out about [how to attach metadata to documents](https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_documents/) and [nodes](https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_nodes/).\\n\\n##Customizing the stages of querying[#](https://docs.llamaindex.ai/en/stable/understanding/querying/querying/#customizing-the-stages-of-querying)\\nLlamaIndex features a low-level composition API that gives you granular control over your querying.\\n\\nIn this example, we customize our retriever to use a different number for top_k and add a post-processing step that requires that the retrieved nodes reach a minimum similarity score to be included. This would give you a lot of data when you have relevant results but potentially no data if you have nothing relevant.\\n\\nfrom llama_index.core import VectorStoreIndex, get_response_synthesizer\\nfrom llama_index.core.retrievers import VectorIndexRetriever\\nfrom llama_index.core.query_engine import RetrieverQueryEngine\\nfrom llama_index.core.postprocessor import SimilarityPostprocessor\\n\\n# build index\\nindex = VectorStoreIndex.from_documents(documents)\\n\\n# configure retriever\\nretriever = VectorIndexRetriever(\\n    index=index,\\n    similarity_top_k=10,\\n)\\n\\n# configure response synthesizer\\nresponse_synthesizer = get_response_synthesizer()\\n\\n# assemble query engine\\nquery_engine = RetrieverQueryEngine(\\n    retriever=retriever,\\n    response_synthesizer=response_synthesizer,\\n    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\\n)\\n\\n# query\\nresponse = query_engine.query(\"What did the author do growing up?\")\\nprint(response)\\nYou can also add your own retrieval, response synthesis, and overall query logic, by implementing the corresponding interfaces.\\n\\nFor a full list of implemented components and the supported configurations, check out our [reference docs](https://docs.llamaindex.ai/en/stable/api_reference/).\\n\\nLet\\'s go into more detail about customizing each step:\\n\\n###Configuring retriever[#](https://docs.llamaindex.ai/en/stable/understanding/querying/querying/#configuring-retriever)\\nretriever = VectorIndexRetriever(\\n    index=index,\\n    similarity_top_k=10,\\n)\\nThere are a huge variety of retrievers that you can learn about in our [module guide on retrievers](https://docs.llamaindex.ai/en/stable/module_guides/querying/retriever/).\\n\\n###Configuring node postprocessors[#](https://docs.llamaindex.ai/en/stable/understanding/querying/querying/#configuring-node-postprocessors)\\nWe support advanced Node filtering and augmentation that can further improve the relevancy of the retrieved Node objects. This can help reduce the time/number of LLM calls/cost or improve response quality.\\n\\nFor example:\\n\\n- KeywordNodePostprocessor: filters nodes by required_keywords and exclude_keywords.\\n- SimilarityPostprocessor: filters nodes by setting a threshold on the similarity score (thus only supported by embedding-based retrievers)\\n- PrevNextNodePostprocessor: augments retrieved Node objects with additional relevant context based on Node relationships.\\nThe full list of node postprocessors is documented in the [Node Postprocessor Reference](https://docs.llamaindex.ai/en/stable/api_reference/postprocessor/).\\n\\nTo configure the desired node postprocessors:\\n\\nnode_postprocessors = [\\n    KeywordNodePostprocessor(\\n        required_keywords=[\"Combinator\"], exclude_keywords=[\"Italy\"]\\n    )\\n]\\nquery_engine = RetrieverQueryEngine.from_args(\\n    retriever, node_postprocessors=node_postprocessors\\n)\\nresponse = query_engine.query(\"What did the author do growing up?\")\\n###Configuring response synthesis[#](https://docs.llamaindex.ai/en/stable/understanding/querying/querying/#configuring-response-synthesis)\\nAfter a retriever fetches relevant nodes, a BaseSynthesizer synthesizes the final response by combining the information.\\n\\nYou can configure it via\\n\\nquery_engine = RetrieverQueryEngine.from_args(\\n    retriever, response_mode=response_mode\\n)\\nRight now, we support the following options:\\n\\n- default: \"create and refine\" an answer by sequentially going through each retrieved Node; This makes a separate LLM call per Node. Good for more detailed answers.\\n- compact: \"compact\" the prompt during each LLM call by stuffing as many Node text chunks that can fit within the maximum prompt size. If there are too many chunks to stuff in one prompt, \"create and refine\" an answer by going through multiple prompts.\\n- tree_summarize: Given a set of Node objects and the query, recursively construct a tree and return the root node as the response. Good for summarization purposes.\\n- no_text: Only runs the retriever to fetch the nodes that would have been sent to the LLM, without actually sending them. Then can be inspected by checking response.source_nodes. The response object is covered in more detail in Section 5.\\n- accumulate: Given a set of Node objects and the query, apply the query to each Node text chunk while accumulating the responses into an array. Returns a concatenated string of all responses. Good for when you need to run the same query separately against each text chunk.\\n##Structured Outputs[#](https://docs.llamaindex.ai/en/stable/understanding/querying/querying/#structured-outputs)\\nYou may want to ensure your output is structured. See our [Query Engines + Pydantic Outputs](https://docs.llamaindex.ai/en/stable/module_guides/querying/structured_outputs/query_engine/) to see how to extract a Pydantic object from a query engine class.\\n\\nAlso make sure to check out our entire [Structured Outputs](https://docs.llamaindex.ai/en/stable/module_guides/querying/structured_outputs/) guide.\\n\\n##Creating your own Query Workflow[#](https://docs.llamaindex.ai/en/stable/understanding/querying/querying/#creating-your-own-query-workflow)\\nIf you want to design complex query flows, you can compose your own query workflow across many different modules, from prompts/LLMs/output parsers to retrievers to response synthesizers to your own custom components.\\n\\nTake a look at our [Workflow Guide](https://docs.llamaindex.ai/en/stable/module_guides/workflow/) for more details.\\n\\n\\n🦙'}, {'meta': {'url': 'https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/agents/', 'fetchedUrl': 'https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/agents/', 'fetchedUrlStatusCode': 200, 'meta': {'title': 'Agents - LlamaIndex'}}, 'text': ' \\n[ Skip to content ](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/agents/#agents)\\n#Agents[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/agents/#agents)\\nPutting together an agent in LlamaIndex can be done by defining a set of tools and providing them to our ReActAgent implementation. We\\'re using it here with OpenAI, but it can be used with any sufficiently capable LLM:\\n\\nfrom llama_index.core.tools import FunctionTool\\nfrom llama_index.llms.openai import OpenAI\\nfrom llama_index.core.agent import ReActAgent\\n\\n\\n# define sample Tool\\ndef multiply(a: int, b: int) -> int:\\n    \"\"\"Multiply two integers and returns the result integer\"\"\"\\n    return a * b\\n\\n\\nmultiply_tool = FunctionTool.from_defaults(fn=multiply)\\n\\n# initialize llm\\nllm = OpenAI(model=\"gpt-3.5-turbo-0613\")\\n\\n# initialize ReAct agent\\nagent = ReActAgent.from_tools([multiply_tool], llm=llm, verbose=True)\\nThese tools can be Python functions as shown above, or they can be LlamaIndex query engines:\\n\\nfrom llama_index.core.tools import QueryEngineTool\\n\\nquery_engine_tools = [\\n    QueryEngineTool(\\n        query_engine=sql_agent,\\n        metadata=ToolMetadata(\\n            name=\"sql_agent\", description=\"Agent that can execute SQL queries.\"\\n        ),\\n    ),\\n]\\n\\nagent = ReActAgent.from_tools(query_engine_tools, llm=llm, verbose=True)\\nYou can learn more in our [Agent Module Guide](https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/).\\n\\n##Native OpenAIAgent[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/agents/#native-openaiagent)\\nWe have an OpenAIAgent implementation built on the [OpenAI API for function calling](https://openai.com/blog/function-calling-and-other-api-updates) that allows you to rapidly build agents:\\n\\n- [OpenAIAgent](https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent/)\\n- [OpenAIAgent with Query Engine Tools](https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_with_query_engine/)\\n- [OpenAIAgent Query Planning](https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_query_plan/)\\n- [OpenAI Assistant](https://docs.llamaindex.ai/en/stable/examples/agent/openai_assistant_agent/)\\n- [OpenAI Assistant Cookbook](https://docs.llamaindex.ai/en/stable/examples/agent/openai_assistant_query_cookbook/)\\n- [Forced Function Calling](https://docs.llamaindex.ai/en/stable/examples/agent/openai_forced_function_call/)\\n- [Parallel Function Calling](https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_parallel_function_calling/)\\n- [Context Retrieval](https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent_context_retrieval/)\\n##Agentic Components within LlamaIndex[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/agents/#agentic-components-within-llamaindex)\\nLlamaIndex provides core modules capable of automated reasoning for different use cases over your data which makes them essentially Agents. Some of these core modules are shown below along with example tutorials.\\n\\nSubQuestionQueryEngine for Multi Document Analysis\\n\\n- [Sub Question Query Engine (Intro)](https://docs.llamaindex.ai/en/stable/examples/query_engine/sub_question_query_engine/)\\n- [10Q Analysis (Uber)](https://docs.llamaindex.ai/en/stable/examples/usecases/10q_sub_question/)\\n- [10K Analysis (Uber and Lyft)](https://docs.llamaindex.ai/en/stable/examples/usecases/10k_sub_question/)\\nQuery Transformations\\n\\n- [How-To](https://docs.llamaindex.ai/en/stable/optimizing/advanced_retrieval/query_transformations/)\\n- [Multi-Step Query Decomposition](https://docs.llamaindex.ai/en/stable/examples/query_transformations/HyDEQueryTransformDemo/) ([Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/docs/examples/query_transformations/HyDEQueryTransformDemo.ipynb))\\nRouting\\n\\n- [Usage](https://docs.llamaindex.ai/en/stable/module_guides/querying/router/)\\n- [Router Query Engine Guide](https://docs.llamaindex.ai/en/stable/examples/query_engine/RouterQueryEngine/) ([Notebook](https://github.com/jerryjliu/llama_index/blob/main/docs../../examples/query_engine/RouterQueryEngine.ipynb))\\nLLM Reranking\\n\\n- [Second Stage Processing How-To](https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/)\\n- [LLM Reranking Guide (Great Gatsby)](https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/LLMReranker-Gatsby/)\\nChat Engines\\n\\n- [Chat Engines How-To](https://docs.llamaindex.ai/en/stable/module_guides/deploying/chat_engines/)\\n##Using LlamaIndex as as Tool within an Agent Framework[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/agents/#using-llamaindex-as-as-tool-within-an-agent-framework)\\nLlamaIndex can be used as as Tool within an agent framework - including LangChain, ChatGPT. These integrations are described below.\\n\\n###LangChain[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/agents/#langchain)\\nWe have deep integrations with LangChain. LlamaIndex query engines can be easily packaged as Tools to be used within a LangChain agent, and LlamaIndex can also be used as a memory module / retriever. Check out our guides/tutorials below!\\n\\nResources\\n\\n- [Building a Chatbot Tutorial](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/chatbots/building_a_chatbot/)\\n- [OnDemandLoaderTool Tutorial](https://docs.llamaindex.ai/en/stable/examples/tools/OnDemandLoaderTool/)\\n###ChatGPT[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/agents/#chatgpt)\\nLlamaIndex can be used as a ChatGPT retrieval plugin (we have a TODO to develop a more general plugin as well).\\n\\nResources\\n\\n- [LlamaIndex ChatGPT Retrieval Plugin](https://github.com/openai/chatgpt-retrieval-plugin#llamaindex)\\n\\nHi, how can I help you?\\n\\n🦙'}, {'meta': {'url': 'https://docs.llamaindex.ai/en/stable/understanding/workflows/unbound_functions/', 'fetchedUrl': 'https://docs.llamaindex.ai/en/stable/understanding/workflows/unbound_functions/', 'fetchedUrlStatusCode': 200, 'meta': {'title': 'Unbound syntax - LlamaIndex'}}, 'text': ' \\n[ Skip to content ](https://docs.llamaindex.ai/en/stable/understanding/workflows/unbound_functions/#workflows-from-unbound-functions)\\n#Workflows from unbound functions[#](https://docs.llamaindex.ai/en/stable/understanding/workflows/unbound_functions/#workflows-from-unbound-functions)\\nThroughout this tutorial we have been showing workflows defined as classes. However, this is not the only way to define a workflow: you can also define the steps in your workflow through independent or \"unbound\" functions and assign them to a workflow using the @step() decorator. Let\\'s see how that works.\\n\\nFirst we create an empty class to hold the steps:\\n\\nclass TestWorkflow(Workflow):\\n    pass\\nNow we can add steps to the workflow by defining functions and decorating them with the @step() decorator:\\n\\n@step(workflow=TestWorkflow)\\ndef some_step(ev: StartEvent) -> StopEvent:\\n    return StopEvent()\\nIn this example, we\\'re adding a starting step to the TestWorkflow class. The @step() decorator takes the workflow argument, which is the class to which the step will be added. The function signature is the same as for a regular step, with the exception of the workflow argument.\\n\\nYou can also add steps this way to any existing workflow class! This can be handy if you just need one extra step in your workflow and don\\'t want to subclass an entire workflow to do it.\\n\\n##That\\'s it![#](https://docs.llamaindex.ai/en/stable/understanding/workflows/unbound_functions/#thats-it)\\nCongratulations, you\\'ve completed the workflows tutorial!\\n\\n\\nHi, how can I help you?\\n\\n🦙'}, {'meta': {'url': 'https://docs.llamaindex.ai/en/stable/understanding/workflows/', 'fetchedUrl': 'https://docs.llamaindex.ai/en/stable/understanding/workflows/', 'fetchedUrlStatusCode': 200, 'meta': {'title': 'Introduction to workflows - LlamaIndex'}}, 'text': ' \\n[ Skip to content ](https://docs.llamaindex.ai/en/stable/understanding/workflows/#workflows-introduction)\\n#Workflows introduction[#](https://docs.llamaindex.ai/en/stable/understanding/workflows/#workflows-introduction)\\n##What is a workflow?[#](https://docs.llamaindex.ai/en/stable/understanding/workflows/#what-is-a-workflow)\\nA workflow is an event-driven, step-based way to control the execution flow of an application.\\n\\nYour application is divided into sections called Steps which are triggered by Events, and themselves emit Events which trigger further steps. By combining steps and events, you can create arbitrarily complex flows that encapsulate logic and make your application more maintainable and easier to understand. A step can be anything from a single line of code to a complex agent. They can have arbitrary inputs and outputs, which are passed around by Events.\\n\\n##An example[#](https://docs.llamaindex.ai/en/stable/understanding/workflows/#an-example)\\nIn this visualization, you can see a moderately complex workflow designed to take a query, optionally improve upon it, and then attempt to answer the query using three different RAG strategies. The LLM gets answers from all three strategies and judges which is the \"best\", and returns that. We can break this flow down:\\n\\n- It is triggered by a StartEvent\\n- A step called judge_query determines if the query is of high quality. If not, a BadQueryEvent is generated.\\n- A BadQueryEvent will trigger a step called improve_query which will attempt to improve the query, which will then trigger a JudgeEvent\\n- A JudgeEvent will trigger judge_query again, creating a loop which can continue until the query is judged of sufficient quality. This is called \"Reflection\" and is a key part of agentic applications that Workflows make easy to implement.\\n- If the query is of sufficient quality, 3 simultaneous events are generated: a NaiveRAGEvent, a HighTopKEvent, and a RerankEvent. These three events trigger 3 associated steps in parallel, which each run a different RAG strategy.\\n- Each of the query steps generates a ResponseEvent. A ResponseEvent triggers a step called judge_response which will wait until it has received all 3 responses.\\n- judge_response will then pick the \"best\" response and return it to the user via a StopEvent.\\n\\n\\n##Why workflows?[#](https://docs.llamaindex.ai/en/stable/understanding/workflows/#why-workflows)\\nAs generative AI applications become more complex, it becomes harder to manage the flow of data and control the execution of the application. Workflows provide a way to manage this complexity by breaking the application into smaller, more manageable pieces.\\n\\nOther frameworks and LlamaIndex itself have attempted to solve this problem previously with directed acyclic graphs (DAGs) but these have a number of limitations that workflows do not:\\n\\n- Logic like loops and branches needed to be encoded into the edges of graphs, which made them hard to read and understand.\\n- Passing data between nodes in a DAG created complexity around optional and default values and which parameters should be passed.\\n- DAGs did not feel natural to developers trying to developing complex, looping, branching AI applications.\\nThe event-based pattern and vanilla python approach of Workflows resolves these problems.\\n\\nFor simple RAG pipelines and linear demos we do not expect you will need Workflows, but as your application grows in complexity, we hope you will reach for them.\\n\\n##Next steps[#](https://docs.llamaindex.ai/en/stable/understanding/workflows/#next-steps)\\nLet\\'s build [a basic workflow](https://docs.llamaindex.ai/en/stable/understanding/workflows/basic_flow/). Follow the tutorial sequence step-by-step to learn the core concepts.\\n\\nOnce you\\'re done, check out our [Workflows component guide](https://docs.llamaindex.ai/en/stable/module_guides/workflow/) as a reference guide + more practical examples on building RAG/agents.\\n\\nIf you\\'re done building and want to deploy your workflow to production, check out [our llama_deploy guide](https://docs.llamaindex.ai/en/stable/module_guides/workflow/deployment.md) ([repo](https://github.com/run-llama/llama_deploy)).\\n\\n\\n🦙'}, {'meta': {'url': 'https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/', 'fetchedUrl': 'https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/', 'fetchedUrlStatusCode': 200, 'meta': {'title': 'Full-Stack Web Application - LlamaIndex'}}, 'text': ' \\n[ Skip to content ](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/#full-stack-web-application)\\n#Full-Stack Web Application[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/#full-stack-web-application)\\nLlamaIndex can be integrated into a downstream full-stack web application. It can be used in a backend server (such as Flask), packaged into a Docker container, and/or directly used in a framework such as Streamlit.\\n\\nWe provide tutorials and resources to help you get started in this area:\\n\\n- [Fullstack Application Guide](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_app_guide/) shows you how to build an app with LlamaIndex as an API and a TypeScript+React frontend\\n- [Fullstack Application with Delphic](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_with_delphic/) walks you through using LlamaIndex with a production-ready web app starter template called Delphic.\\n- The [LlamaIndex Starter Pack](https://github.com/logan-markewich/llama_index_starter_pack) provides very basic flask, streamlit, and docker examples for LlamaIndex.\\n\\n🦙'}, {'meta': {'url': 'https://docs.llamaindex.ai/en/stable/understanding/storing/storing/', 'fetchedUrl': 'https://docs.llamaindex.ai/en/stable/understanding/storing/storing/', 'fetchedUrlStatusCode': 200, 'meta': {'title': 'Storing - LlamaIndex'}}, 'text': ' \\n[ Skip to content ](https://docs.llamaindex.ai/en/stable/understanding/storing/storing/#storing)\\n#Storing[#](https://docs.llamaindex.ai/en/stable/understanding/storing/storing/#storing)\\nOnce you have data [loaded](https://docs.llamaindex.ai/en/stable/understanding/loading/loading/) and [indexed](https://docs.llamaindex.ai/en/stable/understanding/indexing/indexing/), you will probably want to store it to avoid the time and cost of re-indexing it. By default, your indexed data is stored only in memory.\\n\\n##Persisting to disk[#](https://docs.llamaindex.ai/en/stable/understanding/storing/storing/#persisting-to-disk)\\nThe simplest way to store your indexed data is to use the built-in .persist() method of every Index, which writes all the data to disk at the location specified. This works for any type of index.\\n\\nindex.storage_context.persist(persist_dir=\"<persist_dir>\")\\nHere is an example of a Composable Graph:\\n\\ngraph.root_index.storage_context.persist(persist_dir=\"<persist_dir>\")\\nYou can then avoid re-loading and re-indexing your data by loading the persisted index like this:\\n\\nfrom llama_index.core import StorageContext, load_index_from_storage\\n\\n# rebuild storage context\\nstorage_context = StorageContext.from_defaults(persist_dir=\"<persist_dir>\")\\n\\n# load index\\nindex = load_index_from_storage(storage_context)\\nTip\\n\\nImportant: if you had initialized your index with a custom transformations, embed_model, etc., you will need to pass in the same options during load_index_from_storage, or have it set as the [global settings](https://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/settings/).\\n\\n##Using Vector Stores[#](https://docs.llamaindex.ai/en/stable/understanding/storing/storing/#using-vector-stores)\\nAs discussed in [indexing](https://docs.llamaindex.ai/en/stable/understanding/indexing/indexing/), one of the most common types of Index is the VectorStoreIndex. The API calls to create the [embeddings](https://docs.llamaindex.ai/en/stable/understanding/indexing/indexing/#what-is-an-embedding) in a VectorStoreIndex can be expensive in terms of time and money, so you will want to store them to avoid having to constantly re-index things.\\n\\nLlamaIndex supports a [huge number of vector stores](https://docs.llamaindex.ai/en/stable/module_guides/storing/vector_stores/) which vary in architecture, complexity and cost. In this example we\\'ll be using Chroma, an open-source vector store.\\n\\nFirst you will need to install chroma:\\n\\npip install chromadb\\nTo use Chroma to store the embeddings from a VectorStoreIndex, you need to:\\n\\n- initialize the Chroma client\\n- create a Collection to store your data in Chroma\\n- assign Chroma as the vector_store in a StorageContext\\n- initialize your VectorStoreIndex using that StorageContext\\nHere\\'s what that looks like, with a sneak peek at actually querying the data:\\n\\nimport chromadb\\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\\nfrom llama_index.vector_stores.chroma import ChromaVectorStore\\nfrom llama_index.core import StorageContext\\n\\n# load some documents\\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\\n\\n# initialize client, setting path to save data\\ndb = chromadb.PersistentClient(path=\"./chroma_db\")\\n\\n# create collection\\nchroma_collection = db.get_or_create_collection(\"quickstart\")\\n\\n# assign chroma as the vector_store to the context\\nvector_store = ChromaVectorStore(chroma_collection=chroma_collection)\\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\\n\\n# create your index\\nindex = VectorStoreIndex.from_documents(\\n    documents, storage_context=storage_context\\n)\\n\\n# create a query engine and query\\nquery_engine = index.as_query_engine()\\nresponse = query_engine.query(\"What is the meaning of life?\")\\nprint(response)\\nIf you\\'ve already created and stored your embeddings, you\\'ll want to load them directly without loading your documents or creating a new VectorStoreIndex:\\n\\nimport chromadb\\nfrom llama_index.core import VectorStoreIndex\\nfrom llama_index.vector_stores.chroma import ChromaVectorStore\\nfrom llama_index.core import StorageContext\\n\\n# initialize client\\ndb = chromadb.PersistentClient(path=\"./chroma_db\")\\n\\n# get collection\\nchroma_collection = db.get_or_create_collection(\"quickstart\")\\n\\n# assign chroma as the vector_store to the context\\nvector_store = ChromaVectorStore(chroma_collection=chroma_collection)\\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\\n\\n# load your index from stored vectors\\nindex = VectorStoreIndex.from_vector_store(\\n    vector_store, storage_context=storage_context\\n)\\n\\n# create a query engine\\nquery_engine = index.as_query_engine()\\nresponse = query_engine.query(\"What is llama2?\")\\nprint(response)\\nTip\\n\\nWe have a [more thorough example of using Chroma](https://docs.llamaindex.ai/en/stable/examples/vector_stores/ChromaIndexDemo/) if you want to go deeper on this store.\\n\\n###You\\'re ready to query![#](https://docs.llamaindex.ai/en/stable/understanding/storing/storing/#youre-ready-to-query)\\nNow you have loaded data, indexed it, and stored that index, you\\'re ready to [query your data](https://docs.llamaindex.ai/en/stable/understanding/querying/querying/).\\n\\n##Inserting Documents or Nodes[#](https://docs.llamaindex.ai/en/stable/understanding/storing/storing/#inserting-documents-or-nodes)\\nIf you\\'ve already created an index, you can add new documents to your index using the insert method.\\n\\nfrom llama_index.core import VectorStoreIndex\\n\\nindex = VectorStoreIndex([])\\nfor doc in documents:\\n    index.insert(doc)\\nSee the [document management how-to](https://docs.llamaindex.ai/en/stable/module_guides/indexing/document_management/) for more details on managing documents and an example notebook.\\n\\n\\n🦙'}, {'meta': {'url': 'https://docs.llamaindex.ai/en/stable/understanding/tracing_and_debugging/tracing_and_debugging/', 'fetchedUrl': 'https://docs.llamaindex.ai/en/stable/understanding/tracing_and_debugging/tracing_and_debugging/', 'fetchedUrlStatusCode': 200, 'meta': {'title': 'Tracing and Debugging - LlamaIndex'}}, 'text': ' \\n[ Skip to content ](https://docs.llamaindex.ai/en/stable/understanding/tracing_and_debugging/tracing_and_debugging/#tracing-and-debugging)\\n#Tracing and Debugging[#](https://docs.llamaindex.ai/en/stable/understanding/tracing_and_debugging/tracing_and_debugging/#tracing-and-debugging)\\nDebugging and tracing the operation of your application is key to understanding and optimizing it. LlamaIndex provides a variety of ways to do this.\\n\\n##Basic logging[#](https://docs.llamaindex.ai/en/stable/understanding/tracing_and_debugging/tracing_and_debugging/#basic-logging)\\nThe simplest possible way to look into what your application is doing is to turn on debug logging. That can be done anywhere in your application like this:\\n\\nimport logging\\nimport sys\\n\\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\\n##Callback handler[#](https://docs.llamaindex.ai/en/stable/understanding/tracing_and_debugging/tracing_and_debugging/#callback-handler)\\nLlamaIndex provides callbacks to help debug, track, and trace the inner workings of the library. Using the callback manager, as many callbacks as needed can be added.\\n\\nIn addition to logging data related to events, you can also track the duration and number of occurrences of each event.\\n\\nFurthermore, a trace map of events is also recorded, and callbacks can use this data however they want. For example, the LlamaDebugHandler will, by default, print the trace of events after most operations.\\n\\nYou can get a simple callback handler like this:\\n\\nimport llama_index.core\\n\\nllama_index.core.set_global_handler(\"simple\")\\nYou can also learn how to [build you own custom callback handler](https://docs.llamaindex.ai/en/stable/module_guides/observability/callbacks/).\\n\\n##Observability[#](https://docs.llamaindex.ai/en/stable/understanding/tracing_and_debugging/tracing_and_debugging/#observability)\\nLlamaIndex provides one-click observability to allow you to build principled LLM applications in a production setting.\\n\\nThis feature allows you to seamlessly integrate the LlamaIndex library with powerful observability/evaluation tools offered by our partners. Configure a variable once, and you\\'ll be able to do things like the following:\\n\\n- View LLM/prompt inputs/outputs\\n- Ensure that the outputs of any component (LLMs, embeddings) are performing as expected\\n- View call traces for both indexing and querying\\nTo learn more, check out our [observability docs](https://docs.llamaindex.ai/en/stable/module_guides/observability/)\\n\\n\\nHi, how can I help you?\\n\\n🦙'}, {'meta': {'url': 'https://docs.llamaindex.ai/en/stable/understanding/loading/llamacloud/', 'fetchedUrl': 'https://docs.llamaindex.ai/en/stable/understanding/loading/llamacloud/', 'fetchedUrlStatusCode': 200, 'meta': {'title': 'Loading from LlamaCloud - LlamaIndex'}}, 'text': ' \\n[ Skip to content ](https://docs.llamaindex.ai/en/stable/understanding/loading/llamacloud/#loading-from-llamacloud)\\n#Loading from LlamaCloud[#](https://docs.llamaindex.ai/en/stable/understanding/loading/llamacloud/#loading-from-llamacloud)\\nOur enterprise service, [LlamaCloud](https://cloud.llamaindex.ai/), allows you to store and query your data in a fully-managed, scalable, and secure environment. For a full explanation of how to use LlamaCloud, see the [LlamaCloud documentation](https://docs.cloud.llamaindex.ai/), in particular the [framework integration guide](https://docs.cloud.llamaindex.ai/llamacloud/guides/framework_integration).\\n\\n##Using LlamaCloud from LlamaIndex[#](https://docs.llamaindex.ai/en/stable/understanding/loading/llamacloud/#using-llamacloud-from-llamaindex)\\nYou can use LlamaCloud to connect to your data stores and automatically index them. Once an index is created, you can use it in just a few lines of code:\\n\\nimport os\\nfrom llama_index.indices.managed.llama_cloud import LlamaCloudIndex\\n\\nos.environ[\"LLAMA_CLOUD_API_KEY\"] = \"llx-...\"\\n\\nindex = LlamaCloudIndex(\"my_first_index\", project_name=\"Default\")\\nquery_engine = index.as_query_engine()\\nanswer = query_engine.query(\"Example query\")\\nIt\\'s also possible to programmatically load documents into a LlamaCloud index; check the [documentation](https://docs.cloud.llamaindex.ai/llamacloud/guides/framework_integration) for more details.\\n\\n\\n🦙'}, {'meta': {'url': 'https://docs.llamaindex.ai/en/stable/understanding/workflows/concurrent_execution/', 'fetchedUrl': 'https://docs.llamaindex.ai/en/stable/understanding/workflows/concurrent_execution/', 'fetchedUrlStatusCode': 200, 'meta': {'title': 'Concurrent execution - LlamaIndex'}}, 'text': ' \\n[ Skip to content ](https://docs.llamaindex.ai/en/stable/understanding/workflows/concurrent_execution/#concurrent-execution-of-workflows)\\n#Concurrent execution of workflows[#](https://docs.llamaindex.ai/en/stable/understanding/workflows/concurrent_execution/#concurrent-execution-of-workflows)\\nIn addition to looping and branching, workflows can run steps concurrently. This is useful when you have multiple steps that can be run independently of each other and they have time-consuming operations that they await, allowing other steps to run in parallel.\\n\\n##Emitting multiple events[#](https://docs.llamaindex.ai/en/stable/understanding/workflows/concurrent_execution/#emitting-multiple-events)\\nIn our examples so far, we\\'ve only emitted one event from each step. But there are many cases where you would want to run steps in parallel. To do this, you need to emit multiple events. You can do this using send_event:\\n\\nclass ParallelFlow(Workflow):\\n    @step\\n    async def start(self, ctx: Context, ev: StartEvent) -> StepTwoEvent:\\n        ctx.send_event(StepTwoEvent(query=\"Query 1\"))\\n        ctx.send_event(StepTwoEvent(query=\"Query 2\"))\\n        ctx.send_event(StepTwoEvent(query=\"Query 3\"))\\n\\n    @step(num_workers=4)\\n    async def step_two(self, ctx: Context, ev: StepTwoEvent) -> StopEvent:\\n        print(\"Running slow query \", ev.query)\\n        await asyncio.sleep(random.randint(1, 5))\\n\\n        return StopEvent(result=ev.query)\\nIn this example, our start step emits 3 StepTwoEvents. The step_two step is decorated with num_workers=4, which tells the workflow to run up to 4 instances of this step concurrently (this is the default).\\n\\n##Collecting events[#](https://docs.llamaindex.ai/en/stable/understanding/workflows/concurrent_execution/#collecting-events)\\nIf you execute the previous example, you\\'ll note that the workflow stops after whichever query is first to complete. Sometimes that\\'s useful, but other times you\\'ll want to wait for all your slow operations to complete before moving on to another step. You can do this using collect_events:\\n\\nclass ConcurrentFlow(Workflow):\\n    @step\\n    async def start(self, ctx: Context, ev: StartEvent) -> StepTwoEvent:\\n        ctx.send_event(StepTwoEvent(query=\"Query 1\"))\\n        ctx.send_event(StepTwoEvent(query=\"Query 2\"))\\n        ctx.send_event(StepTwoEvent(query=\"Query 3\"))\\n\\n    @step(num_workers=4)\\n    async def step_two(self, ctx: Context, ev: StepTwoEvent) -> StepThreeEvent:\\n        print(\"Running query \", ev.query)\\n        await asyncio.sleep(random.randint(1, 5))\\n        return StepThreeEvent(result=ev.query)\\n\\n    @step\\n    async def step_three(self, ctx: Context, ev: StepThreeEvent) -> StopEvent:\\n        # wait until we receive 3 events\\n        result = ctx.collect_events(ev, [StepThreeEvent] * 3)\\n        if result is None:\\n            return None\\n\\n        # do something with all 3 results together\\n        print(result)\\n        return StopEvent(result=\"Done\")\\nThe collect_events method lives on the Context and takes the event that triggered the step and an array of event types to wait for. In this case, we are awaiting 3 events of the same StepThreeEvent type.\\n\\nThe step_three step is fired every time a StepThreeEvent is received, but collect_events will return None until all 3 events have been received. At that point, the step will continue and you can do something with all 3 results together.\\n\\nThe result returned from collect_events is an array of the events that were collected, in the order that they were received.\\n\\n##Multiple event types[#](https://docs.llamaindex.ai/en/stable/understanding/workflows/concurrent_execution/#multiple-event-types)\\nOf course, you do not need to wait for the same type of event. You can wait for any combination of events you like, such as in this example:\\n\\nclass ConcurrentFlow(Workflow):\\n    @step\\n    async def start(\\n        self, ctx: Context, ev: StartEvent\\n    ) -> StepAEvent | StepBEvent | StepCEvent:\\n        ctx.send_event(StepAEvent(query=\"Query 1\"))\\n        ctx.send_event(StepBEvent(query=\"Query 2\"))\\n        ctx.send_event(StepCEvent(query=\"Query 3\"))\\n\\n    @step\\n    async def step_a(self, ctx: Context, ev: StepAEvent) -> StepACompleteEvent:\\n        print(\"Doing something A-ish\")\\n        return StepACompleteEvent(result=ev.query)\\n\\n    @step\\n    async def step_b(self, ctx: Context, ev: StepBEvent) -> StepBCompleteEvent:\\n        print(\"Doing something B-ish\")\\n        return StepBCompleteEvent(result=ev.query)\\n\\n    @step\\n    async def step_c(self, ctx: Context, ev: StepCEvent) -> StepCCompleteEvent:\\n        print(\"Doing something C-ish\")\\n        return StepCCompleteEvent(result=ev.query)\\n\\n    @step\\n    async def step_three(\\n        self,\\n        ctx: Context,\\n        ev: StepACompleteEvent | StepBCompleteEvent | StepCCompleteEvent,\\n    ) -> StopEvent:\\n        print(\"Received event \", ev.result)\\n\\n        # wait until we receive 3 events\\n        if (\\n            ctx.collect_events(\\n                ev,\\n                [StepCCompleteEvent, StepACompleteEvent, StepBCompleteEvent],\\n            )\\n            is None\\n        ):\\n            return None\\n\\n        # do something with all 3 results together\\n        return StopEvent(result=\"Done\")\\nThere are several changes we\\'ve made to handle multiple event types: * start is now declared as emitting 3 different event types * step_three is now declared as accepting 3 different event types * collect_events now takes an array of the event types to wait for\\n\\nNote that the order of the event types in the array passed to collect_events is important. The events will be returned in the order they are passed to collect_events, regardless of when they were received.\\n\\nThe visualization of this workflow is quite pleasing:\\n\\n\\n\\nNow let\\'s look at how we can extend workflows with [subclassing](https://docs.llamaindex.ai/en/stable/understanding/workflows/subclass/) and other techniques.\\n\\n\\n🦙'}, {'meta': {'url': 'https://docs.llamaindex.ai/en/stable/understanding/', 'fetchedUrl': 'https://docs.llamaindex.ai/en/stable/understanding/', 'fetchedUrlStatusCode': 200, 'meta': {'title': 'Building an LLM Application - LlamaIndex'}}, 'text': \" \\n[ Skip to content ](https://docs.llamaindex.ai/en/stable/understanding/#building-an-llm-application)\\n#Building an LLM application[#](https://docs.llamaindex.ai/en/stable/understanding/#building-an-llm-application)\\nWelcome to the beginning of Understanding LlamaIndex. This is a series of short, bite-sized tutorials on every stage of building an LLM application to get you acquainted with how to use LlamaIndex before diving into more advanced and subtle strategies. If you're an experienced programmer new to LlamaIndex, this is the place to start.\\n\\n##Key steps in building an LLM application[#](https://docs.llamaindex.ai/en/stable/understanding/#key-steps-in-building-an-llm-application)\\nTip\\n\\nIf you've already read our [high-level concepts](https://docs.llamaindex.ai/en/stable/getting_started/concepts/) page you'll recognize several of these steps.\\n\\nThis tutorial has three main parts: Building a RAG pipeline, Building an agent, and Building Workflows, with some smaller sections before and after. Here's what to expect:\\n\\n-\\n[Using LLMs](https://docs.llamaindex.ai/en/stable/understanding/using_llms/using_llms/): hit the ground running by getting started working with LLMs. We'll show you how to use any of our [dozens of supported LLMs](https://docs.llamaindex.ai/en/stable/module_guides/models/llms/modules/), whether via remote API calls or running locally on your machine.\\n\\n-\\nBuilding a RAG pipeline: Retrieval-Augmented Generation (RAG) is a key technique for getting your data into an LLM, and a component of more sophisticated agentic systems. We'll show you how to build a full-featured RAG pipeline that can answer questions about your data. This includes:\\n\\n-\\n[Loading & Ingestion](https://docs.llamaindex.ai/en/stable/understanding/loading/loading/): Getting your data from wherever it lives, whether that's unstructured text, PDFs, databases, or APIs to other applications. LlamaIndex has hundreds of connectors to every data source over at [LlamaHub](https://llamahub.ai/).\\n\\n-\\n[Indexing and Embedding](https://docs.llamaindex.ai/en/stable/understanding/indexing/indexing/): Once you've got your data there are an infinite number of ways to structure access to that data to ensure your applications is always working with the most relevant data. LlamaIndex has a huge number of these strategies built-in and can help you select the best ones.\\n\\n-\\n[Storing](https://docs.llamaindex.ai/en/stable/understanding/storing/storing/): You will probably find it more efficient to store your data in indexed form, or pre-processed summaries provided by an LLM, often in a specialized database known as a Vector Store (see below). You can also store your indexes, metadata and more.\\n\\n-\\n[Querying](https://docs.llamaindex.ai/en/stable/understanding/querying/querying/): Every indexing strategy has a corresponding querying strategy and there are lots of ways to improve the relevance, speed and accuracy of what you retrieve and what the LLM does with it before returning it to you, including turning it into structured responses such as an API.\\n\\n-\\nBuilding an agent: agents are LLM-powered knowledge workers that can interact with the world via a set of tools. Those tools can be RAG engines such as you learned how to build in the previous section, or any arbitrary code. This tutorial includes:\\n\\n-\\n[Building a basic agent](https://docs.llamaindex.ai/en/stable/understanding/agent/basic_agent.md): We show you how to build a simple agent that can interact with the world via a set of tools.\\n\\n-\\n[Using local models with agents](https://docs.llamaindex.ai/en/stable/understanding/agent/local_models/): Agents can be built to use local models, which can be important for performance or privacy reasons.\\n\\n-\\n[Adding RAG to an agent](https://docs.llamaindex.ai/en/stable/understanding/agent/rag_agent/): The RAG pipelines you built in the previous tutorial can be used as a tool by an agent, giving your agent powerful information-retrieval capabilities.\\n\\n-\\n[Adding other tools](https://docs.llamaindex.ai/en/stable/understanding/agent/tools/): Let's add more sophisticated tools to your agent, such as API integrations.\\n\\n-\\nBuilding Workflows: Workflows are a low-level, event-driven abstraction for building agentic applications. They're the base layer you should be using to build any custom, advanced RAG/agent system. You can use the pre-built abstractions you learned above, or build completely agentic applications from scratch. [Get started here](https://docs.llamaindex.ai/en/stable/understanding/workflows/).\\n\\n-\\n[Putting it all together](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/): whether you are building question & answering, chatbots, an API, or an autonomous agent, we show you how to get your application into production.\\n\\n-\\n[Tracing and debugging](https://docs.llamaindex.ai/en/stable/understanding/tracing_and_debugging/tracing_and_debugging/): also called observability, it's especially important with LLM applications to be able to look into the inner workings of what's going on to help you debug problems and spot places to improve.\\n\\n-\\n[Evaluating](https://docs.llamaindex.ai/en/stable/understanding/evaluating/evaluating/): every strategy has pros and cons and a key part of building, shipping and evolving your application is evaluating whether your change has improved your application in terms of accuracy, performance, clarity, cost and more. Reliably evaluating your changes is a crucial part of LLM application development.\\n\\n##Let's get started![#](https://docs.llamaindex.ai/en/stable/understanding/#lets-get-started)\\nReady to dive in? Head to [using LLMs](https://docs.llamaindex.ai/en/stable/understanding/using_llms/using_llms/).\\n\\n\\nHi, how can I help you?\\n\\n🦙\"}, {'meta': {'url': 'https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_with_delphic/', 'fetchedUrl': 'https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_with_delphic/', 'fetchedUrlStatusCode': 200, 'meta': {'title': 'A Guide to Building a Full-Stack LlamaIndex Web App with Delphic - LlamaIndex'}}, 'text': ' \\n[ Skip to content ](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_with_delphic/#a-guide-to-building-a-full-stack-llamaindex-web-app-with-delphic)\\n#A Guide to Building a Full-Stack LlamaIndex Web App with Delphic[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_with_delphic/#a-guide-to-building-a-full-stack-llamaindex-web-app-with-delphic)\\nThis guide seeks to walk you through using LlamaIndex with a production-ready web app starter template called [Delphic](https://github.com/JSv4/Delphic). All code examples here are available from the [Delphic](https://github.com/JSv4/Delphic) repo\\n\\n##What We\\'re Building[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_with_delphic/#what-were-building)\\nHere\\'s a quick demo of the out-of-the-box functionality of Delphic:\\n\\nhttps://user-images.githubusercontent.com/5049984/233236432-aa4980b6-a510-42f3-887a-81485c9644e6.mp4\\n\\n##Architectural Overview[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_with_delphic/#architectural-overview)\\nDelphic leverages the LlamaIndex python library to let users to create their own document collections they can then query in a responsive frontend.\\n\\nWe chose a stack that provides a responsive, robust mix of technologies that can (1) orchestrate complex python processing tasks while providing (2) a modern, responsive frontend and (3) a secure backend to build additional functionality upon.\\n\\nThe core libraries are:\\n\\n- [Django](https://www.djangoproject.com/)\\n- [Django Channels](https://channels.readthedocs.io/en/stable/)\\n- [Django Ninja](https://django-ninja.rest-framework.com/)\\n- [Redis](https://redis.io/)\\n- [Celery](https://docs.celeryq.dev/en/stable/getting-started/introduction.html)\\n- [LlamaIndex](https://gpt-index.readthedocs.io/en/latest/)\\n- [Langchain](https://python.langchain.com/en/latest/index.html)\\n- [React](https://github.com/facebook/react)\\n- Docker & Docker Compose\\nThanks to this modern stack built on the super stable Django web framework, the starter Delphic app boasts a streamlined developer experience, built-in authentication and user management, asynchronous vector store processing, and web-socket-based query connections for a responsive UI. In addition, our frontend is built with TypeScript and is based on MUI React for a responsive and modern user interface.\\n\\n##System Requirements[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_with_delphic/#system-requirements)\\nCelery doesn\\'t work on Windows. It may be deployable with Windows Subsystem for Linux, but configuring that is beyond the scope of this tutorial. For this reason, we recommend you only follow this tutorial if you\\'re running Linux or OSX. You will need Docker and Docker Compose installed to deploy the application. Local development will require node version manager (nvm).\\n\\n##Django Backend[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_with_delphic/#django-backend)\\n###Project Directory Overview[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_with_delphic/#project-directory-overview)\\nThe Delphic application has a structured backend directory organization that follows common Django project conventions. From the repo root, in the ./delphic subfolder, the main folders are:\\n\\n- contrib: This directory contains custom modifications or additions to Django\\'s built-in contrib apps.\\n-\\nindexes: This directory contains the core functionality related to document indexing and LLM integration. It includes:\\n\\n-\\nadmin.py: Django admin configuration for the app\\n\\n- apps.py: Application configuration\\n- models.py: Contains the app\\'s database models\\n- migrations: Directory containing database schema migrations for the app\\n- signals.py: Defines any signals for the app\\n-\\ntests.py: Unit tests for the app\\n\\n-\\ntasks: This directory contains tasks for asynchronous processing using Celery. The index_tasks.py file includes the tasks for creating vector indexes.\\n\\n- users: This directory is dedicated to user management, including:\\n- utils: This directory contains utility modules and functions that are used across the application, such as custom storage backends, path helpers, and collection-related utilities.\\n###Database Models[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_with_delphic/#database-models)\\nThe Delphic application has two core models: Document and Collection. These models represent the central entities the application deals with when indexing and querying documents using LLMs. They\\'re defined in [./delphic/indexes/models.py](https://github.com/JSv4/Delphic/blob/main/delphic/indexes/models.py).\\n\\n-\\nCollection:\\n\\n-\\napi_key: A foreign key that links a collection to an API key. This helps associate jobs with the source API key.\\n\\n- title: A character field that provides a title for the collection.\\n- description: A text field that provides a description of the collection.\\n- status: A character field that stores the processing status of the collection, utilizing the CollectionStatus enumeration.\\n- created: A datetime field that records when the collection was created.\\n- modified: A datetime field that records the last modification time of the collection.\\n- model: A file field that stores the model associated with the collection.\\n-\\nprocessing: A boolean field that indicates if the collection is currently being processed.\\n\\n-\\nDocument:\\n\\n-\\ncollection: A foreign key that links a document to a collection. This represents the relationship between documents and collections.\\n\\n- file: A file field that stores the uploaded document file.\\n- description: A text field that provides a description of the document.\\n- created: A datetime field that records when the document was created.\\n- modified: A datetime field that records the last modification time of the document.\\nThese models provide a solid foundation for collections of documents and the indexes created from them with LlamaIndex.\\n\\n###Django Ninja API[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_with_delphic/#django-ninja-api)\\nDjango Ninja is a web framework for building APIs with Django and Python 3.7+ type hints. It provides a simple, intuitive, and expressive way of defining API endpoints, leveraging Python’s type hints to automatically generate input validation, serialization, and documentation.\\n\\nIn the Delphic repo, the [./config/api/endpoints.py](https://github.com/JSv4/Delphic/blob/main/config/api/endpoints.py) file contains the API routes and logic for the API endpoints. Now, let’s briefly address the purpose of each endpoint in the endpoints.py file:\\n\\n-\\n/heartbeat: A simple GET endpoint to check if the API is up and running. Returns True if the API is accessible. This is helpful for Kubernetes setups that expect to be able to query your container to ensure it\\'s up and running.\\n\\n-\\n/collections/create: A POST endpoint to create a new Collection. Accepts form parameters such as title, description, and a list of files. Creates a new Collection and Document instances for each file, and schedules a Celery task to create an index.\\n\\n@collections_router.post(\"/create\")\\nasync def create_collection(\\n    request,\\n    title: str = Form(...),\\n    description: str = Form(...),\\n    files: list[UploadedFile] = File(...),\\n):\\n    key = None if getattr(request, \"auth\", None) is None else request.auth\\n    if key is not None:\\n        key = await key\\n\\n    collection_instance = Collection(\\n        api_key=key,\\n        title=title,\\n        description=description,\\n        status=CollectionStatusEnum.QUEUED,\\n    )\\n\\n    await sync_to_async(collection_instance.save)()\\n\\n    for uploaded_file in files:\\n        doc_data = uploaded_file.file.read()\\n        doc_file = ContentFile(doc_data, uploaded_file.name)\\n        document = Document(collection=collection_instance, file=doc_file)\\n        await sync_to_async(document.save)()\\n\\n    create_index.si(collection_instance.id).apply_async()\\n\\n    return await sync_to_async(CollectionModelSchema)(...)\\n- /collections/query — a POST endpoint to query a document collection using the LLM. Accepts a JSON payload containing collection_id and query_str, and returns a response generated by querying the collection. We don\\'t actually use this endpoint in our chat GUI (We use a websocket - see below), but you could build an app to integrate to this REST endpoint to query a specific collection.\\n@collections_router.post(\\n    \"/query\",\\n    response=CollectionQueryOutput,\\n    summary=\"Ask a question of a document collection\",\\n)\\ndef query_collection_view(\\n    request: HttpRequest, query_input: CollectionQueryInput\\n):\\n    collection_id = query_input.collection_id\\n    query_str = query_input.query_str\\n    response = query_collection(collection_id, query_str)\\n    return {\"response\": response}\\n- /collections/available: A GET endpoint that returns a list of all collections created with the user\\'s API key. The output is serialized using the CollectionModelSchema.\\n@collections_router.get(\\n    \"/available\",\\n    response=list[CollectionModelSchema],\\n    summary=\"Get a list of all of the collections created with my api_key\",\\n)\\nasync def get_my_collections_view(request: HttpRequest):\\n    key = None if getattr(request, \"auth\", None) is None else request.auth\\n    if key is not None:\\n        key = await key\\n\\n    collections = Collection.objects.filter(api_key=key)\\n\\n    return [{...} async for collection in collections]\\n- /collections/{collection_id}/add_file: A POST endpoint to add a file to an existing collection. Accepts a collection_id path parameter, and form parameters such as file and description. Adds the file as a Document instance associated with the specified collection.\\n@collections_router.post(\\n    \"/{collection_id}/add_file\", summary=\"Add a file to a collection\"\\n)\\nasync def add_file_to_collection(\\n    request,\\n    collection_id: int,\\n    file: UploadedFile = File(...),\\n    description: str = Form(...),\\n):\\n    collection = await sync_to_async(Collection.objects.get)(id=collection_id)\\n###Intro to Websockets[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_with_delphic/#intro-to-websockets)\\nWebSockets are a communication protocol that enables bidirectional and full-duplex communication between a client and a server over a single, long-lived connection. The WebSocket protocol is designed to work over the same ports as HTTP and HTTPS (ports 80 and 443, respectively) and uses a similar handshake process to establish a connection. Once the connection is established, data can be sent in both directions as “frames” without the need to reestablish the connection each time, unlike traditional HTTP requests.\\n\\nThere are several reasons to use WebSockets, particularly when working with code that takes a long time to load into memory but is quick to run once loaded:\\n\\n- Performance: WebSockets eliminate the overhead associated with opening and closing multiple connections for each request, reducing latency.\\n- Efficiency: WebSockets allow for real-time communication without the need for polling, resulting in more efficient use of resources and better responsiveness.\\n- Scalability: WebSockets can handle a large number of simultaneous connections, making it ideal for applications that require high concurrency.\\nIn the case of the Delphic application, using WebSockets makes sense as the LLMs can be expensive to load into memory. By establishing a WebSocket connection, the LLM can remain loaded in memory, allowing subsequent requests to be processed quickly without the need to reload the model each time.\\n\\nThe ASGI configuration file [./config/asgi.py](https://github.com/JSv4/Delphic/blob/main/config/asgi.py) defines how the application should handle incoming connections, using the Django Channels ProtocolTypeRouter to route connections based on their protocol type. In this case, we have two protocol types: \"http\" and \"websocket\".\\n\\nThe “http” protocol type uses the standard Django ASGI application to handle HTTP requests, while the “websocket” protocol type uses a custom TokenAuthMiddleware to authenticate WebSocket connections. The URLRouter within the TokenAuthMiddleware defines a URL pattern for the CollectionQueryConsumer, which is responsible for handling WebSocket connections related to querying document collections.\\n\\napplication = ProtocolTypeRouter(\\n    {\\n        \"http\": get_asgi_application(),\\n        \"websocket\": TokenAuthMiddleware(\\n            URLRouter(\\n                [\\n                    re_path(\\n                        r\"ws/collections/(?P<collection_id>\\\\w+)/query/$\",\\n                        CollectionQueryConsumer.as_asgi(),\\n                    ),\\n                ]\\n            )\\n        ),\\n    }\\n)\\nThis configuration allows clients to establish WebSocket connections with the Delphic application to efficiently query document collections using the LLMs, without the need to reload the models for each request.\\n\\n###Websocket Handler[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_with_delphic/#websocket-handler)\\nThe CollectionQueryConsumer class in [config/api/websockets/queries.py](https://github.com/JSv4/Delphic/blob/main/config/api/websockets/queries.py) is responsible for handling WebSocket connections related to querying document collections. It inherits from the AsyncWebsocketConsumer class provided by Django Channels.\\n\\nThe CollectionQueryConsumer class has three main methods:\\n\\n- connect: Called when a WebSocket is handshaking as part of the connection process.\\n- disconnect: Called when a WebSocket closes for any reason.\\n- receive: Called when the server receives a message from the WebSocket.\\n####Websocket connect listener[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_with_delphic/#websocket-connect-listener)\\nThe connect method is responsible for establishing the connection, extracting the collection ID from the connection path, loading the collection model, and accepting the connection.\\n\\nasync def connect(self):\\n    try:\\n        self.collection_id = extract_connection_id(self.scope[\"path\"])\\n        self.index = await load_collection_model(self.collection_id)\\n        await self.accept()\\n\\n    except ValueError as e:\\n        await self.accept()\\n        await self.close(code=4000)\\n    except Exception as e:\\n        pass\\n####Websocket disconnect listener[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_with_delphic/#websocket-disconnect-listener)\\nThe disconnect method is empty in this case, as there are no additional actions to be taken when the WebSocket is closed.\\n\\n####Websocket receive listener[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_with_delphic/#websocket-receive-listener)\\nThe receive method is responsible for processing incoming messages from the WebSocket. It takes the incoming message, decodes it, and then queries the loaded collection model using the provided query. The response is then formatted as a markdown string and sent back to the client over the WebSocket connection.\\n\\nasync def receive(self, text_data):\\n    text_data_json = json.loads(text_data)\\n\\n    if self.index is not None:\\n        query_str = text_data_json[\"query\"]\\n        modified_query_str = f\"Please return a nicely formatted markdown string to this request:\\\\n\\\\n{query_str}\"\\n        query_engine = self.index.as_query_engine()\\n        response = query_engine.query(modified_query_str)\\n\\n        markdown_response = f\"## Response\\\\n\\\\n{response}\\\\n\\\\n\"\\n        if response.source_nodes:\\n            markdown_sources = (\\n                f\"## Sources\\\\n\\\\n{response.get_formatted_sources()}\"\\n            )\\n        else:\\n            markdown_sources = \"\"\\n\\n        formatted_response = f\"{markdown_response}{markdown_sources}\"\\n\\n        await self.send(json.dumps({\"response\": formatted_response}, indent=4))\\n    else:\\n        await self.send(\\n            json.dumps(\\n                {\"error\": \"No index loaded for this connection.\"}, indent=4\\n            )\\n        )\\nTo load the collection model, the load_collection_model function is used, which can be found in [delphic/utils/collections.py](https://github.com/JSv4/Delphic/blob/main/delphic/utils/collections.py). This function retrieves the collection object with the given collection ID, checks if a JSON file for the collection model exists, and if not, creates one. Then, it sets up the LLM and Settings before loading the VectorStoreIndex using the cache file.\\n\\nfrom llama_index.core import Settings\\n\\n\\nasync def load_collection_model(collection_id: str | int) -> VectorStoreIndex:\\n    \"\"\"\\n    Load the Collection model from cache or the database, and return the index.\\n\\n    Args:\\n        collection_id (Union[str, int]): The ID of the Collection model instance.\\n\\n    Returns:\\n        VectorStoreIndex: The loaded index.\\n\\n    This function performs the following steps:\\n    1. Retrieve the Collection object with the given collection_id.\\n    2. Check if a JSON file with the name \\'/cache/model_{collection_id}.json\\' exists.\\n    3. If the JSON file doesn\\'t exist, load the JSON from the Collection.model FileField and save it to\\n       \\'/cache/model_{collection_id}.json\\'.\\n    4. Call VectorStoreIndex.load_from_disk with the cache_file_path.\\n    \"\"\"\\n    # Retrieve the Collection object\\n    collection = await Collection.objects.aget(id=collection_id)\\n    logger.info(f\"load_collection_model() - loaded collection {collection_id}\")\\n\\n    # Make sure there\\'s a model\\n    if collection.model.name:\\n        logger.info(\"load_collection_model() - Setup local json index file\")\\n\\n        # Check if the JSON file exists\\n        cache_dir = Path(settings.BASE_DIR) / \"cache\"\\n        cache_file_path = cache_dir / f\"model_{collection_id}.json\"\\n        if not cache_file_path.exists():\\n            cache_dir.mkdir(parents=True, exist_ok=True)\\n            with collection.model.open(\"rb\") as model_file:\\n                with cache_file_path.open(\\n                    \"w+\", encoding=\"utf-8\"\\n                ) as cache_file:\\n                    cache_file.write(model_file.read().decode(\"utf-8\"))\\n\\n        # define LLM\\n        logger.info(\\n            f\"load_collection_model() - Setup Settings with tokens {settings.MAX_TOKENS} and \"\\n            f\"model {settings.MODEL_NAME}\"\\n        )\\n        Settings.llm = OpenAI(\\n            temperature=0, model=\"gpt-3.5-turbo\", max_tokens=512\\n        )\\n\\n        # Call VectorStoreIndex.load_from_disk\\n        logger.info(\"load_collection_model() - Load llama index\")\\n        index = VectorStoreIndex.load_from_disk(\\n            cache_file_path,\\n        )\\n        logger.info(\\n            \"load_collection_model() - Llamaindex loaded and ready for query...\"\\n        )\\n\\n    else:\\n        logger.error(\\n            f\"load_collection_model() - collection {collection_id} has no model!\"\\n        )\\n        raise ValueError(\"No model exists for this collection!\")\\n\\n    return index\\n##React Frontend[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_with_delphic/#react-frontend)\\n###Overview[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_with_delphic/#overview)\\nWe chose to use TypeScript, React and Material-UI (MUI) for the Delphic project’s frontend for a couple reasons. First, as the most popular component library (MUI) for the most popular frontend framework (React), this choice makes this project accessible to a huge community of developers. Second, React is, at this point, a stable and generally well-liked framework that delivers valuable abstractions in the form of its virtual DOM while still being relatively stable and, in our opinion, pretty easy to learn, again making it accessible.\\n\\n###Frontend Project Structure[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_with_delphic/#frontend-project-structure)\\nThe frontend can be found in the [/frontend](https://github.com/JSv4/Delphic/tree/main/frontend) directory of the repo, with the React-related components being in /frontend/src . You’ll notice there is a DockerFile in the frontend directory and several folders and files related to configuring our frontend web server — [nginx](https://www.nginx.com/).\\n\\nThe /frontend/src/App.tsx file serves as the entry point of the application. It defines the main components, such as the login form, the drawer layout, and the collection create modal. The main components are conditionally rendered based on whether the user is logged in and has an authentication token.\\n\\nThe DrawerLayout2 component is defined in theDrawerLayour2.tsx file. This component manages the layout of the application and provides the navigation and main content areas.\\n\\nSince the application is relatively simple, we can get away with not using a complex state management solution like Redux and just use React’s useState hooks.\\n\\n###Grabbing Collections from the Backend[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_with_delphic/#grabbing-collections-from-the-backend)\\nThe collections available to the logged-in user are retrieved and displayed in the DrawerLayout2 component. The process can be broken down into the following steps:\\n\\n- Initializing state variables:\\nconst [collections, setCollections] = useState<CollectionModelSchema[]>([]);\\nconst [loading, setLoading] = useState(true);\\nHere, we initialize two state variables: collections to store the list of collections and loading to track whether the collections are being fetched.\\n\\n- Collections are fetched for the logged-in user with the fetchCollections() function:\\nconst\\nfetchCollections = async () = > {\\ntry {\\nconst accessToken = localStorage.getItem(\"accessToken\");\\nif (accessToken) {\\nconst response = await getMyCollections(accessToken);\\nsetCollections(response.data);\\n}\\n} catch (error) {\\nconsole.error(error);\\n} finally {\\nsetLoading(false);\\n}\\n};\\nThe fetchCollections function retrieves the collections for the logged-in user by calling the getMyCollections API function with the user\\'s access token. It then updates the collections state with the retrieved data and sets the loading state to false to indicate that fetching is complete.\\n\\n###Displaying Collections[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_with_delphic/#displaying-collections)\\nThe latest collectios are displayed in the drawer like this:\\n\\n< List >\\n{collections.map((collection) = > (\\n    < div key={collection.id} >\\n    < ListItem disablePadding >\\n    < ListItemButton\\n    disabled={\\n    collection.status != = CollectionStatus.COMPLETE | |\\n    !collection.has_model\\n    }\\n    onClick={() = > handleCollectionClick(collection)}\\nselected = {\\n    selectedCollection & &\\n    selectedCollection.id == = collection.id\\n}\\n>\\n< ListItemText\\nprimary = {collection.title} / >\\n          {collection.status == = CollectionStatus.RUNNING ? (\\n    < CircularProgress\\n    size={24}\\n    style={{position: \"absolute\", right: 16}}\\n    / >\\n): null}\\n< / ListItemButton >\\n    < / ListItem >\\n        < / div >\\n))}\\n< / List >\\nYou’ll notice that the disabled property of a collection’s ListItemButton is set based on whether the collection\\'s status is not CollectionStatus.COMPLETE or the collection does not have a model (!collection.has_model). If either of these conditions is true, the button is disabled, preventing users from selecting an incomplete or model-less collection. Where the CollectionStatus is RUNNING, we also show a loading wheel over the button.\\n\\nIn a separate useEffect hook, we check if any collection in the collections state has a status of CollectionStatus.RUNNING or CollectionStatus.QUEUED. If so, we set up an interval to repeatedly call the fetchCollections function every 15 seconds (15,000 milliseconds) to update the collection statuses. This way, the application periodically checks for completed collections, and the UI is updated accordingly when the processing is done.\\n\\nuseEffect(() = > {\\n    let\\ninterval: NodeJS.Timeout;\\nif (\\n    collections.some(\\n        (collection) = >\\ncollection.status == = CollectionStatus.RUNNING | |\\ncollection.status == = CollectionStatus.QUEUED\\n)\\n) {\\n    interval = setInterval(() = > {\\n    fetchCollections();\\n}, 15000);\\n}\\nreturn () = > clearInterval(interval);\\n}, [collections]);\\n###Chat View Component[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_with_delphic/#chat-view-component)\\nThe ChatView component in frontend/src/chat/ChatView.tsx is responsible for handling and displaying a chat interface for a user to interact with a collection. The component establishes a WebSocket connection to communicate in real-time with the server, sending and receiving messages.\\n\\nKey features of the ChatView component include:\\n\\n- Establishing and managing the WebSocket connection with the server.\\n- Displaying messages from the user and the server in a chat-like format.\\n- Handling user input to send messages to the server.\\n- Updating the messages state and UI based on received messages from the server.\\n- Displaying connection status and errors, such as loading messages, connecting to the server, or encountering errors while loading a collection.\\nTogether, all of this allows users to interact with their selected collection with a very smooth, low-latency experience.\\n\\n####Chat Websocket Client[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_with_delphic/#chat-websocket-client)\\nThe WebSocket connection in the ChatView component is used to establish real-time communication between the client and the server. The WebSocket connection is set up and managed in the ChatView component as follows:\\n\\nFirst, we want to initialize the WebSocket reference:\\n\\nconst websocket = useRef(null);\\n\\nA websocket reference is created using useRef, which holds the WebSocket object that will be used for communication. useRef is a hook in React that allows you to create a mutable reference object that persists across renders. It is particularly useful when you need to hold a reference to a mutable object, such as a WebSocket connection, without causing unnecessary re-renders.\\n\\nIn the ChatView component, the WebSocket connection needs to be established and maintained throughout the lifetime of the component, and it should not trigger a re-render when the connection state changes. By using useRef, you ensure that the WebSocket connection is kept as a reference, and the component only re-renders when there are actual state changes, such as updating messages or displaying errors.\\n\\nThe setupWebsocket function is responsible for establishing the WebSocket connection and setting up event handlers to handle different WebSocket events.\\n\\nOverall, the setupWebsocket function looks like this:\\n\\nconst setupWebsocket = () => {\\n  setConnecting(true);\\n  // Here, a new WebSocket object is created using the specified URL, which includes the\\n  // selected collection\\'s ID and the user\\'s authentication token.\\n\\n  websocket.current = new WebSocket(\\n    `ws://localhost:8000/ws/collections/${selectedCollection.id}/query/?token=${authToken}`,\\n  );\\n\\n  websocket.current.onopen = (event) => {\\n    //...\\n  };\\n\\n  websocket.current.onmessage = (event) => {\\n    //...\\n  };\\n\\n  websocket.current.onclose = (event) => {\\n    //...\\n  };\\n\\n  websocket.current.onerror = (event) => {\\n    //...\\n  };\\n\\n  return () => {\\n    websocket.current?.close();\\n  };\\n};\\nNotice in a bunch of places we trigger updates to the GUI based on the information from the web socket client.\\n\\nWhen the component first opens and we try to establish a connection, the onopen listener is triggered. In the callback, the component updates the states to reflect that the connection is established, any previous errors are cleared, and no messages are awaiting responses:\\n\\nwebsocket.current.onopen = (event) => {\\n  setError(false);\\n  setConnecting(false);\\n  setAwaitingMessage(false);\\n\\n  console.log(\"WebSocket connected:\", event);\\n};\\nonmessageis triggered when a new message is received from the server through the WebSocket connection. In the callback, the received data is parsed and the messages state is updated with the new message from the server:\\n\\nwebsocket.current.onmessage = (event) => {\\n  const data = JSON.parse(event.data);\\n  console.log(\"WebSocket message received:\", data);\\n  setAwaitingMessage(false);\\n\\n  if (data.response) {\\n    // Update the messages state with the new message from the server\\n    setMessages((prevMessages) => [\\n      ...prevMessages,\\n      {\\n        sender_id: \"server\",\\n        message: data.response,\\n        timestamp: new Date().toLocaleTimeString(),\\n      },\\n    ]);\\n  }\\n};\\noncloseis triggered when the WebSocket connection is closed. In the callback, the component checks for a specific close code (4000) to display a warning toast and update the component states accordingly. It also logs the close event:\\n\\nwebsocket.current.onclose = (event) => {\\n  if (event.code === 4000) {\\n    toast.warning(\\n      \"Selected collection\\'s model is unavailable. Was it created properly?\",\\n    );\\n    setError(true);\\n    setConnecting(false);\\n    setAwaitingMessage(false);\\n  }\\n  console.log(\"WebSocket closed:\", event);\\n};\\nFinally, onerror is triggered when an error occurs with the WebSocket connection. In the callback, the component updates the states to reflect the error and logs the error event:\\n\\nwebsocket.current.onerror = (event) => {\\n  setError(true);\\n  setConnecting(false);\\n  setAwaitingMessage(false);\\n\\n  console.error(\"WebSocket error:\", event);\\n};\\n####Rendering our Chat Messages[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_with_delphic/#rendering-our-chat-messages)\\nIn the ChatView component, the layout is determined using CSS styling and Material-UI components. The main layout consists of a container with a flex display and a column-oriented flexDirection. This ensures that the content within the container is arranged vertically.\\n\\nThere are three primary sections within the layout:\\n\\n- The chat messages area: This section takes up most of the available space and displays a list of messages exchanged between the user and the server. It has an overflow-y set to ‘auto’, which allows scrolling when the content overflows the available space. The messages are rendered using the ChatMessage component for each message and a ChatMessageLoading component to show the loading state while waiting for a server response.\\n- The divider: A Material-UI Divider component is used to separate the chat messages area from the input area, creating a clear visual distinction between the two sections.\\n- The input area: This section is located at the bottom and allows the user to type and send messages. It contains a TextField component from Material-UI, which is set to accept multiline input with a maximum of 2 rows. The input area also includes a Button component to send the message. The user can either click the \"Send\" button or press \" Enter\" on their keyboard to send the message.\\nThe user inputs accepted in the ChatView component are text messages that the user types in the TextField. The component processes these text inputs and sends them to the server through the WebSocket connection.\\n\\n##Deployment[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_with_delphic/#deployment)\\n###Prerequisites[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_with_delphic/#prerequisites)\\nTo deploy the app, you\\'re going to need Docker and Docker Compose installed. If you\\'re on Ubuntu or another, common Linux distribution, DigitalOcean has a [great Docker tutorial](https://www.digitalocean.com/community/tutorial_collections/how-to-install-and-use-docker) and another great tutorial for [Docker Compose](https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-compose-on-ubuntu-20-04) you can follow. If those don\\'t work for you, try the [official docker documentation.](https://docs.docker.com/engine/install/)\\n\\n###Build and Deploy[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_with_delphic/#build-and-deploy)\\nThe project is based on django-cookiecutter, and it’s pretty easy to get it deployed on a VM and configured to serve HTTPs traffic for a specific domain. The configuration is somewhat involved, however — not because of this project, but it’s just a fairly involved topic to configure your certificates, DNS, etc.\\n\\nFor the purposes of this guide, let’s just get running locally. Perhaps we’ll release a guide on production deployment. In the meantime, check out the [Django Cookiecutter project docs](https://cookiecutter-django.readthedocs.io/en/latest/deployment-with-docker.html) for starters.\\n\\nThis guide assumes your goal is to get the application up and running for use. If you want to develop, most likely you won’t want to launch the compose stack with the — profiles fullstack flag and will instead want to launch the react frontend using the node development server.\\n\\nTo deploy, first clone the repo:\\n\\ngit clone https://github.com/yourusername/delphic.git\\nChange into the project directory:\\n\\ncd delphic\\nCopy the sample environment files:\\n\\nmkdir -p ./.envs/.local/\\ncp -a ./docs/sample_envs/local/.frontend ./frontend\\ncp -a ./docs/sample_envs/local/.django ./.envs/.local\\ncp -a ./docs/sample_envs/local/.postgres ./.envs/.local\\nEdit the .django and .postgres configuration files to include your OpenAI API key and set a unique password for your database user. You can also set the response token limit in the .django file or switch which OpenAI model you want to use. GPT4 is supported, assuming you’re authorized to access it.\\n\\nBuild the docker compose stack with the --profiles fullstack flag:\\n\\nsudo docker-compose --profiles fullstack -f local.yml build\\nThe fullstack flag instructs compose to build a docker container from the frontend folder and this will be launched along with all of the needed, backend containers. It takes a long time to build a production React container, however, so we don’t recommend you develop this way. Follow the [instructions in the project readme.md](https://github.com/JSv4/Delphic#development) for development environment setup instructions.\\n\\nFinally, bring up the application:\\n\\nsudo docker-compose -f local.yml up\\nNow, visit localhost:3000 in your browser to see the frontend, and use the Delphic application locally.\\n\\n##Using the Application[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_with_delphic/#using-the-application)\\n###Setup Users[#](https://docs.llamaindex.ai/en/stable/understanding/putting_it_all_together/apps/fullstack_with_delphic/#setup-users)\\nIn order to actually use the application (at the moment, we intend to make it possible to share certain models with unauthenticated users), you need a login. You can use either a superuser or non-superuser. In either case, someone needs to first create a superuser using the console:\\n\\nWhy set up a Django superuser? A Django superuser has all the permissions in the application and can manage all aspects of the system, including creating, modifying, and deleting users, collections, and other data. Setting up a superuser allows you to fully control and manage the application.\\n\\nHow to create a Django superuser:\\n\\n1 Run the following command to create a superuser:\\n\\nsudo docker-compose -f local.yml run django python manage.py createsuperuser\\n\\n2 You will be prompted to provide a username, email address, and password for the superuser. Enter the required information.\\n\\nHow to create additional users using Django admin:\\n\\n- Start your Delphic application locally following the deployment instructions.\\n- Visit the Django admin interface by navigating to http://localhost:8000/admin in your browser.\\n- Log in with the superuser credentials you created earlier.\\n- Click on “Users” under the “Authentication and Authorization” section.\\n- Click on the “Add user +” button in the top right corner.\\n- Enter the required information for the new user, such as username and password. Click “Save” to create the user.\\n- To grant the new user additional permissions or make them a superuser, click on their username in the user list, scroll down to the “Permissions” section, and configure their permissions accordingly. Save your changes.\\n\\n🦙'}, {'meta': {'url': 'https://docs.llamaindex.ai/en/stable/understanding/evaluating/evaluating/', 'fetchedUrl': 'https://docs.llamaindex.ai/en/stable/understanding/evaluating/evaluating/', 'fetchedUrlStatusCode': 200, 'meta': {'title': 'Evaluating - LlamaIndex'}}, 'text': ' \\n[ Skip to content ](https://docs.llamaindex.ai/en/stable/understanding/evaluating/evaluating/#evaluating)\\n#Evaluating[#](https://docs.llamaindex.ai/en/stable/understanding/evaluating/evaluating/#evaluating)\\nEvaluation and benchmarking are crucial concepts in LLM development. To improve the performance of an LLM app (RAG, agents), you must have a way to measure it.\\n\\nLlamaIndex offers key modules to measure the quality of generated results. We also offer key modules to measure retrieval quality. You can learn more about how evaluation works in LlamaIndex in our [module guides](https://docs.llamaindex.ai/en/stable/module_guides/evaluating/).\\n\\n##Response Evaluation[#](https://docs.llamaindex.ai/en/stable/understanding/evaluating/evaluating/#response-evaluation)\\nDoes the response match the retrieved context? Does it also match the query? Does it match the reference answer or guidelines? Here\\'s a simple example that evaluates a single response for Faithfulness, i.e. whether the response is aligned to the context, such as being free from hallucinations:\\n\\nfrom llama_index.core import VectorStoreIndex\\nfrom llama_index.llms.openai import OpenAI\\nfrom llama_index.core.evaluation import FaithfulnessEvaluator\\n\\n# create llm\\nllm = OpenAI(model=\"gpt-4\", temperature=0.0)\\n\\n# build index\\n...\\nvector_index = VectorStoreIndex(...)\\n\\n# define evaluator\\nevaluator = FaithfulnessEvaluator(llm=llm)\\n\\n# query index\\nquery_engine = vector_index.as_query_engine()\\nresponse = query_engine.query(\\n    \"What battles took place in New York City in the American Revolution?\"\\n)\\neval_result = evaluator.evaluate_response(response=response)\\nprint(str(eval_result.passing))\\nThe response contains both the response and the source from which the response was generated; the evaluator compares them and determines if the response is faithful to the source.\\n\\nYou can learn more in our module guides about [response evaluation](https://docs.llamaindex.ai/en/stable/module_guides/evaluating/usage_pattern/).\\n\\n##Retrieval Evaluation[#](https://docs.llamaindex.ai/en/stable/understanding/evaluating/evaluating/#retrieval-evaluation)\\nAre the retrieved sources relevant to the query? This is a simple example that evaluates a single retrieval:\\n\\nfrom llama_index.core.evaluation import RetrieverEvaluator\\n\\n# define retriever somewhere (e.g. from index)\\n# retriever = index.as_retriever(similarity_top_k=2)\\nretriever = ...\\n\\nretriever_evaluator = RetrieverEvaluator.from_metric_names(\\n    [\"mrr\", \"hit_rate\"], retriever=retriever\\n)\\n\\nretriever_evaluator.evaluate(\\n    query=\"query\", expected_ids=[\"node_id1\", \"node_id2\"]\\n)\\nThis compares what was retrieved for the query to a set of nodes that were expected to be retrieved.\\n\\nIn reality you would want to evaluate a whole batch of retrievals; you can learn how do this in our module guide on [retrieval evaluation](https://docs.llamaindex.ai/en/stable/module_guides/evaluating/usage_pattern_retrieval/).\\n\\n##Related concepts[#](https://docs.llamaindex.ai/en/stable/understanding/evaluating/evaluating/#related-concepts)\\nYou may be interested in [analyzing the cost of your application](https://docs.llamaindex.ai/en/stable/understanding/evaluating/cost_analysis/) if you are making calls to a hosted, remote LLM.\\n\\n\\n🦙'}, {'meta': {'url': 'https://docs.llamaindex.ai/en/stable/understanding/loading/loading/', 'fetchedUrl': 'https://docs.llamaindex.ai/en/stable/understanding/loading/loading/', 'fetchedUrlStatusCode': 200, 'meta': {'title': 'Loading Data (Ingestion) - LlamaIndex'}}, 'text': ' \\n[ Skip to content ](https://docs.llamaindex.ai/en/stable/understanding/loading/loading/#loading-data-ingestion)\\n#Loading Data (Ingestion)[#](https://docs.llamaindex.ai/en/stable/understanding/loading/loading/#loading-data-ingestion)\\nBefore your chosen LLM can act on your data, you first need to process the data and load it. This has parallels to data cleaning/feature engineering pipelines in the ML world, or ETL pipelines in the traditional data setting.\\n\\nThis ingestion pipeline typically consists of three main stages:\\n\\n- Load the data\\n- Transform the data\\n- Index and store the data\\nWe cover indexing/storage in [future](https://docs.llamaindex.ai/en/stable/understanding/indexing/indexing/) [sections](https://docs.llamaindex.ai/en/stable/understanding/storing/storing/). In this guide we\\'ll mostly talk about loaders and transformations.\\n\\n##Loaders[#](https://docs.llamaindex.ai/en/stable/understanding/loading/loading/#loaders)\\nBefore your chosen LLM can act on your data you need to load it. The way LlamaIndex does this is via data connectors, also called Reader. Data connectors ingest data from different data sources and format the data into Document objects. A Document is a collection of data (currently text, and in future, images and audio) and metadata about that data.\\n\\n###Loading using SimpleDirectoryReader[#](https://docs.llamaindex.ai/en/stable/understanding/loading/loading/#loading-using-simpledirectoryreader)\\nThe easiest reader to use is our SimpleDirectoryReader, which creates documents out of every file in a given directory. It is built in to LlamaIndex and can read a variety of formats including Markdown, PDFs, Word documents, PowerPoint decks, images, audio and video.\\n\\nfrom llama_index.core import SimpleDirectoryReader\\n\\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\\n###Using Readers from LlamaHub[#](https://docs.llamaindex.ai/en/stable/understanding/loading/loading/#using-readers-from-llamahub)\\nBecause there are so many possible places to get data, they are not all built-in. Instead, you download them from our registry of data connectors, [LlamaHub](https://docs.llamaindex.ai/en/stable/understanding/loading/llamahub/).\\n\\nIn this example LlamaIndex downloads and installs the connector called [DatabaseReader](https://llamahub.ai/l/readers/llama-index-readers-database), which runs a query against a SQL database and returns every row of the results as a Document:\\n\\nfrom llama_index.core import download_loader\\n\\nfrom llama_index.readers.database import DatabaseReader\\n\\nreader = DatabaseReader(\\n    scheme=os.getenv(\"DB_SCHEME\"),\\n    host=os.getenv(\"DB_HOST\"),\\n    port=os.getenv(\"DB_PORT\"),\\n    user=os.getenv(\"DB_USER\"),\\n    password=os.getenv(\"DB_PASS\"),\\n    dbname=os.getenv(\"DB_NAME\"),\\n)\\n\\nquery = \"SELECT * FROM users\"\\ndocuments = reader.load_data(query=query)\\nThere are hundreds of connectors to use on [LlamaHub](https://llamahub.ai)!\\n\\n###Creating Documents directly[#](https://docs.llamaindex.ai/en/stable/understanding/loading/loading/#creating-documents-directly)\\nInstead of using a loader, you can also use a Document directly.\\n\\nfrom llama_index.core import Document\\n\\ndoc = Document(text=\"text\")\\n##Transformations[#](https://docs.llamaindex.ai/en/stable/understanding/loading/loading/#transformations)\\nAfter the data is loaded, you then need to process and transform your data before putting it into a storage system. These transformations include chunking, extracting metadata, and embedding each chunk. This is necessary to make sure that the data can be retrieved, and used optimally by the LLM.\\n\\nTransformation input/outputs are Node objects (a Document is a subclass of a Node). Transformations can also be stacked and reordered.\\n\\nWe have both a high-level and lower-level API for transforming documents.\\n\\n###High-Level Transformation API[#](https://docs.llamaindex.ai/en/stable/understanding/loading/loading/#high-level-transformation-api)\\nIndexes have a .from_documents() method which accepts an array of Document objects and will correctly parse and chunk them up. However, sometimes you will want greater control over how your documents are split up.\\n\\nfrom llama_index.core import VectorStoreIndex\\n\\nvector_index = VectorStoreIndex.from_documents(documents)\\nvector_index.as_query_engine()\\nUnder the hood, this splits your Document into Node objects, which are similar to Documents (they contain text and metadata) but have a relationship to their parent Document.\\n\\nIf you want to customize core components, like the text splitter, through this abstraction you can pass in a custom transformations list or apply to the global Settings:\\n\\nfrom llama_index.core.node_parser import SentenceSplitter\\n\\ntext_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=10)\\n\\n# global\\nfrom llama_index.core import Settings\\n\\nSettings.text_splitter = text_splitter\\n\\n# per-index\\nindex = VectorStoreIndex.from_documents(\\n    documents, transformations=[text_splitter]\\n)\\n###Lower-Level Transformation API[#](https://docs.llamaindex.ai/en/stable/understanding/loading/loading/#lower-level-transformation-api)\\nYou can also define these steps explicitly.\\n\\nYou can do this by either using our transformation modules (text splitters, metadata extractors, etc.) as standalone components, or compose them in our declarative [Transformation Pipeline interface](https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/).\\n\\nLet\\'s walk through the steps below.\\n\\n####Splitting Your Documents into Nodes[#](https://docs.llamaindex.ai/en/stable/understanding/loading/loading/#splitting-your-documents-into-nodes)\\nA key step to process your documents is to split them into \"chunks\"/Node objects. The key idea is to process your data into bite-sized pieces that can be retrieved / fed to the LLM.\\n\\nLlamaIndex has support for a wide range of [text splitters](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/), ranging from paragraph/sentence/token based splitters to file-based splitters like HTML, JSON.\\n\\nThese can be [used on their own or as part of an ingestion pipeline](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/).\\n\\nfrom llama_index.core import SimpleDirectoryReader\\nfrom llama_index.core.ingestion import IngestionPipeline\\nfrom llama_index.core.node_parser import TokenTextSplitter\\n\\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\\n\\npipeline = IngestionPipeline(transformations=[TokenTextSplitter(), ...])\\n\\nnodes = pipeline.run(documents=documents)\\n###Adding Metadata[#](https://docs.llamaindex.ai/en/stable/understanding/loading/loading/#adding-metadata)\\nYou can also choose to add metadata to your documents and nodes. This can be done either manually or with [automatic metadata extractors](https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_metadata_extractor/).\\n\\nHere are guides on 1) [how to customize Documents](https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_documents/), and 2) [how to customize Nodes](https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_nodes/).\\n\\ndocument = Document(\\n    text=\"text\",\\n    metadata={\"filename\": \"<doc_file_name>\", \"category\": \"<category>\"},\\n)\\n###Adding Embeddings[#](https://docs.llamaindex.ai/en/stable/understanding/loading/loading/#adding-embeddings)\\nTo insert a node into a vector index, it should have an embedding. See our [ingestion pipeline](https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/) or our [embeddings guide](https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings/) for more details.\\n\\n###Creating and passing Nodes directly[#](https://docs.llamaindex.ai/en/stable/understanding/loading/loading/#creating-and-passing-nodes-directly)\\nIf you want to, you can create nodes directly and pass a list of Nodes directly to an indexer:\\n\\nfrom llama_index.core.schema import TextNode\\n\\nnode1 = TextNode(text=\"<text_chunk>\", id_=\"<node_id>\")\\nnode2 = TextNode(text=\"<text_chunk>\", id_=\"<node_id>\")\\n\\nindex = VectorStoreIndex([node1, node2])\\n\\nHi, how can I help you?\\n\\n🦙'}, {'meta': {'url': 'https://docs.llamaindex.ai/en/stable/understanding/agent/', 'fetchedUrl': 'https://docs.llamaindex.ai/en/stable/understanding/agent/', 'fetchedUrlStatusCode': 200, 'meta': {'title': 'Building a basic agent - LlamaIndex'}}, 'text': ' \\n[ Skip to content ](https://docs.llamaindex.ai/en/stable/understanding/agent/#building-a-basic-agent)\\n#Building a basic agent[#](https://docs.llamaindex.ai/en/stable/understanding/agent/#building-a-basic-agent)\\nIn LlamaIndex, an agent is a semi-autonomous piece of software powered by an LLM that is given a task and executes a series of steps towards solving that task. It is given a set of tools, which can be anything from arbitrary functions up to full LlamaIndex query engines, and it selects the best available tool to complete each step. When each step is completed, the agent judges whether the task is now complete, in which case it returns a result to the user, or whether it needs to take another step, in which case it loops back to the start.\\n\\nIn LlamaIndex, you can either use our prepackaged agents/tools or [build your own agentic workflows from scratch](https://docs.llamaindex.ai/en/stable/understanding/workflows/), covered in the \"Building Workflows\" section. This section covers our prepackaged agents and tools.\\n\\n\\n\\n##Getting started[#](https://docs.llamaindex.ai/en/stable/understanding/agent/#getting-started)\\nYou can find all of this code in [the tutorial repo](https://github.com/run-llama/python-agents-tutorial).\\n\\nTo avoid conflicts and keep things clean, we\\'ll start a new Python virtual environment. You can use any virtual environment manager, but we\\'ll use poetry here:\\n\\npoetry init\\npoetry shell\\nAnd then we\\'ll install the LlamaIndex library and some other dependencies that will come in handy:\\n\\npip install llama-index python-dotenv\\nIf any of this gives you trouble, check out our more detailed [installation guide](https://docs.llamaindex.ai/en/stable/understanding/getting_started/installation/).\\n\\n##OpenAI Key[#](https://docs.llamaindex.ai/en/stable/understanding/agent/#openai-key)\\nOur agent will be powered by OpenAI\\'s GPT-3.5-Turbo LLM, so you\\'ll need an [API key](https://platform.openai.com/). Once you have your key, you can put it in a .env file in the root of your project:\\n\\nOPENAI_API_KEY=sk-proj-xxxx\\nIf you don\\'t want to use OpenAI, we\\'ll show you how to use other models later.\\n\\n##Bring in dependencies[#](https://docs.llamaindex.ai/en/stable/understanding/agent/#bring-in-dependencies)\\nWe\\'ll start by importing the components of LlamaIndex we need, as well as loading the environment variables from our .env file:\\n\\nfrom dotenv import load_dotenv\\n\\nload_dotenv()\\nfrom llama_index.core.agent import ReActAgent\\nfrom llama_index.llms.openai import OpenAI\\nfrom llama_index.core.tools import FunctionTool\\n##Create basic tools[#](https://docs.llamaindex.ai/en/stable/understanding/agent/#create-basic-tools)\\nFor this simple example we\\'ll be creating two tools: one that knows how to multiply numbers together, and one that knows how to add them.\\n\\ndef multiply(a: float, b: float) -> float:\\n    \"\"\"Multiply two numbers and returns the product\"\"\"\\n    return a * b\\n\\n\\nmultiply_tool = FunctionTool.from_defaults(fn=multiply)\\n\\n\\ndef add(a: float, b: float) -> float:\\n    \"\"\"Add two numbers and returns the sum\"\"\"\\n    return a + b\\n\\n\\nadd_tool = FunctionTool.from_defaults(fn=add)\\nAs you can see, these are regular vanilla Python functions. The docstring comments provide metadata to the agent about what the tool does: if your LLM is having trouble figuring out which tool to use, these docstrings are what you should tweak first.\\n\\nAfter each function is defined we create FunctionTool objects from these functions, which wrap them in a way that the agent can understand.\\n\\n##Initialize the LLM[#](https://docs.llamaindex.ai/en/stable/understanding/agent/#initialize-the-llm)\\nGPT-3.5-Turbo is going to be doing the work today:\\n\\nllm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\\nYou could also pick another popular model accessible via API, such as those from [Mistral](https://docs.llamaindex.ai/en/stable/understanding/examples/llm/mistralai/), [Claude from Anthropic](https://docs.llamaindex.ai/en/stable/understanding/examples/llm/anthropic/) or [Gemini from Google](https://docs.llamaindex.ai/en/stable/understanding/examples/llm/gemini/).\\n\\n##Initialize the agent[#](https://docs.llamaindex.ai/en/stable/understanding/agent/#initialize-the-agent)\\nNow we create our agent. In this case, this is a [ReAct agent](https://klu.ai/glossary/react-agent-model), a relatively simple but powerful agent. We give it an array containing our two tools, the LLM we just created, and set verbose=True so we can see what\\'s going on:\\n\\nagent = ReActAgent.from_tools([multiply_tool, add_tool], llm=llm, verbose=True)\\n##Ask a question[#](https://docs.llamaindex.ai/en/stable/understanding/agent/#ask-a-question)\\nWe specify that it should use a tool, as this is pretty simple and GPT-3.5 doesn\\'t really need this tool to get the answer.\\n\\nresponse = agent.chat(\"What is 20+(2*4)? Use a tool to calculate every step.\")\\nThis should give you output similar to the following:\\n\\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\\nAction: multiply\\nAction Input: {\\'a\\': 2, \\'b\\': 4}\\nObservation: 8\\nThought: I need to add 20 to the result of the multiplication.\\nAction: add\\nAction Input: {\\'a\\': 20, \\'b\\': 8}\\nObservation: 28\\nThought: I can answer without using any more tools. I\\'ll use the user\\'s language to answer\\nAnswer: The result of 20 + (2 * 4) is 28.\\nThe result of 20 + (2 * 4) is 28.\\nAs you can see, the agent picks the correct tools one after the other and combines the answers to give the final result. Check the [repo](https://github.com/run-llama/python-agents-tutorial/blob/main/1_basic_agent.py) to see what the final code should look like.\\n\\nCongratulations! You\\'ve built the most basic kind of agent. Next you can find out how to use [local models](https://docs.llamaindex.ai/en/stable/understanding/agent/local_models/) or skip to [adding RAG to your agent](https://docs.llamaindex.ai/en/stable/understanding/agent/rag_agent/).\\n\\n\\n🦙'}]}\n"
          ]
        }
      ],
      "source": [
        "url = \"https://api.usescraper.com/crawler/jobs/{}/data\".format(response[\"id\"])\n",
        "\n",
        "data_res = requests.request(\"GET\", url, headers=headers)\n",
        "\n",
        "data_res = json.loads(data_res.text)\n",
        "\n",
        "print(data_res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8VEQvJkITLJ",
        "outputId": "1b5bfa6b-62f4-42df-f05e-9104ad839b55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "URL: https://docs.llamaindex.ai/en/stable/understanding/evaluating/cost_analysis/usage_pattern/\n",
            "Title: Usage Pattern - LlamaIndex\n",
            "Content:  \n",
            "[ Skip to content ](https://docs.llamaindex.ai/en/stable/understanding/evaluating/cost_analysis/usage_pattern/#usage-pattern)\n",
            "#Usage Pattern[#](https://docs.llamaindex.ai/en/stable/understanding/evaluating/cost_analysis/usage_pattern/#usage-pattern)\n",
            "##Estimating LLM and Embedding Token Counts[#](https://docs.llamaindex.ai/en/stable/understanding/evaluating/cost_analysis/usage_pattern/#estimating-llm-and-embedding-token-counts)\n",
            "In order to measure LLM and Embedding token counts, you'll need to\n",
            " ...\n"
          ]
        }
      ],
      "source": [
        "print(\"URL:\", data_res[\"data\"][0][\"meta\"][\"url\"])\n",
        "print(\"Title:\", data_res[\"data\"][0][\"meta\"][\"meta\"][\"title\"])\n",
        "print(\"Content:\", data_res[\"data\"][0][\"text\"][0:500], \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rt2nyuLhSYLR"
      },
      "source": [
        "## Convert to Document\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YEieGzSFSXas"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.schema import Document\n",
        "\n",
        "# Convert the chunks to Document objects so the LlamaIndex framework can process them.\n",
        "documents = [\n",
        "    Document(\n",
        "        text=row[\"text\"],\n",
        "        metadata={\"title\": row[\"meta\"][\"meta\"][\"title\"], \"url\": row[\"meta\"][\"url\"]},\n",
        "    )\n",
        "    for row in data_res[\"data\"]\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqbJG5a1i3Jo"
      },
      "source": [
        "# Create RAG Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "wxmiQDv3SXV6"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0, model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "tCVhv4OkSXTV"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "quwJI61dNVr-"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "6KpeCRMBUgup"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\n",
        "\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model\n",
        "Settings.text_splitter = text_splitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "nWTBidwoZSO0"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "RUuJO0IIYSeU"
      },
      "outputs": [],
      "source": [
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "6_s2LkH6YX1V"
      },
      "outputs": [],
      "source": [
        "res = query_engine.query(\"What is a query engine?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "02zdJNqIZKep",
        "outputId": "46f2d8de-0418-4132-de65-f6ed2e38469e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'A query engine is a component that facilitates the process of querying by allowing users to send prompts to a language model (LLM) and receive responses. It operates by retrieving relevant documents from an index, processing those documents, and synthesizing a response based on the query and the retrieved data. The query engine can be customized to adjust retrieval strategies, post-processing steps, and response synthesis methods to enhance the querying experience.'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res.response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuCcgP0nZSIl",
        "outputId": "b2a63277-20b6-41d1-f60a-5bedf36ca408"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Node ID\t 2b0135eb-b0db-4597-b6f4-e99dec59eb20\n",
            "Title\t Querying - LlamaIndex\n",
            "URL\t https://docs.llamaindex.ai/en/stable/understanding/querying/querying/\n",
            "Score\t 0.5797795511788522\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Node ID\t f5952992-33fe-4883-a264-996bb5062f61\n",
            "Title\t Querying - LlamaIndex\n",
            "URL\t https://docs.llamaindex.ai/en/stable/understanding/querying/querying/\n",
            "Score\t 0.503252077356493\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "# Show the retrieved nodes\n",
        "for src in res.source_nodes:\n",
        "    print(\"Node ID\\t\", src.node_id)\n",
        "    print(\"Title\\t\", src.metadata[\"title\"])\n",
        "    print(\"URL\\t\", src.metadata[\"url\"])\n",
        "    print(\"Score\\t\", src.score)\n",
        "    print(\"-_\" * 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "W6vplMnZ8VZM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
